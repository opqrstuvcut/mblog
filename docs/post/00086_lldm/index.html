<!DOCTYPE html>
<html lang="ja">
<head><script src="/mblog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=mblog/livereload" data-no-instant defer></script>
  
    <title>拡散言語モデルのLLaDA :: MatLoverによるMatlab以外のブログ</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性 2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります. 提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.
本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.
自己回帰型モデルの限界 従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：
逐次処理のため推論効率が悪い 「Reversal Curse」に弱い（参考：THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A” Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです. 従来のLLMのアプローチ LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.
$$ \max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)). $$
特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.
$$ p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}). $$
LLaDAのアプローチ：拡散モデル型のLLM LLaDAは、自己回帰ではなく拡散モデルのアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.
事前学習（Pretraining） 事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.
損失関数は次の通りです：
$$ \mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right]. $$
" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="http://localhost:1313/mblog/post/00086_lldm/" />


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LFC5W8DKV1"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-LFC5W8DKV1');
        }
      </script>



  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/main.min.775ac2af004d44c22a6d000fbd1d9af529642f5cef27399d0280d180af2c2e9b.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/menu.min.310d32205bdedd6f43144e3c3273c9deecd238eba5f9108db5ea96ca0cfbe377.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/post.min.ad50c7f4d00e7975918f37fc74c6029e1959a40d66fb5b2c6564a8715e985573.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/syntax.min.e9ab635cf918bc84b901eb65c0b2caa74c9544245e3647c1af5c129896ef276e.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="http://localhost:1313/mblog/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/mblog/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="ja" />
<meta property="og:type" content="article" />
<meta property="og:title" content="拡散言語モデルのLLaDA">
<meta property="og:description" content="BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性 2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります. 提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.
本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.
自己回帰型モデルの限界 従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：
逐次処理のため推論効率が悪い 「Reversal Curse」に弱い（参考：THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A” Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです. 従来のLLMのアプローチ LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.
$$ \max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)). $$
特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.
$$ p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}). $$
LLaDAのアプローチ：拡散モデル型のLLM LLaDAは、自己回帰ではなく拡散モデルのアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.
事前学習（Pretraining） 事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.
損失関数は次の通りです：
$$ \mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right]. $$
" />
<meta property="og:url" content="http://localhost:1313/mblog/post/00086_lldm/" />
<meta property="og:site_name" content="MatLoverによるMatlab以外のブログ" />

  <meta property="og:image" content="http://localhost:1313/mblog/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">

  <meta property="article:section" content="ディープラーニング" />

  <meta property="article:section" content="LLM" />

  <meta property="article:section" content="NLP" />


  <meta property="article:published_time" content="2025-06-30 00:00:00 &#43;0000 UTC" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="http://localhost:1313/mblog/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/mblog/">Home</a></li>
        
      
        
          <li><a href="/mblog/page/archives/">Archives</a></li>
        
      
        
          <li><a href="/mblog/page/search/">Search</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/mblog/" >Home</a></li>
        
      
        
          <li><a href="/mblog/page/archives/" >Archives</a></li>
        
      
      
        <li>
          <ul class="menu">
            <li class="menu__trigger">&nbsp;▾</li>
            <li>
              <ul class="menu__dropdown">
                
                  
                    <li><a href="/mblog/page/search/" >Search</a></li>
                  
                
              </ul>
            </li>
          </ul>
        </li>
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="http://localhost:1313/mblog/post/00086_lldm/">拡散言語モデルのLLaDA</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-06-30</time></div>

  
    <span class="post-tags">
      
      #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/nlp/">NLP</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/%E6%8B%A1%E6%95%A3%E3%83%A2%E3%83%87%E3%83%AB/">拡散モデル</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/llm/">LLM</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/llama/">LLaMA</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/llada/">LLaDA</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/bert/">BERT</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/">大規模言語モデル</a>&nbsp;
      
      #<a href="http://localhost:1313/mblog/tags/mlm/">MLM</a>&nbsp;
      
    </span>
  
  


  

  <div class="post-content"><div>
        <h1 id="bertを拡張した生成モデル拡散型llmlladaの概要と可能性">BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性<a href="#bertを拡張した生成モデル拡散型llmlladaの概要と可能性" class="hanchor" ariaLabel="Anchor">#</a> </h1>
<p>2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります.
提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.</p>
<p><img src="fig1.png" alt="性能比較top"></p>
<p>本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.</p>
<hr>
<h2 id="自己回帰型モデルの限界">自己回帰型モデルの限界<a href="#自己回帰型モデルの限界" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：</p>
<ul>
<li>逐次処理のため推論効率が悪い</li>
<li>「Reversal Curse」に弱い（参考：<a href="https://arxiv.org/pdf/2309.12288%EF%BC%89">THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A”</a>
<ul>
<li>Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです.</li>
</ul>
</li>
</ul>
<p><img src="reversal_curse.png" alt="Reversal Curse"></p>
<h2 id="従来のllmのアプローチ">従来のLLMのアプローチ<a href="#従来のllmのアプローチ" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.</p>
<p>$$
\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)).
$$</p>
<p>特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.</p>
<p>$$
p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}).
$$</p>
<h2 id="lladaのアプローチ拡散モデル型のllm">LLaDAのアプローチ：拡散モデル型のLLM<a href="#lladaのアプローチ拡散モデル型のllm" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>LLaDAは、自己回帰ではなく<strong>拡散モデル</strong>のアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.</p>
<h3 id="事前学習pretraining">事前学習（Pretraining）<a href="#事前学習pretraining" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><img src="fig2_1.png" alt="事前学習の概要"></p>
<p>事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.</p>
<p>損失関数は次の通りです：</p>
<p>$$
\mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right].
$$</p>
<p>この損失関数を小さくできるモデルはたしかに言語を学んでいると言えそうですし、実際BERTは分類問題や固有表現抽出などで高い性能を出すことができます.<br>
しかし、言語モデルとしてもともと解きたかった問題</p>
<p>$$
\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x)
$$</p>
<p>についてもうまくいくモデルなのかは謎です.<br>
これについては、<a href="https://arxiv.org/abs/2408.08252">既存研究</a>で以下が成り立つことがわかっています.</p>
<p>$$
\mathbb{E}_{p_{data}(x_0)}[\log p_\theta(x_0)] \geq -\mathcal{L}(\theta).
$$</p>
<p>マスクされた問題に関する損失関数にマイナスを付けた式により下から最大化したい式を抑えられることをあらわしているので、損失関数を小さくできるほど言語モデルの意味で良いモデルで良いモデルと言えます.</p>
<h3 id="fine-tuning">Fine Tuning<a href="#fine-tuning" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><img src="fig2_2.png" alt="Fine Tuningの概要"></p>
<p>事前学習後はFine Tuningをして命令に従えるように学習をしていきます.<br>
データセットとしてプロンプト（指令）と期待される出力のペアが用意されていて、モデルには出力部分の一部をマスクして入力をし、マスクされた部分を予測するように学習させます.</p>
<p>論文で用いているデータセットには以下の問題が含まれているようです.</p>
<ul>
<li>コード生成</li>
<li>数学問題の解答</li>
<li>会話応答</li>
</ul>
<p>この学習により、LLaDAもGPTのような指令に従うような機能を獲得できます.</p>
<h3 id="推論inference">推論（Inference）<a href="#推論inference" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p><img src="fig2_3.png" alt="推論の概要"></p>
<p>ここまで学習しても、困るのが推論です（ちなみに昔紹介したBERTで文を生成する方法の<a href="https://arxiv.org/abs/1902.04094">論文</a>ではトークンを順番にマスクして生成というのを繰り返すギブスサンプリングの形を取っていました）.</p>
<p>LLaDAの推論は以下のようにして徐々にキレイな文章を生成していきます.</p>
<ol>
<li>出力部分をすべてマスク</li>
<li>マスクされたトークンを推論</li>
<li>ランダムなトークンを一部 or 確信度が低いトークンを選択して再度マスク</li>
<li>所定回数まで2～3を繰り返す</li>
</ol>
<p>一部のトークンを固定してそれ以外を再生成といったプロセスを繰り返しますので、ノイズを徐々に取り除いていく拡散モデルのようなことをしているイメージにはなっています.
このように推論することで、文全体を一気に生成するよりも安定した品質が得られているのだと思います.</p>
<p>なお、推論したい部分全体に対して上記の処理をしていく方法の他、semi-autoregressiveという推論したい部分をいくつかのBlockに分けて、左のBlockから上記の処理をおこなっていく方法も提案されています. Blockを小さくしていくと、より自己回帰モデルに近いものになっていきます.</p>
<p><img src="fig4.png" alt="semi-autoregressive"></p>
<h2 id="実験">実験<a href="#実験" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<h3 id="スケーラビリティ">スケーラビリティ<a href="#スケーラビリティ" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>まず投下計算量（モデルのサイズ、学習トークン数）に対して順当に性能が向上するかを確認した結果が以下になります（ちなみにFLOPsはモデルのパラメーター数を$N$、学習データのトークン数を$D$としたときに$6ND$で計算されています）.</p>
<p><img src="fig3.png" alt="スケーラビリティ"></p>
<p>オレンジの提案手法でもスケーラビリティが成り立っており、MMLU（一般知識を問う問題）やGSM8K（算数の文章問題）では自己回帰型モデルよりも優位な結果が得られています.</p>
<h3 id="他モデルとの比較">他モデルとの比較<a href="#他モデルとの比較" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<h4 id="事前学習モデル">事前学習モデル<a href="#事前学習モデル" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<p>以下は事前学習モデルでの各タスクの推論性能をあらわしています.</p>
<p><img src="tab1.png" alt="性能比較"></p>
<p>学習データが異なるのですが、LLaDAはLLaMA2と比較するとほとんどのタスクで上回っています.LLaMA3とはほぼ互角かやや劣る感じですね.とはいえ、拡散言語モデルの可能性を十分に感じさせる結果です.<br>
論文では特に言及がなかったと思うのですが、こうやって比較するとQwenは性能高いですね. 全然勝てていません.</p>
<h4 id="ファインチューニング済モデル">ファインチューニング済モデル<a href="#ファインチューニング済モデル" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<p>ファインチューニングした場合の性能比較した結果は以下のとおりです.</p>
<p><img src="tab2.png" alt="性能比較FT"></p>
<p>ファインチューニングしたもの同士で比較するとLLaMA3には劣っているかなという印象ですが、一部のタスクでは上回ることができています. MMLUでは性能が下がっているのですが、Fine Tuning用のデータセットと合っていない可能性が指摘されています.</p>
<h3 id="reversal-curseへの耐性">Reversal Curseへの耐性<a href="#reversal-curseへの耐性" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>中国語の詩のデータセットを作り、与えられた詩の前後の句を当てる問題を解かせた結果が以下になります.</p>
<p><img src="tab3.png" alt="ReversalCurseの結果"></p>
<p>入力された詩の後の句を当てる問題（Forward）はGPTとQwenは高い精度で当てることができていますが、前の句を当てる問題（Reversal）は半分以下しか当たらなくなります. 一方で、提案手法はあまり性能を落とさずに推論できています（とはいえ少し精度が低くはなっていますので解決とは言いづらいですね）.</p>
<p>論文にはあまり考察がなかったと記憶しているのですが、提案手法はBidirectionalな処理をおこなうため、前後問わずに詩の周辺にはあるトークンの並びが出やすいという傾向を得ることができているのかもしれません.</p>
<h3 id="サンプリング戦略とその影響">サンプリング戦略とその影響<a href="#サンプリング戦略とその影響" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>LLaDAでは以下のようなトークン再マスク戦略が検討されました：</p>
<ul>
<li>ランダム</li>
<li>確信度ベース</li>
<li>Block単位の逐次生成（semi-autoregressive remasking）</li>
</ul>
<p>それらを比較した結果が以下のとおりです.</p>
<p><img src="tab5.png" alt="サンプリング戦略"></p>
<p>Fine Tuningまで考えると、低い確信度を再度マスク &amp; semi-autoregressiveが良いという結果になっています.<br>
semi-autoregressiveを使わずに低い確信度を再度マスクする方法を採用した場合に、ファインチューニング済モデルでは非常に低い性能になっています. ファインチューニングのときに|EOS|トークンでトークン列にパディングをしており、そこの確信度が低いため|EOS|以外の部分のトークンが再マスクされず文の改善がおこなわれないのが理由のようです.
これに対して、semi-autoregressiveを用いて左のBlockから順に生成していくことでパディング部分の悪さを回避できています.</p>
<h3 id="推論のサンプリングのステップ数">推論のサンプリングのステップ数<a href="#推論のサンプリングのステップ数" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>提案手法はサンプリング回数を増やせばより性能が良くなりそうですが、実際どうなのでしょうか？それをあらわしたのが次の図です.</p>
<p><img src="fig5.png" alt="サンプリングステップ数"></p>
<p>横軸が対数スケールなのですが、ステップ数（NFEs）を増やすごとに性能が上がっていくのが分かります. とはいえ1024回もモデルの推論を実行するのは計算コストが大きすぎますね.</p>
<h3 id="推論の過程">推論の過程<a href="#推論の過程" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>推論の過程をあらわしたのが次の図で、色が薄いほど推論のステップのなかで先にトークンが固定され、濃いほどあとの方にトークンが固定されたことをあらわします.</p>
<p><img src="tab4_1.png" alt="推論過程"></p>
<p>少しおもしろいのが答えの72よりも後に埋まっている部分があることです（同じように計算式の答えよりも後に式が埋まっていたりしますね）.<br>
人間の思考とは異なる結果にはなっているのですが、再度サンプリングしても同じトークンが得られている（=実はステップによってあまり改善が必要でない例）ということもあるかと思うので、これだけだとちょっと考察しづらいですね.<br>
ただ、ぼやっとした考えみたいなものを明確にしていっていると捉えると人間に近いとも言えるのかもしれません.</p>
<h2 id="感想">感想<a href="#感想" class="hanchor" ariaLabel="Anchor">#</a> </h2>
<p>LLaDAのアイデア自体はBERTに近く、マスク予測の延長線上にあります.<br>
BERTでも文章が作れるのでは？と思っていましたが、世の中的にはなかなか芽が出ていなかったところにこういった手法が出たことは嬉しく思います.<br>
最近になってようやくこの種の手法がうまくいったのは計算量やデータ量の違いなんでしょうか. 自己回帰モデル型で培われたノウハウのおかげなのかもしれませんね.</p>
<p>Gemini Diffusionは非常に高速で話題になりましたが、今回の提案手法だとサンプリングが多く速度は速くないはずなので、Gemini Diffusionがどうやっているのか気になるところです（実際、論文では推論速度については言及が特にないはずです）.素のモデルの性能が非常に高くてサンプリングステップが少ないのかもしれませんね.</p>

      </div></div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/mblog/bundle.min.js"></script>





  
</div>

</body>
</html>
