<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on MatLoverによるMatlab以外のブログ</title>
    <link>https://opqrstuvcut.github.io/post/</link>
    <description>Recent content in Posts on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 31 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://opqrstuvcut.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OpenAIのPythonモジュール利用で「Unsupported file format」がでる</title>
      <link>https://opqrstuvcut.github.io/posts/openai%E3%81%AEpython%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E5%88%A9%E7%94%A8%E3%81%A7unsupported-file-format%E3%81%8C%E3%81%A7%E3%82%8B/</link>
      <pubDate>Thu, 31 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/openai%E3%81%AEpython%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB%E5%88%A9%E7%94%A8%E3%81%A7unsupported-file-format%E3%81%8C%E3%81%A7%E3%82%8B/</guid>
      <description>&lt;p&gt;下記のように、メモリ上の音声データをBytesIOに書き込み、OpenAIのAPIを実行しようとしました．&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;openai&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;openai&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OpenAI&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BytesIO&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;voice&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seek&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;transcription&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;audio&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transcriptions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-4o-mini-transcribe&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;language&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ja&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;response_format&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;この場合、下記のようなエラーが出てしまいます．&lt;/p&gt;</description>
    </item>
    <item>
      <title>「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介</title>
      <link>https://opqrstuvcut.github.io/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</guid>
      <description>&lt;p&gt;今回紹介するのは&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2506.08008&#34;&gt;Hidden in plain sight: VLMs overlook their visual representations&lt;/a&gt;&#xA;です.&lt;/p&gt;&#xA;&lt;p&gt;テキストの生成というよりも画像が中心となるタスクに対し、オープンソースのVisual Language Modelの性能について調査した論文になっています.&#xA;DINOやCLIPをLLMに組み込んだマルチモーダルモデルは、単体のViT系のモデルよりも性能が&lt;strong&gt;大きく下がる&lt;/strong&gt;ことを示しています.&lt;/p&gt;</description>
    </item>
    <item>
      <title>拡散言語モデルのLLaDA</title>
      <link>https://opqrstuvcut.github.io/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</guid>
      <description>&lt;h1 id=&#34;bertを拡張した生成モデル拡散型llmlladaの概要と可能性&#34;&gt;BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性&lt;/h1&gt;&#xA;&lt;p&gt;2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります.&#xA;提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.&lt;/p&gt;</description>
    </item>
    <item>
      <title>外部知識を活用して効率的に性能向上を達成したYOLO-RD</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</link>
      <pubDate>Sat, 31 May 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</guid>
      <description>&lt;p&gt;YOLO-RD (Retriever-Dictionary) は、物体検出の分野で定番となっているYOLO（You Only Look Once）シリーズの最新研究です.&#xA;今回は、ICLR2025で発表されたYOLO-RD(&lt;a href=&#34;https://arxiv.org/abs/2410.15346&#34;&gt;https://arxiv.org/abs/2410.15346&lt;/a&gt;)について解説します.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Label StudioのAPIを利用したデータ連携のメモ</title>
      <link>https://opqrstuvcut.github.io/posts/label-studio%E3%81%AEapi%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%80%A3%E6%90%BA%E3%81%AE%E3%83%A1%E3%83%A2/</link>
      <pubDate>Thu, 24 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/label-studio%E3%81%AEapi%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%80%A3%E6%90%BA%E3%81%AE%E3%83%A1%E3%83%A2/</guid>
      <description>&lt;p&gt;Label StudioのAPIを利用するとき用のメモになります．下記で出てくる例は物体検出を例にしています．&lt;/p&gt;&#xA;&lt;h2 id=&#34;taskの一覧の取得&#34;&gt;Taskの一覧の取得&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;requests&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_HOST&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;LABEL_STUDIO_HOST&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_PROJECT_ID&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;LABEL_STUDIO_PROJECT_ID&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_TOKEN&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;LABEL_STUDIO_TOKEN&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# page_sizeとpageによって取得できるタスクの範囲が変わるため、必要に応じて変更.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tasks&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requests&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_HOST&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/api/tasks/?project=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_PROJECT_ID&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;amp;page_size=1000&amp;amp;page=1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;headers&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Authorization&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Token &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_TOKEN&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;taskの新規登録&#34;&gt;Taskの新規登録&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;task_info&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;requests&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;post&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_HOST&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/api/tasks/?project=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_PROJECT_ID&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;headers&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Authorization&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Token &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_TOKEN&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;storage_file_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# storage_file_pathはlocalならlocalのパス、Cloud上ならばCloud上のパス.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;project&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LABEL_STUDIO_PROJECT_ID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;file_upload&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# sync済みのdata source上のデータを読み込む場合は1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;pre-annotationの登録&#34;&gt;Pre-Annotationの登録&lt;/h2&gt;&#xA;&lt;p&gt;案件によっては、既存のモデルやシステムからラフなアノテーションが得られるときがあります．このときにはPre-Annotationを利用すると良いでしょう．&lt;/p&gt;</description>
    </item>
    <item>
      <title>回転しているBounding Box向けのIoUのKFIoU</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%9B%9E%E8%BB%A2%E3%81%97%E3%81%A6%E3%81%84%E3%82%8Bbounding-box%E5%90%91%E3%81%91%E3%81%AEiou%E3%81%AEkfiou/</link>
      <pubDate>Sat, 22 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%9B%9E%E8%BB%A2%E3%81%97%E3%81%A6%E3%81%84%E3%82%8Bbounding-box%E5%90%91%E3%81%91%E3%81%AEiou%E3%81%AEkfiou/</guid>
      <description>&lt;h2 id=&#34;従来手法の問題点&#34;&gt;従来手法の問題点&lt;/h2&gt;&#xA;&lt;p&gt;回転しているBounding Box向けの微分可能なIoUの計算というのは簡単ではありません．既存手法としてGWDやKLDがありますが、問題ごとにハイパーパラメータの調整が必要になります．これを解決してより扱いやすく性能が高い手法になったのが&lt;a href=&#34;https://arxiv.org/abs/2201.12558&#34;&gt;KFIoU&lt;/a&gt;になります．&lt;/p&gt;</description>
    </item>
    <item>
      <title>特徴量の次元の柔軟性が高いマトリョーシカ表現学習</title>
      <link>https://opqrstuvcut.github.io/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/</guid>
      <description>&lt;p&gt;一般には、分類問題向けに学習させたディープラーニングモデルから得られる特徴量の次元はあとから変更することはできず、学習のときに固定されてしまいます．&#xA;もしも、学習後に精度をあまり落とさずに次元を小さくできるのであれば、計算リソースやサービスの要求に応じた次元を選択できるため非常に便利です．&#xA;それを実現するための方法として&lt;a href=&#34;https://arxiv.org/abs/2205.13147&#34;&gt;Matryoshka Representation Learning（マトリョーシカ表現学習）&lt;/a&gt;があります．&lt;br&gt;&#xA;なお、マトリョーシカ表現学習はAzureのAI Searchのベクトル検索で利用可能になっています．&lt;/p&gt;</description>
    </item>
    <item>
      <title>WAIC</title>
      <link>https://opqrstuvcut.github.io/posts/waic/</link>
      <pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/waic/</guid>
      <description>&lt;p&gt;かなり前に「渡辺澄夫ベイズ理論100問 with R/Stan」を読み終わったのもあり、忘れないうちにWAICを自分なりにまとめておきます．&lt;/p&gt;&#xA;&lt;h2 id=&#34;waic&#34;&gt;WAIC&lt;/h2&gt;&#xA;&lt;h3 id=&#34;waicの適用範囲&#34;&gt;WAICの適用範囲&lt;/h3&gt;&#xA;&lt;p&gt;情報量基準のAICやTICは正則性を仮定していますが、実際には正則性が成り立たないケースが多いです．&lt;br&gt;&#xA;WAICは正則性を仮定せずとも利用できる情報量基準になっていて、適用範囲が広いです．ただし、どんなケースでもOKかといえばそうではなく、次の条件を満たしている必要があります（「渡辺澄夫ベイズ理論100問 with R/Stan」の(1.21)）．&lt;/p&gt;</description>
    </item>
    <item>
      <title>RT-DETR v2のファインチューニング</title>
      <link>https://opqrstuvcut.github.io/posts/rt-detr-v2%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/</link>
      <pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/rt-detr-v2%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/</guid>
      <description>&lt;p&gt;RT-DETR v2のファインチューニングをおこなったのでメモ。&lt;br&gt;&#xA;レポジトリは &lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR&#34;&gt;https://github.com/lyuwenyu/RT-DETR&lt;/a&gt; を参照してください。 また、本記事ではPyTorch版を前提としています。&lt;/p&gt;&#xA;&lt;h2 id=&#34;手順&#34;&gt;手順&lt;/h2&gt;&#xA;&lt;p&gt;PyTorch版の&lt;a href=&#34;https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetrv2_pytorch&#34;&gt;README&lt;/a&gt;を見てみるとそれほど記載がないですが、結構簡単にファインチューニングが可能になっています。&lt;br&gt;&#xA;おおまかに以下の手順になります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Were RNNs All We Needed?を読んだのでまとめ</title>
      <link>https://opqrstuvcut.github.io/posts/were-rnns-all-we-needed%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%81%BE%E3%81%A8%E3%82%81/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/were-rnns-all-we-needed%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%81%BE%E3%81%A8%E3%82%81/</guid>
      <description>&lt;p&gt;Were RNNs All We Needed?を読んだので、その内容をまとめておきます。&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2410.01201&#34;&gt;https://arxiv.org/abs/2410.01201&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;概要&#34;&gt;概要&lt;/h2&gt;&#xA;&lt;p&gt;Transformerを用いたアーキテクチャの場合、推論に時系列長の二乗に比例した計算量が必要となるため、単純には非常に長い時系列データを扱うことはできません．&lt;br&gt;&#xA;ちょうど一年前くらいにMambaという状態空間モデルベースの手法が提案されており、このMambaならば時系列長に比例した計算量となるため計算量的にはMambaが好ましいです．また学習も効率良くおこなえるうえ、精度的にも良い性能が得られることが分かってきており、有望な手法の1つです．&lt;/p&gt;</description>
    </item>
    <item>
      <title>おすすめの(Neo)Vimプラグイン</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%81%8A%E3%81%99%E3%81%99%E3%82%81%E3%81%AEneovim%E3%83%97%E3%83%A9%E3%82%B0%E3%82%A4%E3%83%B3/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%81%8A%E3%81%99%E3%81%99%E3%82%81%E3%81%AEneovim%E3%83%97%E3%83%A9%E3%82%B0%E3%82%A4%E3%83%B3/</guid>
      <description>&lt;p&gt;NeoVimで利用しているプラグインはたくさんあるのですが、個人的に激推なプラグインを紹介します。&lt;br&gt;&#xA;下記の設定例の記載が特にないプラグインはLazyでプラグインを読み込んでいるだけのものになります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPU周りがおかしくなったときのメモ（ubuntu）</title>
      <link>https://opqrstuvcut.github.io/posts/gpu%E5%91%A8%E3%82%8A%E3%81%8C%E3%81%8A%E3%81%8B%E3%81%97%E3%81%8F%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D%E3%81%AE%E3%83%A1%E3%83%A2ubuntu/</link>
      <pubDate>Thu, 06 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/gpu%E5%91%A8%E3%82%8A%E3%81%8C%E3%81%8A%E3%81%8B%E3%81%97%E3%81%8F%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D%E3%81%AE%E3%83%A1%E3%83%A2ubuntu/</guid>
      <description>&lt;p&gt;いまだにちょくちょくGPU周りの設定がおかしくなることがあるのでメモ。たまに問題が起きる毎にメモが追加されます。&lt;/p&gt;&#xA;&lt;h1 id=&#34;nvidia-smiが遅いubuntuが起動しているのにguiが何も表示されない&#34;&gt;①nvidia-smiが遅い、ubuntuが起動しているのにGUIが何も表示されない&lt;/h1&gt;&#xA;&lt;h2 id=&#34;現象&#34;&gt;現象&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;何も画面に映らなくなる現象です。sshとかはいけます。&lt;/li&gt;&#xA;&lt;li&gt;nvidia-smiを実行すると普通は一瞬でGPUの状況が表示されますが、このケースでは1分以上かかったりします。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;解決方法&#34;&gt;解決方法&lt;/h2&gt;&#xA;&lt;p&gt;cudaを入れ直して再起動したら直りました。原因はよくわかりません。&lt;br&gt;&#xA;aptでcudaを入れている場合は次のような感じです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>magma-nvimで理想に近いVimでのJupyter環境を作る</title>
      <link>https://opqrstuvcut.github.io/posts/magma-nvim%E3%81%A7%E7%90%86%E6%83%B3%E3%81%AB%E8%BF%91%E3%81%84vim%E3%81%A7%E3%81%AEjupyter%E7%92%B0%E5%A2%83%E3%82%92%E4%BD%9C%E3%82%8B/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/magma-nvim%E3%81%A7%E7%90%86%E6%83%B3%E3%81%AB%E8%BF%91%E3%81%84vim%E3%81%A7%E3%81%AEjupyter%E7%92%B0%E5%A2%83%E3%82%92%E4%BD%9C%E3%82%8B/</guid>
      <description>&lt;p&gt;vimで開発しているとブラウザからJupyterを触るのが嫌になってきます。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;開発中にターミナルとブラウザへ移動が面倒&lt;/li&gt;&#xA;&lt;li&gt;vimで使っているformatterとかlinterをそのまま使いたい&lt;/li&gt;&#xA;&lt;li&gt;Jupyter上でもvimキーバインドを設定しているけど、ブラウザのキーバインドと被る&lt;/li&gt;&#xA;&lt;li&gt;vimからIPython上に実行結果を表示する方法もあるが、残念ながらめんどくさかったりで運用面があわない&lt;/li&gt;&#xA;&lt;li&gt;PyCharmやVSCodeを使えばいいじゃんという声が聞こえてきますが、とりあえずPyCharmは気になるところが多くて疲れました&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;前々からvimで完結するようにしたいなぁと思っていたのですが、ようやくそれがある程度実現されてきたので紹介していきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>App Runnerを実戦投入してのメモ</title>
      <link>https://opqrstuvcut.github.io/posts/app-runner%E3%82%92%E5%AE%9F%E6%88%A6%E6%8A%95%E5%85%A5%E3%81%97%E3%81%A6%E3%81%AE%E3%83%A1%E3%83%A2/</link>
      <pubDate>Thu, 25 May 2023 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/app-runner%E3%82%92%E5%AE%9F%E6%88%A6%E6%8A%95%E5%85%A5%E3%81%97%E3%81%A6%E3%81%AE%E3%83%A1%E3%83%A2/</guid>
      <description>&lt;p&gt;簡単にAPIサーバーを用意する方法としてGCPではCloud Run、AWSではApp Runnerが挙げられると思います。&lt;/p&gt;&#xA;&lt;p&gt;今回は最近使ってみたApp Runnerについていくつかメモがてら書いていきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>docker composeでAWS ECSにデプロイするときのtips</title>
      <link>https://opqrstuvcut.github.io/posts/docker-compose%E3%81%A7aws-ecs%E3%81%AB%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AEtips/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/docker-compose%E3%81%A7aws-ecs%E3%81%AB%E3%83%87%E3%83%97%E3%83%AD%E3%82%A4%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AEtips/</guid>
      <description>&lt;p&gt;docker composeを使ってAWSのECSにアプリをデプロイ可能ですが、もしかすると役立つものがあるかもしれないのでメモを残しておきます。&#xA;作業したのが半年前なので少し情報が古いかもしれないのです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>いつの間にかOpenCVのVideoCaptureが正しく向きに対応できるようになっていた</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%81%84%E3%81%A4%E3%81%AE%E9%96%93%E3%81%AB%E3%81%8Bopencv%E3%81%AEvideocapture%E3%81%8C%E6%AD%A3%E3%81%97%E3%81%8F%E5%90%91%E3%81%8D%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%A6%E3%81%84%E3%81%9F/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%81%84%E3%81%A4%E3%81%AE%E9%96%93%E3%81%AB%E3%81%8Bopencv%E3%81%AEvideocapture%E3%81%8C%E6%AD%A3%E3%81%97%E3%81%8F%E5%90%91%E3%81%8D%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%A6%E3%81%84%E3%81%9F/</guid>
      <description>&lt;h1 id=&#34;昔の話&#34;&gt;昔の話&lt;/h1&gt;&#xA;&lt;p&gt;OpenCVのVideoCaptureを使っていると、あれって思うことがありました。&lt;br&gt;&#xA;動画によって、読み込まれたフレームの向きが正しかったり、90度回転していたりするんですよね。特にスマートフォンで撮影した動画で問題が起きていました。&lt;br&gt;&#xA;もちろん、一般的な動画プレーヤーで再生すると正しく表示されるような動画です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>スピアマンの順位相関係数の導出</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%82%B9%E3%83%94%E3%82%A2%E3%83%9E%E3%83%B3%E3%81%AE%E9%A0%86%E4%BD%8D%E7%9B%B8%E9%96%A2%E4%BF%82%E6%95%B0%E3%81%AE%E5%B0%8E%E5%87%BA/</link>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%82%B9%E3%83%94%E3%82%A2%E3%83%9E%E3%83%B3%E3%81%AE%E9%A0%86%E4%BD%8D%E7%9B%B8%E9%96%A2%E4%BF%82%E6%95%B0%E3%81%AE%E5%B0%8E%E5%87%BA/</guid>
      <description>&lt;p&gt;スピアマンの順位相関係数の導出のメモになります。&lt;/p&gt;&#xA;&lt;h1 id=&#34;導出&#34;&gt;導出&lt;/h1&gt;&#xA;&lt;p&gt;$n$個のデータに対する2種類の値をそれぞれ$x_1,\cdots,x_n$と$y_1,\cdots,y_n$とします。&lt;br&gt;&#xA;そして、それらを何らかの方法で並べたときの順位をあらわす関数を$x_i$に対しては$R: \mathbb{R} \rightarrow \mathbb{N}$、$y_i$に対しては$S: \mathbb{R} \rightarrow \mathbb{N}$と定義します。なお、もしも同じ数が与えられたときは、適当に異なる順位をつけるとしておきます。$R$と$S$は順位をあらわす自然数に写す関数であるため全射です。&lt;br&gt;&#xA;また$R(x_1),\cdots,R(x_n)$の平均を$\bar{R}$、標準偏差を$\sigma_R$、$S(y_1),\cdots,S(y_n)$の平均を$\bar{S}$、標準偏差を$\sigma_S$とします。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tabularデータ向けのサーベイ論文を読んだのでメモ</title>
      <link>https://opqrstuvcut.github.io/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/</link>
      <pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/</guid>
      <description>&lt;p&gt;Deep Learning(DL)を用いたテーブルデータ向けの手法は色々提案されており、度々、精度面で勾配ブースティング法を超えたとか超えないと話題になる気がします。&lt;br&gt;&#xA;テーブルデータ周りのDL手法に詳しくない身からすると実際のところどうなのかというのは謎だったので、サーベイ論文を読んでみました。&lt;br&gt;&#xA;読んだ論文：&lt;a href=&#34;https://arxiv.org/abs/2110.01889&#34;&gt;Deep Neural Networks and Tabular Data: A Survey&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>YOLOv5モデルをONNXモデルにして使いたいけど後処理が面倒なとき</title>
      <link>https://opqrstuvcut.github.io/posts/yolov5%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92onnx%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%81%97%E3%81%A6%E4%BD%BF%E3%81%84%E3%81%9F%E3%81%84%E3%81%91%E3%81%A9%E5%BE%8C%E5%87%A6%E7%90%86%E3%81%8C%E9%9D%A2%E5%80%92%E3%81%AA%E3%81%A8%E3%81%8D/</link>
      <pubDate>Sun, 17 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/yolov5%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92onnx%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%81%97%E3%81%A6%E4%BD%BF%E3%81%84%E3%81%9F%E3%81%84%E3%81%91%E3%81%A9%E5%BE%8C%E5%87%A6%E7%90%86%E3%81%8C%E9%9D%A2%E5%80%92%E3%81%AA%E3%81%A8%E3%81%8D/</guid>
      <description>&lt;h1 id=&#34;困ったこと&#34;&gt;困ったこと&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34;&gt;YOLOv5&lt;/a&gt;は便利なライブラリですが、ONNXへモデルを変換したときにちょっと困ったことがあります。&lt;br&gt;&#xA;というのも、変換後のONNXモデルにはNMSなどの後処理が含まれていないため、後処理は別途用意する必要があります。&lt;br&gt;&#xA;公式ではPyTorchの関数を使ったNMSになっているため、そのまま後処理のコードをコピーしようとすれば実行環境上にONNX RuntimeとPyTorchの両方を用意しないといけません。でもせっかくONNXを使うなら、環境にPyTorchを入れたくないですよね。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FastAPI &#43; uvicornの構成のサーバーで時間経過でメモリ使用量が増えるとき</title>
      <link>https://opqrstuvcut.github.io/posts/fastapi--uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/</link>
      <pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/fastapi--uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/</guid>
      <description>&lt;h1 id=&#34;問題発生時の状況&#34;&gt;問題発生時の状況&lt;/h1&gt;&#xA;&lt;p&gt;AWSのECS上にFastAPI + uvicornの構成でのサーバーをたてました。内容的には普通のREST APIです。&lt;br&gt;&#xA;とりあえずは順調に動作していたのですが、たまにコンテナが再起動しているっぽいけどなんだろうと思って調べていたところ、メモリ使用量が次のようになっていました。&lt;br&gt;&#xA;&#xA;&#xA;&#xA;&#x9;&#xA;&#x9;&lt;a href=&#34;https://opqrstuvcut.github.io/posts/fastapi--uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/memory_usage_ng.png&#34;&gt;&#xA;&#x9;&lt;img src=&#34;https://opqrstuvcut.github.io/posts/fastapi--uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/memory_usage_ng_hu_827a181089c8ec1f.png&#34; alt=&#34;メモリ使用量&#34;&gt;&#xA;&#x9;&lt;/a&gt;&#xA;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>NVIDIAのGPUのdriverの更新</title>
      <link>https://opqrstuvcut.github.io/posts/nvidia%E3%81%AEgpu%E3%81%AEdriver%E3%81%AE%E6%9B%B4%E6%96%B0/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/nvidia%E3%81%AEgpu%E3%81%AEdriver%E3%81%AE%E6%9B%B4%E6%96%B0/</guid>
      <description>&lt;h1 id=&#34;nvidiaのgpuのdriver更新手順&#34;&gt;NVIDIAのGPUのdriver更新手順&lt;/h1&gt;&#xA;&lt;p&gt;色々手順はあると思いますが、1つのやり方のメモです。&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;古いドライバを削除しておく&#xA;&lt;ul&gt;&#xA;&lt;li&gt;公式からCUDA Toolkitをダウンロードしてインストールした場合は次で削除できるはず。&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ cd /usr/local/cuda-x/bin &#xA;$ sudo cuda-uninstaller&#xA;$ sudo nvidia-uninstall&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;もしapt-getを使って古いドライバを入れていたら次のコマンドで消え去るはず。nvidia containerが入っている場合はそれも消えるので、嫌な人は注意。&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt-get remove --purge nvidia\* libnvidia-\*&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;CUDA Toolkitのダウンロードとインストール（Installer Typeはrunfileが一番ラク）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CUDA 11.2ならここの手順に従うhttps://developer.nvidia.com/cuda-11.2.1-download-archive&lt;/li&gt;&#xA;&lt;li&gt;最新版のCUDAはここの手順に従うhttps://developer.nvidia.com/cuda-downloads&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;nvidia-smiコマンドを実行して動けばOK&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Individual Conditional Expectation</title>
      <link>https://opqrstuvcut.github.io/posts/individual-conditional-expectation/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/individual-conditional-expectation/</guid>
      <description>&lt;p&gt;Individual Conditional Expectation(ICE)は任意のモデルのある特徴量に対するデータごとの挙動を確認する手法です。&lt;br&gt;&#xA;例えば、ある特定のデータのある特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかを見ます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Partial Dependence Plot</title>
      <link>https://opqrstuvcut.github.io/posts/partial-dependence-plot/</link>
      <pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/partial-dependence-plot/</guid>
      <description>&lt;p&gt;Partial Dependence Plotは任意のモデルのある特徴量に対するglobalな挙動を確認できる手法です。&lt;br&gt;&#xA;例えば、特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかがわかります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWSのDead Letter QueueのメッセージをもとのQueueに戻す</title>
      <link>https://opqrstuvcut.github.io/posts/aws%E3%81%AEdead-letter-queue%E3%81%AE%E3%83%A1%E3%83%83%E3%82%BB%E3%83%BC%E3%82%B8%E3%82%92%E3%82%82%E3%81%A8%E3%81%AEqueue%E3%81%AB%E6%88%BB%E3%81%99/</link>
      <pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/aws%E3%81%AEdead-letter-queue%E3%81%AE%E3%83%A1%E3%83%83%E3%82%BB%E3%83%BC%E3%82%B8%E3%82%92%E3%82%82%E3%81%A8%E3%81%AEqueue%E3%81%AB%E6%88%BB%E3%81%99/</guid>
      <description>&lt;p&gt;AWSのSQSを使うときにちょっと困るのが、アプリの不具合等でDead Letter Queueに送られたメッセージをもとのQueueに戻したいケースです。&#xA;調べた感じでは、通常のAWS CLIでは簡単にはできなさそうです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>CANINEの論文を読んだメモ</title>
      <link>https://opqrstuvcut.github.io/posts/canine%E3%81%AE%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%83%A1%E3%83%A2/</link>
      <pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/canine%E3%81%AE%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%83%A1%E3%83%A2/</guid>
      <description>&lt;p&gt;BERTの系列でCharacterレベルでのembedding手法であるCANINEが提案され、これに似たような手法が盛んになるのではという考えのもと論文を読んだメモを書いておきます。&#xA;CANINEってなんて読むべきなんでしょう？&lt;/p&gt;</description>
    </item>
    <item>
      <title>画像認識モデルの性能をあげるためのTips</title>
      <link>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E3%81%82%E3%81%92%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AEtips/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E3%81%82%E3%81%92%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AEtips/</guid>
      <description>&lt;p&gt;画像分類モデルを作っているときに予測精度をあげるのに役に立ったなぁという方法の一覧のメモです。&#xA;簡単にできるものから順に紹介しているつもりです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>m3u8ファイルとtsファイルのdownload</title>
      <link>https://opqrstuvcut.github.io/posts/m3u8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%A8ts%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%AEdownload/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/m3u8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%A8ts%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%AEdownload/</guid>
      <description>&lt;p&gt;ライブストリーミングや動画の配信するためにm3u8とtsファイルを利用するケースがあります。&#xA;tsファイルは細切れになった小さい動画になっており、m3u8ファイルはそれらの情報をもっているプレイリストになります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>This version of ChromeDriver only supports Chrome version xxとなったとき</title>
      <link>https://opqrstuvcut.github.io/posts/this-version-of-chromedriver-only-supports-chrome-version-xx%E3%81%A8%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/this-version-of-chromedriver-only-supports-chrome-version-xx%E3%81%A8%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D/</guid>
      <description>&lt;p&gt;Google Chromeのバージョンをあげたあと、&lt;a href=&#34;https://github.com/sczhengyabin/Image-Downloader&#34;&gt;ImageDownloader&lt;/a&gt;などを使っているときに次のようなエラーメッセージがでることがあります。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Message: session not created: This version of ChromeDriver only supports Chrome version 86&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最後の86のところはChromeのバージョンによって変わります。&lt;/p&gt;&#xA;&lt;p&gt;これはChromeDriverのバージョンとGoogle Chromeのバージョンが異なっていることによって起きるエラーです。&#xA;そのため、ChromeDriverのバージョンをあげればよいです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sum Treeで重みにそってサンプリングする（Python実装）</title>
      <link>https://opqrstuvcut.github.io/posts/sum-tree%E3%81%A7%E9%87%8D%E3%81%BF%E3%81%AB%E3%81%9D%E3%81%A3%E3%81%A6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8Bpython%E5%AE%9F%E8%A3%85/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/sum-tree%E3%81%A7%E9%87%8D%E3%81%BF%E3%81%AB%E3%81%9D%E3%81%A3%E3%81%A6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8Bpython%E5%AE%9F%E8%A3%85/</guid>
      <description>&lt;h1 id=&#34;問題設定&#34;&gt;問題設定&lt;/h1&gt;&#xA;&lt;p&gt;次のような設定でサンプリングをしたいことはよくあると思います。&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;3つのデータがあり、それぞれに重みがつけられているとする。&#xA;それぞれ、データ1の重みは10、データ2の重みは20、データ3の重みは30である。&#xA;このときに各データの重みと全体の重みの和の比を確率としてサンプリングをしたい。&#xA;つまり、データ1は10/60、データ2は20/60、データ3は30/60の確率でサンプリングすることになる。&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;シンプルな方法&#34;&gt;シンプルな方法&lt;/h1&gt;&#xA;&lt;p&gt;さきほどの問題設定のとき、簡単にサンプリングする方法は正規化された重みの和を順に足していき、一様分布からサンプリングした乱数がその和を超えたときのデータを取得するという方法です。&#xA;手順は次のようになります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>GCPのWorkload IdentityでGKEとGCPサービスとの連携を安全におこなう</title>
      <link>https://opqrstuvcut.github.io/posts/gcp%E3%81%AEworkload-identity%E3%81%A7gke%E3%81%A8gcp%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E3%82%92%E5%AE%89%E5%85%A8%E3%81%AB%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</link>
      <pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/gcp%E3%81%AEworkload-identity%E3%81%A7gke%E3%81%A8gcp%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E3%82%92%E5%AE%89%E5%85%A8%E3%81%AB%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</guid>
      <description>&lt;h1 id=&#34;workload-identityについて&#34;&gt;Workload Identityについて&lt;/h1&gt;&#xA;&lt;p&gt;Compute Engineのホストからは何もせずにGCPのサービスにアクセス可能ですが、GKEを利用しているときに、クラスタ内のコンテナからStorageなどのサービスにどうやってアクセスするかという話がでてきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorBoardのDocker Image</title>
      <link>https://opqrstuvcut.github.io/posts/tensorboard%E3%81%AEdocker-image/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/tensorboard%E3%81%AEdocker-image/</guid>
      <description>&lt;p&gt;たまにTensorBoardを使うときに、ホスト環境などにTensorBoardを入れるより、それ用のコンテナをたてたくなったので、そのメモです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>docker-composeのbuildがはじまらないとき</title>
      <link>https://opqrstuvcut.github.io/posts/docker-compose%E3%81%AEbuild%E3%81%8C%E3%81%AF%E3%81%98%E3%81%BE%E3%82%89%E3%81%AA%E3%81%84%E3%81%A8%E3%81%8D/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/docker-compose%E3%81%AEbuild%E3%81%8C%E3%81%AF%E3%81%98%E3%81%BE%E3%82%89%E3%81%AA%E3%81%84%E3%81%A8%E3%81%8D/</guid>
      <description>&lt;p&gt;最近では実験用の環境なんかもDockerコンテナ上に用意することも多いです。&lt;br&gt;&#xA;docker-composeも使うわけですが、たまにdocker-composeのbuildがいつになってもはじまらないことがあります。&lt;br&gt;&#xA;Building xxxがずっと表示されて、そこから進まないわけです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPUサーバーでのTensorFlow &#43; uWSGIでFailed to get device properties, error code: 3</title>
      <link>https://opqrstuvcut.github.io/posts/gpu%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E3%81%AEtensorflow--uwsgi%E3%81%A7failed-to-get-device-properties-error-code-3/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/gpu%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E3%81%AEtensorflow--uwsgi%E3%81%A7failed-to-get-device-properties-error-code-3/</guid>
      <description>&lt;p&gt;GPUサーバー上でTensorFlowを動かすアプリを作成し、nginxとの間にはuWSGIを挟む構成にしていたところ、次のエラーが出てしまいました。&lt;/p&gt;</description>
    </item>
    <item>
      <title>動画データから前景と背景を分離する</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E5%89%8D%E6%99%AF%E3%81%A8%E8%83%8C%E6%99%AF%E3%82%92%E5%88%86%E9%9B%A2%E3%81%99%E3%82%8B/</link>
      <pubDate>Thu, 20 Aug 2020 11:04:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E5%89%8D%E6%99%AF%E3%81%A8%E8%83%8C%E6%99%AF%E3%82%92%E5%88%86%E9%9B%A2%E3%81%99%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像から前景と背景を分けるのは以前に取り上げたのですが、動画でもOpenCVで前景と背景をわけることが可能です。ここでいう前景は動いている物体を指します。&lt;/p&gt;</description>
    </item>
    <item>
      <title>connectedComponentsで連結した領域の取得</title>
      <link>https://opqrstuvcut.github.io/posts/connectedcomponents%E3%81%A7%E9%80%A3%E7%B5%90%E3%81%97%E3%81%9F%E9%A0%98%E5%9F%9F%E3%81%AE%E5%8F%96%E5%BE%97/</link>
      <pubDate>Tue, 18 Aug 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/connectedcomponents%E3%81%A7%E9%80%A3%E7%B5%90%E3%81%97%E3%81%9F%E9%A0%98%E5%9F%9F%E3%81%AE%E5%8F%96%E5%BE%97/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVでは二値画像から結合している領域の抽出をおこなうことができます。&#xA;こういうのは自分で実装すると大変なので、大変助かりますね。&lt;/p&gt;</description>
    </item>
    <item>
      <title>抽出した輪郭の描画</title>
      <link>https://opqrstuvcut.github.io/posts/%E6%8A%BD%E5%87%BA%E3%81%97%E3%81%9F%E8%BC%AA%E9%83%AD%E3%81%AE%E6%8F%8F%E7%94%BB/</link>
      <pubDate>Mon, 17 Aug 2020 11:03:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E6%8A%BD%E5%87%BA%E3%81%97%E3%81%9F%E8%BC%AA%E9%83%AD%E3%81%AE%E6%8F%8F%E7%94%BB/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVのfindContoursで見つけた輪郭はdrawContoursで簡単に描画できます。&#xA;次のようにして使えます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>findContoursで輪郭の検出</title>
      <link>https://opqrstuvcut.github.io/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/</link>
      <pubDate>Sun, 16 Aug 2020 17:56:36 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像から物体の輪郭を見つけたくなることが多々あります。&#xA;そんなときにもOpenCVを利用することができます。&lt;/p&gt;&#xA;&lt;h1 id=&#34;findcontoursで輪郭抽出&#34;&gt;findContoursで輪郭抽出&lt;/h1&gt;&#xA;&lt;p&gt;次の画像から輪郭の抽出をおこなうことを考えます。&lt;br&gt;&#xA;&lt;img src=&#34;https://opqrstuvcut.github.io/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/5f9b4bcba6cba8859de1bf74287d31b8.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;&#xA;最初に次のように二値化しておきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>テンプレートマッチングで画像から物体をみつける</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E7%89%A9%E4%BD%93%E3%82%92%E3%81%BF%E3%81%A4%E3%81%91%E3%82%8B/</link>
      <pubDate>Sat, 15 Aug 2020 11:09:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E7%89%A9%E4%BD%93%E3%82%92%E3%81%BF%E3%81%A4%E3%81%91%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;カメラを固定しておいて、何らかの被写体を取り続けるということはよくある問題設定です。&#xA;ただし、被写体の位置が毎回少しズレるということも多々あります。&#xA;そんなときにテンプレートマッチングを使うことができます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>minMaxLocで最大と最小の位置を楽に取得</title>
      <link>https://opqrstuvcut.github.io/posts/minmaxloc%E3%81%A7%E6%9C%80%E5%A4%A7%E3%81%A8%E6%9C%80%E5%B0%8F%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%82%92%E6%A5%BD%E3%81%AB%E5%8F%96%E5%BE%97/</link>
      <pubDate>Tue, 11 Aug 2020 11:04:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/minmaxloc%E3%81%A7%E6%9C%80%E5%A4%A7%E3%81%A8%E6%9C%80%E5%B0%8F%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%82%92%E6%A5%BD%E3%81%AB%E5%8F%96%E5%BE%97/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;行列の最大値、最小値はNumPyのmaxやmin、またそれらのインデックスはargmaxやargminを使えば取得できるのですが、OpenCVでは一発ですべて取得できます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>compHistでヒストグラム比較をいろいろなやり方でおこなう</title>
      <link>https://opqrstuvcut.github.io/posts/comphist%E3%81%A7%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AF%94%E8%BC%83%E3%82%92%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E3%82%84%E3%82%8A%E6%96%B9%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</link>
      <pubDate>Mon, 10 Aug 2020 13:20:47 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/comphist%E3%81%A7%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AF%94%E8%BC%83%E3%82%92%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E3%82%84%E3%82%8A%E6%96%B9%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像処理の領域では画像から特徴量をあらわすヒストグラムを生成することがよくあります。&#xA;特徴量としてヒストグラムを生成するということは、比較をすることもよくあるということで、今回はヒストグラムの比較を扱います。&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenCVのヒストグラムの計算はNumPyより断然速い</title>
      <link>https://opqrstuvcut.github.io/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/</link>
      <pubDate>Mon, 10 Aug 2020 11:03:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像処理や集計、機械学習では何かとヒストグラムを計算するケースがありますね。&lt;/p&gt;&#xA;&lt;p&gt;これに伴い、ヒストグラムを計算できるライブラリは色々あるかと思いますが、OpenCVでもヒストグラムを計算する機能をもっています。&#xA;&lt;strong&gt;NumPyでもヒストグラムの計算できるじゃない&lt;/strong&gt;、と思いますが、実はOpenCVの方がNumPyのヒストグラムよりも断然速いです。今回はその辺りの比較もおこなっていきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Grabcutsで背景と猫を分離したい</title>
      <link>https://opqrstuvcut.github.io/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/</link>
      <pubDate>Sun, 09 Aug 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;次のような画像があったとします。&lt;br&gt;&#xA;&lt;img src=&#34;https://opqrstuvcut.github.io/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/b7906bd0d4bbe04cf62dab0eda766889.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;ここから猫だけ抽出したいときに、ツールを使えば少し手間はかかりますが、切り取れると思います。&lt;br&gt;&#xA;実はOpenCVのGrabcutsを使えば非常に簡単にそれが実現できます。&#xA;（ディープラーニング使えばできるよね？はおいておいて）&lt;/p&gt;</description>
    </item>
    <item>
      <title>Watershedで領域検出</title>
      <link>https://opqrstuvcut.github.io/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/</link>
      <pubDate>Sat, 08 Aug 2020 11:10:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Watershedと呼ばれる方法を使うと、指定したマーカーの情報と画像のエッジから画像中の領域の分割をおこなってくれます。&#xA;マーカーとしては、この位置は領域1、この位置は領域2それ以外は背景だよといった感じの情報を与えます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>画像の距離変換をおこなう</title>
      <link>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%AE%E8%B7%9D%E9%9B%A2%E5%A4%89%E6%8F%9B%E3%82%92%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</link>
      <pubDate>Fri, 07 Aug 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%AE%E8%B7%9D%E9%9B%A2%E5%A4%89%E6%8F%9B%E3%82%92%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像に対する距離変換とは、グレースケールの画像において、ピクセルから最も近い0の値をもつピクセルまでの距離を求めたものです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>floodFillで領域に色を塗る</title>
      <link>https://opqrstuvcut.github.io/posts/floodfill%E3%81%A7%E9%A0%98%E5%9F%9F%E3%81%AB%E8%89%B2%E3%82%92%E5%A1%97%E3%82%8B/</link>
      <pubDate>Thu, 06 Aug 2020 11:08:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/floodfill%E3%81%A7%E9%A0%98%E5%9F%9F%E3%81%AB%E8%89%B2%E3%82%92%E5%A1%97%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVのfloodFillを使うことで、選んだ点の周辺の似たような色のピクセルを塗りつぶすことができます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hough変換で円を検出</title>
      <link>https://opqrstuvcut.github.io/posts/hough%E5%A4%89%E6%8F%9B%E3%81%A7%E5%86%86%E3%82%92%E6%A4%9C%E5%87%BA/</link>
      <pubDate>Wed, 05 Aug 2020 11:05:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/hough%E5%A4%89%E6%8F%9B%E3%81%A7%E5%86%86%E3%82%92%E6%A4%9C%E5%87%BA/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Hough変換は直線を検出する方法として前回紹介したのですが、Hough変換を応用することで、円の検出も行えます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hough（ハフ）変換で直線を見つけよう</title>
      <link>https://opqrstuvcut.github.io/posts/hough%E3%83%8F%E3%83%95%E5%A4%89%E6%8F%9B%E3%81%A7%E7%9B%B4%E7%B7%9A%E3%82%92%E8%A6%8B%E3%81%A4%E3%81%91%E3%82%88%E3%81%86/</link>
      <pubDate>Tue, 04 Aug 2020 19:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/hough%E3%83%8F%E3%83%95%E5%A4%89%E6%8F%9B%E3%81%A7%E7%9B%B4%E7%B7%9A%E3%82%92%E8%A6%8B%E3%81%A4%E3%81%91%E3%82%88%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Hough変換は画像から直線をみつける方法です。&lt;/p&gt;&#xA;&lt;h1 id=&#34;簡単な原理&#34;&gt;簡単な原理&lt;/h1&gt;&#xA;&lt;p&gt;入力として2値画像を考えます。&#xA;Hough変換では候補となる直線を用意し、直線上にいくつ0でないピクセルがあるかを数えます。&#xA;このピクセルの個数が指定したしきい値以上であった場合、その候補の直線は正しい直線として扱います。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Canny法でエッジ検出</title>
      <link>https://opqrstuvcut.github.io/posts/canny%E6%B3%95%E3%81%A7%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA/</link>
      <pubDate>Mon, 03 Aug 2020 20:17:14 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/canny%E6%B3%95%E3%81%A7%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;エッジ検出の方法として、Canny法というものがあります。&#xA;SobelフィルタやLaplacianフィルタもエッジ検出ができるわけですが、Canny法を使うとより正確に輪郭を検出することが可能です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ヒストグラム平坦化</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96/</link>
      <pubDate>Sun, 02 Aug 2020 22:50:29 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;今日はヒストグラム平坦化を扱います。&lt;/p&gt;&#xA;&lt;p&gt;ヒストグラム平坦化はコントラストが偏っているような画像を補正します。&#xA;結果として、コントラストがある程度平坦化された結果が得られます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-Local Means Denoisingでノイズ除去</title>
      <link>https://opqrstuvcut.github.io/posts/non-local-means-denoising%E3%81%A7%E3%83%8E%E3%82%A4%E3%82%BA%E9%99%A4%E5%8E%BB/</link>
      <pubDate>Sat, 01 Aug 2020 10:04:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/non-local-means-denoising%E3%81%A7%E3%83%8E%E3%82%A4%E3%82%BA%E9%99%A4%E5%8E%BB/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;non-local-means-denoisingのアイデア&#34;&gt;Non-Local Means Denoisingのアイデア&lt;/h1&gt;&#xA;&lt;p&gt;今回はノイズ除去を扱うのですが、特にガウスノイズを考えます。&#xA;これは平均が0となるノイズですので、着目しているピクセルにある意味で&lt;strong&gt;似ている&lt;/strong&gt;ピクセルを画像中から探してきて、それらの平均を取れば、ノイズの影響が消えたピクセルが得られるはずです。&#xA;これがNon-Local Means Denoisingのアイデアになります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>inpaintで画像の修復をする</title>
      <link>https://opqrstuvcut.github.io/posts/inpaint%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%AE%E5%BE%A9%E3%82%92%E3%81%99%E3%82%8B/</link>
      <pubDate>Fri, 31 Jul 2020 11:04:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/inpaint%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%AE%E5%BE%A9%E3%82%92%E3%81%99%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像に汚れがついたり、傷がついているケースの修復には、最近ではディープラーニングを使った手法が色々出ていますが、画像処理の範囲でもできることがあります。&#xA;今回はOpenCVで修復をおこなってみます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>透過変換で斜めから撮った画像を上から見下ろす</title>
      <link>https://opqrstuvcut.github.io/posts/%E9%80%8F%E9%81%8E%E5%A4%89%E6%8F%9B%E3%81%A7%E6%96%9C%E3%82%81%E3%81%8B%E3%82%89%E6%92%AE%E3%81%A3%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E4%B8%8A%E3%81%8B%E3%82%89%E8%A6%8B%E4%B8%8B%E3%82%8D%E3%81%99/</link>
      <pubDate>Thu, 30 Jul 2020 11:04:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E9%80%8F%E9%81%8E%E5%A4%89%E6%8F%9B%E3%81%A7%E6%96%9C%E3%82%81%E3%81%8B%E3%82%89%E6%92%AE%E3%81%A3%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E4%B8%8A%E3%81%8B%E3%82%89%E8%A6%8B%E4%B8%8B%E3%82%8D%E3%81%99/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;透過変換とは&#34;&gt;透過変換とは？&lt;/h1&gt;&#xA;&lt;p&gt;透過変換はアフィン変換よりも柔軟な変換になっていまして、アフィン変換ではできない台形への変換が可能です。また台形から長方形への変換も可能です。&#xA;つまり、斜めに写っているものを上から見たような感じに変換ができるというわけです。&lt;/p&gt;</description>
    </item>
    <item>
      <title>画像へのアフィン変換</title>
      <link>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%B8%E3%81%AE%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B/</link>
      <pubDate>Wed, 29 Jul 2020 11:03:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%B8%E3%81%AE%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;アフィン変換といえば、普通は2次元上の点や図形を拡大縮小したり、回転したり、平行移動したりといった変換をさします。&#xA;式の話をすると、ある2次元上の点$(x,y)$の$(x&amp;rsquo;, y&amp;rsquo;)$へのアフィン変換は次のようにして表現できます。&#xA;$$\begin{pmatrix}x&amp;rsquo; \\ y&amp;rsquo;  \\ 1 \end{pmatrix} =\begin{pmatrix} a &amp;amp; b &amp;amp; c\\ e &amp;amp; f &amp;amp; g \\ 0 &amp;amp; 0 &amp;amp; 1  \end{pmatrix} \begin{pmatrix}x \\ y \\ 1 \end{pmatrix}.  $$&#xA;$a,b,e,f$の値によって拡大縮小、回転をおこなうようにできますし、$c,g$の値によって平行移動が可能です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>動画の書き込み</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E6%9B%B8%E3%81%8D%E8%BE%BC%E3%81%BF/</link>
      <pubDate>Tue, 28 Jul 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E6%9B%B8%E3%81%8D%E8%BE%BC%E3%81%BF/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;opencvでの動画の書き込み方&#34;&gt;OpenCVでの動画の書き込み方&lt;/h1&gt;&#xA;&lt;p&gt;次のようにしてtest.mp4という名前の動画を作成します。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fourcc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;VideoWriter_fourcc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;4&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;v&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;writer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;VideoWriter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;test.mp4&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                         &lt;span class=&#34;n&#34;&gt;fourcc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                         &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                         &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1920&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1080&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;writer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isOpened&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;第二引数のfourccは動画のコーデックをあらわしており、mp4のときにはcv2.VideoWriter_fourccの引数には&amp;quot;m&amp;quot;, &amp;ldquo;p&amp;rdquo;, &amp;ldquo;4&amp;rdquo;, &amp;ldquo;v&amp;quot;を指定します。他にもmpgで保存するときには&amp;quot;D&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;V&amp;rdquo;, &amp;ldquo;X&amp;quot;を指定したりできます。拡張子に対応してどういうコーデックが指定できるかは、ググっていただくのが良いかと思います。&#xA;また、第三引数にFPSを第四引数に動画の横と縦の大きさを指定しています。&#xA;isOpenedメソッドにより動画を書き込むための準備ができているかを確認できます。FalseのときにはPCがコーデックに対応していなかったりで上手くいっていません。&lt;/p&gt;</description>
    </item>
    <item>
      <title>動画の読みこみ</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E8%AA%AD%E3%81%BF%E3%81%93%E3%81%BF/</link>
      <pubDate>Mon, 27 Jul 2020 22:53:54 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E8%AA%AD%E3%81%BF%E3%81%93%E3%81%BF/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;今日はOpenCVでの動画の読み書きを扱います。&lt;/p&gt;&#xA;&lt;h1 id=&#34;動画の読み込み&#34;&gt;動画の読み込み&lt;/h1&gt;&#xA;&lt;p&gt;動画の読み込みは簡単です。&lt;/p&gt;&#xA;&lt;p&gt;最初に次のように保存されている動画を開きます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>sepFilter2Dで分離可能フィルタを使って高速化</title>
      <link>https://opqrstuvcut.github.io/posts/sepfilter2d%E3%81%A7%E5%88%86%E9%9B%A2%E5%8F%AF%E8%83%BD%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96/</link>
      <pubDate>Sun, 26 Jul 2020 11:06:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/sepfilter2d%E3%81%A7%E5%88%86%E9%9B%A2%E5%8F%AF%E8%83%BD%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVのfilter2Dを使うのは良いのですが、分離可能フィルタのときには&lt;strong&gt;sepFilter2D&lt;/strong&gt;を使うことで、&lt;strong&gt;高速化&lt;/strong&gt;できます。&#xA;今回はこのsepFilter2Dを扱います。&lt;/p&gt;</description>
    </item>
    <item>
      <title>filter2Dで任意のカーネルを扱う</title>
      <link>https://opqrstuvcut.github.io/posts/filter2d%E3%81%A7%E4%BB%BB%E6%84%8F%E3%81%AE%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86/</link>
      <pubDate>Sat, 25 Jul 2020 12:12:06 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/filter2d%E3%81%A7%E4%BB%BB%E6%84%8F%E3%81%AE%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVではいろいろなカーネルによる演算が用意されていますが、自分で定義したカーネルを使いたいこともあります。&#xA;そんなときにはfilter2Dが活躍します。&lt;/p&gt;</description>
    </item>
    <item>
      <title>膨張と収縮の組み合わせによるopeningとclosing</title>
      <link>https://opqrstuvcut.github.io/posts/%E8%86%A8%E5%BC%B5%E3%81%A8%E5%8F%8E%E7%B8%AE%E3%81%AE%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%81%AB%E3%82%88%E3%82%8Bopening%E3%81%A8closing/</link>
      <pubDate>Tue, 21 Jul 2020 22:31:32 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E8%86%A8%E5%BC%B5%E3%81%A8%E5%8F%8E%E7%B8%AE%E3%81%AE%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%81%AB%E3%82%88%E3%82%8Bopening%E3%81%A8closing/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像に対する膨張と収縮の組み合わせによって、openingとclosingという2つの操作が実現できます。&#xA;openingは周辺よりもピクセル値が大きい点を取り除くことができ、closingは周辺よりもピクセル値が小さい点を取り除くことができます。これによってノイズの除去や連結した領域を分割したり、逆に連結させたりできます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>erodeで猫を収縮させる</title>
      <link>https://opqrstuvcut.github.io/posts/erode%E3%81%A7%E7%8C%AB%E3%82%92%E5%8F%8E%E7%B8%AE%E3%81%95%E3%81%9B%E3%82%8B/</link>
      <pubDate>Mon, 20 Jul 2020 23:14:16 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/erode%E3%81%A7%E7%8C%AB%E3%82%92%E5%8F%8E%E7%B8%AE%E3%81%95%E3%81%9B%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;erodeによる収縮&#34;&gt;erodeによる収縮&lt;/h1&gt;&#xA;&lt;p&gt;erodeは指定した局所領域内の最小値を取るような操作になります。&lt;/p&gt;&#xA;&lt;p&gt;具体的な例で説明していきます。&#xA;次のようなピクセル値をもった3×3の画像があったとします。&lt;/p&gt;</description>
    </item>
    <item>
      <title>dilateで猫を膨張させる</title>
      <link>https://opqrstuvcut.github.io/posts/dilate%E3%81%A7%E7%8C%AB%E3%82%92%E8%86%A8%E5%BC%B5%E3%81%95%E3%81%9B%E3%82%8B/</link>
      <pubDate>Sun, 19 Jul 2020 20:40:11 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/dilate%E3%81%A7%E7%8C%AB%E3%82%92%E8%86%A8%E5%BC%B5%E3%81%95%E3%81%9B%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenCVで用意されているdilateを使うことで、画像の中の物体などを膨張させることができます。&#xA;ただ膨張させるだけだとあまり使いみちがあるのかよく分かりませんが、収縮などと組み合わせることで色々な用途があります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Laplacianで画像の2階微分</title>
      <link>https://opqrstuvcut.github.io/posts/laplacian%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE2%E9%9A%8E%E5%BE%AE%E5%88%86/</link>
      <pubDate>Sat, 18 Jul 2020 13:05:29 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/laplacian%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE2%E9%9A%8E%E5%BE%AE%E5%88%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;今回はLaplacianを扱います。&lt;/p&gt;&#xA;&lt;h1 id=&#34;そもそものlaplacian&#34;&gt;そもそものLaplacian&lt;/h1&gt;&#xA;&lt;p&gt;Laplacianの復習的な話ですが、2階偏微分可能な関数$f(x,y)$に対して以下をLaplacianといいます。&#xA;$$ \Delta f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}.  $$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sobelフィルタで微分</title>
      <link>https://opqrstuvcut.github.io/posts/sobel%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%A7%E5%BE%AE%E5%88%86/</link>
      <pubDate>Thu, 16 Jul 2020 14:06:05 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/sobel%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%A7%E5%BE%AE%E5%88%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;よくある画像処理のオペレーターとして、画像の微分があります。&#xA;いくつかやり方はありますが、今日はSobel微分を取り上げます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>imencodeとimdecodeによるメモリ上での画像圧縮</title>
      <link>https://opqrstuvcut.github.io/posts/imencode%E3%81%A8imdecode%E3%81%AB%E3%82%88%E3%82%8B%E3%83%A1%E3%83%A2%E3%83%AA%E4%B8%8A%E3%81%A7%E3%81%AE%E7%94%BB%E5%83%8F%E5%9C%A7%E7%B8%AE/</link>
      <pubDate>Thu, 16 Jul 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/imencode%E3%81%A8imdecode%E3%81%AB%E3%82%88%E3%82%8B%E3%83%A1%E3%83%A2%E3%83%AA%E4%B8%8A%E3%81%A7%E3%81%AE%E7%94%BB%E5%83%8F%E5%9C%A7%E7%B8%AE/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像をpngなどからjpgに変換したいときに、ぱっと思いつくのはファイルを読み込んで、それをjpgの拡張子で書き込みした後に再度読み込みなおすことです。&#xA;1度動かすならばそれでも良いのですが、何度も繰り返しおこなう場合にはファイルの読み書きの時間が気になります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>サンプルコードでなにかとあらわれるガウス平滑化</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AA%E3%81%AB%E3%81%8B%E3%81%A8%E3%81%82%E3%82%89%E3%82%8F%E3%82%8C%E3%82%8B%E3%82%AC%E3%82%A6%E3%82%B9%E5%B9%B3%E6%BB%91%E5%8C%96/</link>
      <pubDate>Tue, 14 Jul 2020 18:30:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AA%E3%81%AB%E3%81%8B%E3%81%A8%E3%81%82%E3%82%89%E3%82%8F%E3%82%8C%E3%82%8B%E3%82%AC%E3%82%A6%E3%82%B9%E5%B9%B3%E6%BB%91%E5%8C%96/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;今日はなにかとサンプルコードで使われるガウス平滑化です。&lt;/p&gt;&#xA;&lt;h1 id=&#34;ガウス平滑化とは&#34;&gt;ガウス平滑化とは&lt;/h1&gt;&#xA;&lt;p&gt;前々回取り上げた単純平滑化は局所領域の平均をとることで、平滑化をおこないました。これは局所領域内の各ピクセルの重み付けがすべて等しいともいえます。&#xA;ガウス平滑化では二次元のガウス分布を離散化した値を重みとして利用するような平滑化になります。&#xA;$$g(x,y) = \frac{1}{2\pi\sqrt{\sigma^2}}\exp\left(-\frac{x^2 + y^2}{\sigma^2}\right).$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>外れ値に強いMedianBlur</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%A4%96%E3%82%8C%E5%80%A4%E3%81%AB%E5%BC%B7%E3%81%84medianblur/</link>
      <pubDate>Mon, 13 Jul 2020 11:00:00 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%A4%96%E3%82%8C%E5%80%A4%E3%81%AB%E5%BC%B7%E3%81%84medianblur/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;単純平滑化の場合には、局所領域内での平均を取るため、周辺とは大きく異なるピクセル値をもつピクセルがあると、その影響が大きすぎて上手くいかない場合があります。&#xA;そのようなケースでは中央値を使うようにすると、上手くいくかもしれません。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AdaptiveThresholdで照明環境が微妙な画像を二値化</title>
      <link>https://opqrstuvcut.github.io/posts/adaptivethreshold%E3%81%A7%E7%85%A7%E6%98%8E%E7%92%B0%E5%A2%83%E3%81%8C%E5%BE%AE%E5%A6%99%E3%81%AA%E7%94%BB%E5%83%8F%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96/</link>
      <pubDate>Sat, 11 Jul 2020 09:22:28 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/adaptivethreshold%E3%81%A7%E7%85%A7%E6%98%8E%E7%92%B0%E5%A2%83%E3%81%8C%E5%BE%AE%E5%A6%99%E3%81%AA%E7%94%BB%E5%83%8F%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;画像処理で結構シビアなのが、照明環境です。&#xA;例えば次の画像のように、画像の中で明暗が異なると、大津の二値化ではうまくいきません。&lt;/p&gt;</description>
    </item>
    <item>
      <title>大津の二値化で楽をする</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%A4%A7%E6%B4%A5%E3%81%AE%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%A7%E6%A5%BD%E3%82%92%E3%81%99%E3%82%8B/</link>
      <pubDate>Fri, 10 Jul 2020 13:08:19 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%A4%A7%E6%B4%A5%E3%81%AE%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%A7%E6%A5%BD%E3%82%92%E3%81%99%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;大津の2値化とは&#34;&gt;大津の2値化とは？&lt;/h1&gt;&#xA;&lt;p&gt;シンプルな二値化では、何かしらのしきい値を決めてあげる必要がありました。&lt;/p&gt;&#xA;&lt;p&gt;人間がグレースケール値のヒストグラムを見てしきい値を決めたり、試行錯誤するというのも良いですが、場合によってはしきい値を自動で決定したくなります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>貧乏人なのでPoor Man’s BERTを読んで解説</title>
      <link>https://opqrstuvcut.github.io/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/</link>
      <pubDate>Sun, 21 Jun 2020 15:22:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;最近自然言語処理をよくやっていて、BERTを使うことも多いです。&#xA;BERTの性能は高く素晴らしいのですが、実際使う上では、私のような計算リソース弱者には辛いところがあります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWSのLambdaからPostgresを利用</title>
      <link>https://opqrstuvcut.github.io/posts/aws%E3%81%AElambda%E3%81%8B%E3%82%89postgres%E3%82%92%E5%88%A9%E7%94%A8/</link>
      <pubDate>Mon, 04 May 2020 13:35:16 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/aws%E3%81%AElambda%E3%81%8B%E3%82%89postgres%E3%82%92%E5%88%A9%E7%94%A8/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;AWSのLambda（Python）からPostgresを利用するためのライブラリの使い方のメモです。何もトラブルなく使えましたが、一応。&#xA;ライブラリのレポジトリは&lt;a href=&#34;https://github.com/jkehler/awslambda-psycopg2&#34;&gt;こちら&lt;/a&gt;です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>関数が上に凸であることの必要十分条件はヘッセ行列が半負定値の証明</title>
      <link>https://opqrstuvcut.github.io/posts/%E9%96%A2%E6%95%B0%E3%81%8C%E4%B8%8A%E3%81%AB%E5%87%B8%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E5%BF%85%E8%A6%81%E5%8D%81%E5%88%86%E6%9D%A1%E4%BB%B6%E3%81%AF%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97%E3%81%8C%E5%8D%8A%E8%B2%A0%E5%AE%9A%E5%80%A4%E3%81%AE%E8%A8%BC%E6%98%8E/</link>
      <pubDate>Wed, 11 Mar 2020 00:08:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E9%96%A2%E6%95%B0%E3%81%8C%E4%B8%8A%E3%81%AB%E5%87%B8%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E5%BF%85%E8%A6%81%E5%8D%81%E5%88%86%E6%9D%A1%E4%BB%B6%E3%81%AF%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97%E3%81%8C%E5%8D%8A%E8%B2%A0%E5%AE%9A%E5%80%A4%E3%81%AE%E8%A8%BC%E6%98%8E/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;関数が上に凸であることの必要十分条件はヘッセ行列が半負定値であることです。ネット上だと日本語でまとまっている文献があんまりないかもと思ったので、今回はこの証明をまとめます。&#xA;なお、関数が下に凸のときにはヘッセ行列は半正定値となります。上に凸の定義を使っているところを下に凸の定義に置き換え、正定値を負定値に置き換えれば、同じ議論が可能です。&#xA;また出てくる関数$f$は暗黙的に定義域で2階微分可能としています。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？</title>
      <link>https://opqrstuvcut.github.io/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/</link>
      <pubDate>Mon, 02 Mar 2020 18:01:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;みんながよく使うKL(Kullback–Leibler) divergenceの話題です。&#xA;KL divergenceといえば2つの確率分布の違いを計算できるやつですね。&#xA;KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。&#xA;今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>画像と自然言語でのマルチモーダルなImageBERT</title>
      <link>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/</link>
      <pubDate>Mon, 24 Feb 2020 19:46:50 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;最近Microsoftから発表されたImageBERTについて紹介します。&lt;br&gt;&#xA;ImageBERTはBERTの入力に自然言語だけではなく、画像も受け付けるようにしたマルチモーダルなモデルです。&#xA;また論文ではモデルのアーキテクチャだけではなく、学習方法にも新たな提案がされています。&lt;br&gt;&#xA;実験ではImage-to-Sentenceでの検索とSentence-to-Imageの検索タスクでSOTAが示されています。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pandasのgroupbyの使い方をまとめる</title>
      <link>https://opqrstuvcut.github.io/posts/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/</link>
      <pubDate>Fri, 14 Feb 2020 12:04:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Pandasのgroupbyについては雰囲気でやっていたところがありますので、ちょっと真面目に使い方を調べてみました。使っているPandasのバージョンは1.0.1です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>PandasのDataFrameを最高に簡単にMarkdownの表として出力</title>
      <link>https://opqrstuvcut.github.io/posts/pandas%E3%81%AEdataframe%E3%82%92%E6%9C%80%E9%AB%98%E3%81%AB%E7%B0%A1%E5%8D%98%E3%81%ABmarkdown%E3%81%AE%E8%A1%A8%E3%81%A8%E3%81%97%E3%81%A6%E5%87%BA%E5%8A%9B/</link>
      <pubDate>Thu, 13 Feb 2020 01:55:35 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/pandas%E3%81%AEdataframe%E3%82%92%E6%9C%80%E9%AB%98%E3%81%AB%E7%B0%A1%E5%8D%98%E3%81%ABmarkdown%E3%81%AE%E8%A1%A8%E3%81%A8%E3%81%97%E3%81%A6%E5%87%BA%E5%8A%9B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Pandas1.0からは次のようにしてDataFrameをMarkdownの表として出力できます。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_markdown&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下のように表示されます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>モデルの予測結果を説明するLIMEの理論</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/</link>
      <pubDate>Wed, 12 Feb 2020 00:23:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;モデルの予測結果を説明する方法として&lt;strong&gt;LIME&lt;/strong&gt;があります。&#xA;LIMEはディープラーニングに限らず、任意のモデルに対して予測結果を適用することができます。&#xA;また手法としては結構有名かと思います。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Uber製の機械学習モデルのデバッグツールManifold</title>
      <link>https://opqrstuvcut.github.io/posts/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/</link>
      <pubDate>Tue, 28 Jan 2020 22:52:36 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Uberが公開している機械学習モデルの予測と特徴量の関係性を可視化するツールである&lt;a href=&#34;https://github.com/uber/manifold#upload-csv-to-demo-app&#34;&gt;Manifold&lt;/a&gt;を紹介します。&lt;/p&gt;&#xA;&lt;h1 id=&#34;manifoldを試す&#34;&gt;Manifoldを試す&lt;/h1&gt;&#xA;&lt;p&gt;Manifoldでできることを見ていきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flutterで吹き出しを作る</title>
      <link>https://opqrstuvcut.github.io/posts/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/</link>
      <pubDate>Tue, 28 Jan 2020 00:29:30 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;吹き出しのライブラリ&#34;&gt;吹き出しのライブラリ&lt;/h1&gt;&#xA;&lt;p&gt;Flutterで吹き出しを出すためのライブラリとして&lt;a href=&#34;https://github.com/vi-k/bubble&#34;&gt;Bubble&lt;/a&gt;があります。こちらを使うと吹き出しを簡単に表示できます。&#xA;もう一つ&lt;a href=&#34;https://github.com/NilsBacke/PHSpeechBubble&#34;&gt;SpeechBubble&lt;/a&gt;というライブラリもありますが、Bubbleのほうが色々オプションが設定できます。&lt;/p&gt;&#xA;&lt;h1 id=&#34;bubble&#34;&gt;Bubble&lt;/h1&gt;&#xA;&lt;p&gt;Bubbleを使うと以下のような吹き出しが簡単に表示できます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Matplotlibの凡例を外側に表示したい人へ</title>
      <link>https://opqrstuvcut.github.io/posts/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/</link>
      <pubDate>Mon, 20 Jan 2020 21:09:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Matplotlibの凡例を外側に出したい人用に色々な例を書いておきます。&lt;/p&gt;&#xA;&lt;p&gt;次のような凡例の位置をいじらずに表示した状態からいじっていきます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pythonのnamedtupleを使おう</title>
      <link>https://opqrstuvcut.github.io/posts/python%E3%81%AEnamedtuple%E3%82%92%E4%BD%BF%E3%81%8A%E3%81%86/</link>
      <pubDate>Mon, 06 Jan 2020 21:57:05 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/python%E3%81%AEnamedtuple%E3%82%92%E4%BD%BF%E3%81%8A%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Pythonのnamedtuple使ってますか？&#xA;案外使っていない方が多いので、ご紹介しておきます。&lt;/p&gt;&#xA;&lt;h1 id=&#34;namedtupleとは&#34;&gt;namedtupleとは？&lt;/h1&gt;&#xA;&lt;p&gt;通常のタプルはインデックス指定でのみ要素を参照します。一方で、NamedTupleはタプルの各要素を&lt;strong&gt;名前&lt;/strong&gt;によって参照できます。&lt;br&gt;&#xA;例えばpというnamedtupleの要素にnameというものがあれば、次のようにして参照できます。&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERTを軽量化したALBERTの概要</title>
      <link>https://opqrstuvcut.github.io/posts/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/</link>
      <pubDate>Sat, 28 Dec 2019 23:36:43 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt;のパラメータの数を減らしたモデルであるALBERTについての概要を書いていきます。&lt;/p&gt;&#xA;&lt;p&gt;参考論文：&lt;a href=&#34;https://arxiv.org/abs/1909.11942&#34;&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;問題意識&#34;&gt;問題意識&lt;/h1&gt;&#xA;&lt;p&gt;2018年に提案されたBERTは自然言語界隈では非常に上手くいった手法です。先程論文の引用数を見たら、もう3000を超えていまして、この数字を見てもよくわかります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ディープラーニングのモデルの特徴量の寄与を求めるDeepLift</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</link>
      <pubDate>Thu, 19 Dec 2019 02:03:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。&lt;/p&gt;</description>
    </item>
    <item>
      <title>FlutterでS3にファイルをアップロードする</title>
      <link>https://opqrstuvcut.github.io/posts/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/</link>
      <pubDate>Sun, 08 Dec 2019 19:04:32 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;FlutterでS3へファイルをアップロードするための公式のライブラリはありませんが、有志によるライブラリ&lt;a href=&#34;https://pub.dev/packages/amazon_s3_cognito&#34;&gt;amazon_s3_cognito&lt;/a&gt;があります。&#xA;今回はこちらの紹介+forkしてちょっと修正したのでよければ使ってねという話になります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ディープラーニング向けの特徴量の寄与を求めるIntegrated Gradientsの解説</title>
      <link>https://opqrstuvcut.github.io/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</link>
      <pubDate>Sun, 08 Dec 2019 16:17:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;機械学習のモデルの出力に対する入力された特徴量の寄与を求める手法の1つに、Integrated Gradientsというものがあります。&#xA;Integrated Gradientsはディープラーニング向けの手法ですが、他のディープラーニング向けの手法では満たしていない公理（性質）をいくつも満たしているという点で優れています。&#xA;今回はそんなIntegrated Gradientsを解説します。&lt;/p&gt;</description>
    </item>
    <item>
      <title>CNNで画像中のピクセルの座標情報を考慮できるCoordConv</title>
      <link>https://opqrstuvcut.github.io/posts/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/</link>
      <pubDate>Sat, 30 Nov 2019 21:57:17 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;CNNの表現能力の高さはすばらしいものがありますが、何でもうまくいくわけではありません。例えば、画像中の位置情報を考慮しないと解けないような問題は、通常のCNNではうまく対応できません（具体的な例はこの後説明します）。&lt;br&gt;&#xA;このような問題に対応した手法としてCoordConvというものがあります。CoordConvは座標情報をCNNのなかに組み込む手法で、これを使うことで解けるようになるケースや性能が大きく改善されるようなケースがあります。また「効くか分からないけど、とりあえず組み込む」ということをしても、デメリットはそれほどありません。&lt;/p&gt;</description>
    </item>
    <item>
      <title>安易に逆行列を数値計算するのはやめよう</title>
      <link>https://opqrstuvcut.github.io/posts/%E5%AE%89%E6%98%93%E3%81%AB%E9%80%86%E8%A1%8C%E5%88%97%E3%82%92%E6%95%B0%E5%80%A4%E8%A8%88%E7%AE%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AF%E3%82%84%E3%82%81%E3%82%88%E3%81%86/</link>
      <pubDate>Fri, 15 Nov 2019 01:35:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/%E5%AE%89%E6%98%93%E3%81%AB%E9%80%86%E8%A1%8C%E5%88%97%E3%82%92%E6%95%B0%E5%80%A4%E8%A8%88%E7%AE%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AF%E3%82%84%E3%82%81%E3%82%88%E3%81%86/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;逆行列を使った計算というのは機械学習ではそれなりに出てきます。&#xA;例えば、最小二乗法では&#xA;$$ x = (X^T X) ^{-1} Xb$$&#xA;の形の式を計算する必要がありますし、正規分布の分散を扱うときにも逆行列が出てきます。&#xA;こういうときにnp.linalg.invを使って逆行列を求めて、その後にベクトルとの積を求めるは簡単にできますから、特に何も考えずにそういうふうにしたくなります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>MinIOでローカルにS3みたいなものを作って開発する</title>
      <link>https://opqrstuvcut.github.io/posts/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/</link>
      <pubDate>Sat, 09 Nov 2019 12:48:01 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;AWSのS3を使うようなシステムを開発するときに、S3と連携する部分だけAWSにつなぐより、ローカルにS3が欲しいなぁってふと思いました。でもそんな都合が良い話があるわけないよなぁ、なんて思ったら実はありました！その名も&lt;strong&gt;MinIO&lt;/strong&gt;。&#xA;今回はMinIOの使い方を簡単にご紹介します。とても簡単です。&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERTでおこなうポケモンの説明文生成</title>
      <link>https://opqrstuvcut.github.io/posts/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/</link>
      <pubDate>Thu, 07 Nov 2019 11:42:23 +0900</pubDate>
      <guid>https://opqrstuvcut.github.io/posts/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;概要&#34;&gt;概要&lt;/h1&gt;&#xA;&lt;p&gt;自然言語界隈では非常によく話題になるBERTですが、BERTを使った文生成を実装してみたので今回はその話をします。BERTの事前学習モデルが文生成のタスクで使えたら、比較的少なめの学習データでもそれっぽく文生成できたりしないかなぁと思ってやってみました。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
