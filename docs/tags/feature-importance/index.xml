<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feature Importance on MatLoverによるMatlab以外のブログ</title>
    <link>http://localhost:1313/mblog/tags/feature-importance/</link>
    <description>Recent content in Feature Importance on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Sun, 08 Dec 2019 16:17:01 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/mblog/tags/feature-importance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ディープラーニング向けの特徴量の寄与を求めるIntegrated Gradientsの解説</title>
      <link>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</link>
      <pubDate>Sun, 08 Dec 2019 16:17:01 +0900</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;機械学習のモデルの出力に対する入力された特徴量の寄与を求める手法の1つに、Integrated Gradientsというものがあります。&#xA;Integrated Gradientsはディープラーニング向けの手法ですが、他のディープラーニング向けの手法では満たしていない公理（性質）をいくつも満たしているという点で優れています。&#xA;今回はそんなIntegrated Gradientsを解説します。&lt;/p&gt;&#xA;&lt;p&gt;参考論文：&lt;a href=&#34;https://arxiv.org/abs/1703.01365&#34;&gt;Axiomatic Attribution for Deep Networks&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;先にbaselineのお話&#34;&gt;先にbaselineのお話&lt;/h1&gt;&#xA;&lt;p&gt;本題に入る前に、大事な考え方であるbaselineを説明しておきます。&lt;/p&gt;&#xA;&lt;p&gt;人間が何か起こったことに対して原因を考えるとき、何かの基準となる事がその人の中にはあり、それに比べ、「ここが良くない」とか「ここが良かったから結果としてこういう結果になったんだな」、と考えるんじゃないでしょうか。&#xA;Integrated Gradientsの場合もその考え方を用います。&#xA;先程の例の基準がbaselineと呼ばれ、画像のタスクでは例えば真っ黒の画像が使われたり、自然言語のタスクではすべてを0にしたembeddingが使われたりします（これは手法によって異なります）。つまり、真っ黒の何も写っていない画像に比べて猫の写った画像はこういう風に異なるから、これは猫の画像と判断したんだな、というように考えていくことになります。&lt;/p&gt;&#xA;&lt;h1 id=&#34;2つの公理&#34;&gt;2つの公理&lt;/h1&gt;&#xA;&lt;p&gt;特徴量の寄与を求める既存手法の中でも勾配を用いた手法というのは多いです。しかしながら、論文中では勾配を用いた既存手法には問題があると指摘しています。&#xA;例えばGuided back-propagationは次のSensitivity(a)を満たしていませんし、DeepLiftはImplementation Invarianceを満たしていません。&lt;/p&gt;&#xA;&lt;h2 id=&#34;sensitivitya&#34;&gt;Sensitivity(a)&lt;/h2&gt;&#xA;&lt;p&gt;Sensitivity(a)の定義は以下のとおりです（ちなみにaと書いてあるのはbもあるということです。詳しく知りたい方は論文を参照ください）。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Sensitivity(a): 入力値に対する出力がbaselineの出力と異なったとき、baselineと異なる値をもつ入力の特徴量の寄与は非ゼロである。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;次のような例を考えると、勾配を用いる手法におけるSensitivity(a)の必要性がわかります。&#xA;$f(x) = 1 - {\rm Relu}(1-x)$というネットワークを考えます。baselineが$x=0$、入力値が$x=2$とします。$f(0)=0$、$f(2)=1$となりますのでbaselineとは出力値が変わっています。しかしながら、$x=2$では勾配が$0$になりますので、例えば「勾配×入力値」で寄与を求める場合、寄与も$0$になります。&#xA;baselineに比べて出力値が変わったのに、寄与が$0$というのはおかしい結果だというのは納得いく話かなと思います。&#xA;このため、Sensitivity(a)は寄与を求める手法として満たすべきものだと著者は主張しています。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/53d896e3bb5aef4b00f65f9615a86e72.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;implementation-invariance&#34;&gt;Implementation Invariance&lt;/h2&gt;&#xA;&lt;p&gt;Implementation Invarianceの定義は以下のとおりです。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Implementation Invariance: 実装方法が異なっていても、同じ入力に対しては求まる寄与値は等しい。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;具体例を次に示します。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Implementation Invarianceの例&lt;/strong&gt;&#xA;例えば勾配${\partial f}/{\partial x}$を計算する手法の場合、この計算は隠れ層の出力$h$を使って、 $$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial h}\frac{\partial h}{\partial x}$$&#xA;とあらわせます。&#xA;勾配を求める際に${\partial f}/{\partial x}$を直接計算しても、連鎖律を使って右辺の計算を用いても結果は一緒になります。&#xA;このケースはImplementation Invarianceを満たします。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Implementation Invarianceではない例&lt;/strong&gt;&#xA;DeepLiftの場合は離散化した勾配を用いて寄与を計算します。&#xA;連続値を扱っている限りは連鎖律が成り立ちますが、離散化すると連鎖律が成り立たなくな**。&#xA;つまり、&#xA;$$ \frac{f(x_1) - f(x_0)}{x_1 - x_0} \neq \frac{f(x_1) - f(x_0)}{h(x_1) - h(x_0)} \frac{h(x_1) - h(x_0)}{x_1 -x_0}$$&#xA;となります。&#xA;このように計算方法（実装方法）によって結果が変わる場合はImplementation Invarianceを満たしません。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
