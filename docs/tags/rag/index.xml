<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RAG on MatLoverによるMatlab以外のブログ</title>
    <link>http://localhost:1313/mblog/tags/rag/</link>
    <description>Recent content in RAG on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 31 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/mblog/tags/rag/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>外部知識を活用して効率的に性能向上を達成したYOLO-RD</title>
      <link>http://localhost:1313/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</link>
      <pubDate>Sat, 31 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</guid>
      <description>&lt;p&gt;YOLO-RD (Retriever-Dictionary) は、物体検出の分野で定番となっているYOLO（You Only Look Once）シリーズの最新研究です.&#xA;今回は、ICLR2025で発表されたYOLO-RD(&lt;a href=&#34;https://arxiv.org/abs/2410.15346&#34;&gt;https://arxiv.org/abs/2410.15346&lt;/a&gt;)について解説します.&lt;/p&gt;&#xA;&lt;h2 id=&#34;従来手法の限界と問題意識&#34;&gt;従来手法の限界と問題意識&lt;/h2&gt;&#xA;&lt;p&gt;現在の画像系のモデルのアプローチは大きくわけて2種類です.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;CNN: 畳み込みによって得られる局所情報を組み合わせて推論&lt;/li&gt;&#xA;&lt;li&gt;Transformer: 画像内の広範な相互作用を活用して推論&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;これらも現在ではかなり高い精度を達成できるようになりましたが、「入力画像とモデル内で計算される情報」しか活用できません（当たり前なのですが）.&lt;br&gt;&#xA;もしRAGのように入力画像に応じて外部からの情報を参照することができれば、モデルの推論性能が向上するのでは？というのがYOLO-RDの発想になります.&lt;br&gt;&#xA;人間も、例えば犬のような動物を見たときに、犬ってこういう形だなぁと連想し、それから確かに目の前の動物は犬だと判断したり、あるいは犬じゃない他の動物かも？と判断していると思います．このように適宜情報を連想するという話に近いのかなと思います．&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig1.png&#34; alt=&#34;YOLO-RDの概要&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;yolo-rdの概要&#34;&gt;YOLO-RDの概要&lt;/h2&gt;&#xA;&lt;p&gt;YOLO-RDは推論時に外部の知識（特徴量）を参照する仕組みを組み込んだYOLOです.&lt;br&gt;&#xA;これにより、モデルが画像からは直接取得できない情報を補い、パラメータ数を大きく増やすことなく高精度化を実現しています.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig2.png&#34; alt=&#34;YOLO-RDの全体像&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;上図はYOLO-RDの全体像をあらわした図です.&#xA;バックボーンから得られた特徴量をもとにして、辞書内の特徴量(atomというようです)ごとの重みを決めるというのをRetrieverでおこないます.&lt;br&gt;&#xA;得られた重みによる重み付き和を辞書内のatomに対して計算し、Neck層に渡すという流れになっています.&lt;/p&gt;&#xA;&lt;p&gt;式であらわすと次のようになっています.&lt;br&gt;&#xA;バックボーンから得られた$(w,h)$の位置のピクセルの特徴量$X_{w,h} \in \mathbb{R}^f$を用いて、&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;Y_{w,h} = \lambda \cdot X_{w,h} + (1 - \lambda) \cdot \sum_{i=1}^N c&amp;rsquo;_{i,w,h} \cdot \alpha_i.&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;を新たな特徴量とします.ここで&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$\alpha_i  \in \mathbb{R}^f$ は2-ノルムが1のatom&lt;/li&gt;&#xA;&lt;li&gt;$c&amp;rsquo;_{i,w,h} \in \mathbb{R}$はatomの取捨選択を表現している係数&lt;/li&gt;&#xA;&lt;li&gt;$\lambda \in [0, 1] $はハイパーパラメータ&lt;/li&gt;&#xA;&lt;li&gt;$N \in \mathbb{N}$は辞書サイズ&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;をあらわします.&lt;/p&gt;&#xA;&lt;h3 id=&#34;retriever&#34;&gt;Retriever&lt;/h3&gt;&#xA;&lt;p&gt;$(w,h)$上のピクセルの$i$番目のatomの係数$c&amp;rsquo;_{i,w,h}$は以下のようにして計算します.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;行列$W^G \in \mathbb{R}^{N \times f}$との内積によってベースとなるatomの重み$Y   \in \mathbb{R}^{N \times W \times H}$を計算&#xA;$$Y = G(X) = W^G \cdot X. $$&lt;/li&gt;&#xA;&lt;li&gt;depthwise convolutionをおこない、近傍ピクセルの情報の取り込み($i$はチャネル)&#xA;$$c_i = E(Y^{(i)}) = W^{E^{(i)}} * Y^{(i)}.$$&lt;/li&gt;&#xA;&lt;li&gt;学習を適切に進めるためにPositional Normalizationを適用（$\gamma,\beta$は学習するパラメーター）&#xA;$$&#xA;\begin{align*}&#xA;c^{\prime}_{i,w,h} &amp;amp;= \frac{c_{i,w,h} - \mu_{c_{w,h}}}{\sqrt{\sigma_{c_{w,h}} + \varepsilon}} \cdot \gamma + \beta,\\&#xA;\mu_{c_{w,h}} &amp;amp;= \frac{1}{N} \sum_{n=1}^N c_{n,w,h},\\&#xA;\sigma_{c_{w,h}} &amp;amp;= \frac{1}{N}\sum_{n=1}^N (c_{w,h} - \mu_{c_{w,h}})^2.&#xA;\end{align*}&#xA;$$&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1と2は一度のconvolutionにまとめることができますが、計算コストのためにこのようになっています.&lt;br&gt;&#xA;また、3についてはatomの重み付き和が$X$と同じになってしまうことを防ぐためにおこなわれています.バックボーンから得られる特徴量が良い特徴量であるという前提にたつと、そこに学習初期にはノイズに近いような値になっている辞書情報を足すことにメリットがありません.そして、辞書情報側のパラメーターをうまく活用してlossを下げるよりも、バックボーンと同じ特徴量がNeck層に渡るほうが早くlossを下げられるため、結果的に辞書が意味をなさなくなります.Normalizationを入れることで、$X$をそのままコピーするような出力が難しくなるため、適切に学習が進むようになります.&lt;/p&gt;</description>
    </item>
    <item>
      <title>特徴量の次元の柔軟性が高いマトリョーシカ表現学習</title>
      <link>http://localhost:1313/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/</guid>
      <description>&lt;p&gt;一般には、分類問題向けに学習させたディープラーニングモデルから得られる特徴量の次元はあとから変更することはできず、学習のときに固定されてしまいます．&#xA;もしも、学習後に精度をあまり落とさずに次元を小さくできるのであれば、計算リソースやサービスの要求に応じた次元を選択できるため非常に便利です．&#xA;それを実現するための方法として&lt;a href=&#34;https://arxiv.org/abs/2205.13147&#34;&gt;Matryoshka Representation Learning（マトリョーシカ表現学習）&lt;/a&gt;があります．&lt;br&gt;&#xA;なお、マトリョーシカ表現学習はAzureのAI Searchのベクトル検索で利用可能になっています．&lt;/p&gt;&#xA;&lt;h2 id=&#34;マトリョーシカ表現学習の概要&#34;&gt;マトリョーシカ表現学習の概要&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/fig1.png&#34; alt=&#34;マトリョーシカ表現学習の概要&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;学習&#34;&gt;学習&lt;/h3&gt;&#xA;&lt;p&gt;一般にはディープラーニングモデルを用いて$L$クラス分類問題を解くとき、入力データ$x$に対してembedding $z \in \mathbb{R}^d $を計算し、それに対して重み$W \in \mathbb{R}^{L \times d}$を用いて&#xA;$$&#xA;W z&#xA;$$&#xA;を計算します。その後、$Wz$の値から分類問題が解けるようにモデルの学習を進めていきます．$z$は$d$次元であることを前提にしていますので、残念ながら学習後に先頭から$m$次元目までを切り取った$z_{1:m}$のようなembeddingの一部をみても良い特徴量にはなりません．&lt;/p&gt;&#xA;&lt;p&gt;マトリョーシカ表現学習(MRL)ではこれを解決するため、事前にembeddingの次元の集合を$\mathcal{M}$を定義しておき（例えば$\mathcal{M}=\{8,16,\cdots,1024, 2048\}$）、各次元ごとにembeddingを切り出してそれを用いて分類問題が解けるように学習をしていきます．&lt;br&gt;&#xA;具体的には、データセットを$\mathcal{D} = \{(x_1,y_1),\cdots,(x_N, y_N) \}$、各$\mathcal{M}$の要素$m$ ごとに用意した重みを$W^{(m)} \in \mathbb{R}^{L \times m}$、スケーリングのパラメータを$c_m$、分類用のLossを$\mathcal{L}$としたとき、以下の最小化問題を解くように学習をおこないます．&#xA;$$&#xA;\begin{align*}&#xA;\min_{\{W^{(m)}\}_{m \in \mathcal{M}}, \theta_F} \frac{1}{N} \sum_{(x_i, y_i) \in \mathcal{D}} \sum_{m \in \mathcal{M}} c_m \cdot \mathcal{L} \left(W^{(m)} \cdot F(x_i;\theta_F)         _{1:m};y_i  \right  )&#xA;\end{align*}&#xA;$$&#xA;ここで、embeddingがモデルのパラメータ$\theta_F$に依存していることを明示するため、$x_i$に対応するembedding $z_i \in \mathbb{R}^d$を$F(x_i;\theta_F)$とあらわしています．&lt;br&gt;&#xA;この式であらわしているように、小さい次元のembeddingでも分類問題が解けるようにすることで、そのような一部のembeddingだけでも良い特徴量になることを期待しています．このように、embeddingがマトリョーシカのように入れ子の形になっているためマトリョーシカ表現学習という名前になっています．&lt;br&gt;&#xA;また、もしかすると$c_m$の調整が少し面倒なのかな？と思いましたが、論文ではすべて$c_m=1$としているようです．&lt;/p&gt;&#xA;&lt;p&gt;この手法で少し気になってくるのは、重み行列$W^{(m)}$をそれぞれの$m$ごとに用意するとメモリ使用量が増えることです．&lt;br&gt;&#xA;これに対しては、共通の重み行列$W$を1つ用意し、$W^{m} = W_{1:m}$のように$W$の1行目から$m$行目までを切り出すという形で定義する方法が提案されています．これをEfficient Matryoshka Representation Learning（MRL-E）と呼んでいます．&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
