<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Integrated Gradients on MatLoverによるMatlab以外のブログ</title>
    <link>http://localhost:1313/mblog/tags/integrated-gradients/</link>
    <description>Recent content in Integrated Gradients on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 19 Dec 2019 02:03:01 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/mblog/tags/integrated-gradients/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ディープラーニングのモデルの特徴量の寄与を求めるDeepLift</title>
      <link>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</link>
      <pubDate>Thu, 19 Dec 2019 02:03:01 +0900</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。&lt;/p&gt;&#xA;&lt;p&gt;参考文献：&lt;a href=&#34;https://arxiv.org/pdf/1704.02685.pdf&#34;&gt;Learning Important Features Through Propagating Activation Differences&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;従来法の問題点&#34;&gt;従来法の問題点&lt;/h1&gt;&#xA;&lt;p&gt;DeepLiftを提案している論文では、以下の2つが従来手法の問題点として挙げられています。&lt;/p&gt;&#xA;&lt;h2 id=&#34;saturation-problem&#34;&gt;saturation problem&lt;/h2&gt;&#xA;&lt;p&gt;saturation problemは勾配が0であるような区間では寄与が0になってしまう問題です。&#xA;従来手法には勾配を利用する手法が多いですが、そのような手法ではsaturation problemが発生してしまいます。&#xA;以下の図をご覧ください。&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/a78fcdb1dc3c5d2431c1ab31da893c9d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;図中の関数は$y = 1 - {\rm ReLU(1 - x)}$で、この関数を1つのネットワークとして考えてみます。&#xA;この関数では$x &amp;lt; 1$では勾配が$1$となり、$x&amp;gt;1$では勾配が$0$になります。&#xA;入力が$x=0$の場合に比べれば、$x=2$の場合は出力値が1だけ大きくなるため、寄与は$x=0$の場合よりも大きくなって欲しいです。しかしながら、寄与=勾配$\times$入力とする寄与の計算方法の場合、&#xA;$x = 0 $では残念ながら寄与が等しく0になってしまいます。&#xA;このようにReLUによって勾配が0になってしまうことは、Integrated Gradientsの提案論文のなかでも同様に問題として挙げられています。&lt;/p&gt;&#xA;&lt;h2 id=&#34;discontinuous-gradients&#34;&gt;discontinuous gradients&lt;/h2&gt;&#xA;&lt;p&gt;2つ目に挙げられている問題がdiscontinuous gradientsです。これも下図をご覧ください。&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/b29d1ab9a442911e96451ac4cccdbc63.png&#34; alt=&#34;&#34;&gt;&#xA;左から、ネットワークをあらわしている関数$y={\rm ReLU(x - 10)}$、その勾配、寄与=勾配$\times $入力です。&#xA;このような関数に対しては計算される寄与値が$x=10$で不連続となり、$x=10$までは寄与が全く無いのに、$x=10$を超えると突然寄与の値が$10$を超えるようになります。&#xA;入力値のちょっとした差で寄与が大きく変わるのは良くないですね。&lt;/p&gt;&#xA;&lt;h1 id=&#34;deeplift&#34;&gt;DeepLift&lt;/h1&gt;&#xA;&lt;p&gt;前述した2つの問題を解決するDeepLiftのアイディアと適用結果について述べていきます。DeepLift以外にも、&lt;a href=&#34;https://qrunch.net/@opqrstuvcut/entries/FKxqQpXc0lhh3LMn&#34;&gt;Integrated Gradients&lt;/a&gt;がこれら2つの問題を解決していますが、求まった寄与が直感的ではない場合があります。このことは適用結果で示します。&lt;/p&gt;&#xA;&lt;p&gt;なお、DeepLiftで利用されているアイディアの1つとして、RevealCancel Ruleというものがありますが、書くのが大変になりそうなので省略します。&lt;/p&gt;&#xA;&lt;h2 id=&#34;deepliftのアイディア&#34;&gt;DeepLiftのアイディア&lt;/h2&gt;&#xA;&lt;p&gt;DeepLiftはIntegrated GradientsやSHAPと同様に、基準となる点を決めておき、そこから入力$x$がどれだけ異なるか、また基準点と$x$のネットワークの出力がどれだけ異なるかをもとにして寄与値を計算していきます。&#xA;この基準となる点を$x_1^0, \cdots, x_n^0$としておきます。&lt;/p&gt;&#xA;&lt;p&gt;ディープラーニングで使われる計算は線形変換と非線形変換の2つに分けられ、DeepLiftではこれによって次のように寄与の計算方法が変わってきます。&lt;/p&gt;&#xA;&lt;h3 id=&#34;linear-rule&#34;&gt;Linear Rule&lt;/h3&gt;&#xA;&lt;p&gt;まず線形変換の方からです。線形変換には全結合層、畳み込み層が該当します。&lt;/p&gt;&#xA;&lt;p&gt;入力（あるいはある隠れ層の出力）$x_1,\cdots, x_n$から次の層のあるニューロン$y$が、重み$w_i$とバイバス$b$を用いて次のようにあらわされるとします。&#xA;$$y =  \sum_{i=1}^N w_i x_i + b$$&#xA;基準点$x_1^0, \cdots, x_n^0$でも同様に&#xA;$$y^0 =  \sum_{i=1}^N w_i x_i^0 + b$$&#xA;となります。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ディープラーニング向けの特徴量の寄与を求めるIntegrated Gradientsの解説</title>
      <link>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</link>
      <pubDate>Sun, 08 Dec 2019 16:17:01 +0900</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;機械学習のモデルの出力に対する入力された特徴量の寄与を求める手法の1つに、Integrated Gradientsというものがあります。&#xA;Integrated Gradientsはディープラーニング向けの手法ですが、他のディープラーニング向けの手法では満たしていない公理（性質）をいくつも満たしているという点で優れています。&#xA;今回はそんなIntegrated Gradientsを解説します。&lt;/p&gt;&#xA;&lt;p&gt;参考論文：&lt;a href=&#34;https://arxiv.org/abs/1703.01365&#34;&gt;Axiomatic Attribution for Deep Networks&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;先にbaselineのお話&#34;&gt;先にbaselineのお話&lt;/h1&gt;&#xA;&lt;p&gt;本題に入る前に、大事な考え方であるbaselineを説明しておきます。&lt;/p&gt;&#xA;&lt;p&gt;人間が何か起こったことに対して原因を考えるとき、何かの基準となる事がその人の中にはあり、それに比べ、「ここが良くない」とか「ここが良かったから結果としてこういう結果になったんだな」、と考えるんじゃないでしょうか。&#xA;Integrated Gradientsの場合もその考え方を用います。&#xA;先程の例の基準がbaselineと呼ばれ、画像のタスクでは例えば真っ黒の画像が使われたり、自然言語のタスクではすべてを0にしたembeddingが使われたりします（これは手法によって異なります）。つまり、真っ黒の何も写っていない画像に比べて猫の写った画像はこういう風に異なるから、これは猫の画像と判断したんだな、というように考えていくことになります。&lt;/p&gt;&#xA;&lt;h1 id=&#34;2つの公理&#34;&gt;2つの公理&lt;/h1&gt;&#xA;&lt;p&gt;特徴量の寄与を求める既存手法の中でも勾配を用いた手法というのは多いです。しかしながら、論文中では勾配を用いた既存手法には問題があると指摘しています。&#xA;例えばGuided back-propagationは次のSensitivity(a)を満たしていませんし、DeepLiftはImplementation Invarianceを満たしていません。&lt;/p&gt;&#xA;&lt;h2 id=&#34;sensitivitya&#34;&gt;Sensitivity(a)&lt;/h2&gt;&#xA;&lt;p&gt;Sensitivity(a)の定義は以下のとおりです（ちなみにaと書いてあるのはbもあるということです。詳しく知りたい方は論文を参照ください）。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Sensitivity(a): 入力値に対する出力がbaselineの出力と異なったとき、baselineと異なる値をもつ入力の特徴量の寄与は非ゼロである。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;次のような例を考えると、勾配を用いる手法におけるSensitivity(a)の必要性がわかります。&#xA;$f(x) = 1 - {\rm Relu}(1-x)$というネットワークを考えます。baselineが$x=0$、入力値が$x=2$とします。$f(0)=0$、$f(2)=1$となりますのでbaselineとは出力値が変わっています。しかしながら、$x=2$では勾配が$0$になりますので、例えば「勾配×入力値」で寄与を求める場合、寄与も$0$になります。&#xA;baselineに比べて出力値が変わったのに、寄与が$0$というのはおかしい結果だというのは納得いく話かなと思います。&#xA;このため、Sensitivity(a)は寄与を求める手法として満たすべきものだと著者は主張しています。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/53d896e3bb5aef4b00f65f9615a86e72.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;implementation-invariance&#34;&gt;Implementation Invariance&lt;/h2&gt;&#xA;&lt;p&gt;Implementation Invarianceの定義は以下のとおりです。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Implementation Invariance: 実装方法が異なっていても、同じ入力に対しては求まる寄与値は等しい。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;具体例を次に示します。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Implementation Invarianceの例&lt;/strong&gt;&#xA;例えば勾配${\partial f}/{\partial x}$を計算する手法の場合、この計算は隠れ層の出力$h$を使って、 $$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial h}\frac{\partial h}{\partial x}$$&#xA;とあらわせます。&#xA;勾配を求める際に${\partial f}/{\partial x}$を直接計算しても、連鎖律を使って右辺の計算を用いても結果は一緒になります。&#xA;このケースはImplementation Invarianceを満たします。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Implementation Invarianceではない例&lt;/strong&gt;&#xA;DeepLiftの場合は離散化した勾配を用いて寄与を計算します。&#xA;連続値を扱っている限りは連鎖律が成り立ちますが、離散化すると連鎖律が成り立たなくな**。&#xA;つまり、&#xA;$$ \frac{f(x_1) - f(x_0)}{x_1 - x_0} \neq \frac{f(x_1) - f(x_0)}{h(x_1) - h(x_0)} \frac{h(x_1) - h(x_0)}{x_1 -x_0}$$&#xA;となります。&#xA;このように計算方法（実装方法）によって結果が変わる場合はImplementation Invarianceを満たしません。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
