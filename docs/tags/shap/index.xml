<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SHAP on MatLoverによるMatlab以外のブログ</title>
    <link>http://localhost:1313/mblog/tags/shap/</link>
    <description>Recent content in SHAP on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 19 Dec 2019 02:03:01 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/mblog/tags/shap/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ディープラーニングのモデルの特徴量の寄与を求めるDeepLift</title>
      <link>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</link>
      <pubDate>Thu, 19 Dec 2019 02:03:01 +0900</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</guid>
      <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。&lt;/p&gt;&#xA;&lt;p&gt;参考文献：&lt;a href=&#34;https://arxiv.org/pdf/1704.02685.pdf&#34;&gt;Learning Important Features Through Propagating Activation Differences&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;従来法の問題点&#34;&gt;従来法の問題点&lt;/h1&gt;&#xA;&lt;p&gt;DeepLiftを提案している論文では、以下の2つが従来手法の問題点として挙げられています。&lt;/p&gt;&#xA;&lt;h2 id=&#34;saturation-problem&#34;&gt;saturation problem&lt;/h2&gt;&#xA;&lt;p&gt;saturation problemは勾配が0であるような区間では寄与が0になってしまう問題です。&#xA;従来手法には勾配を利用する手法が多いですが、そのような手法ではsaturation problemが発生してしまいます。&#xA;以下の図をご覧ください。&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/a78fcdb1dc3c5d2431c1ab31da893c9d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;図中の関数は$y = 1 - {\rm ReLU(1 - x)}$で、この関数を1つのネットワークとして考えてみます。&#xA;この関数では$x &amp;lt; 1$では勾配が$1$となり、$x&amp;gt;1$では勾配が$0$になります。&#xA;入力が$x=0$の場合に比べれば、$x=2$の場合は出力値が1だけ大きくなるため、寄与は$x=0$の場合よりも大きくなって欲しいです。しかしながら、寄与=勾配$\times$入力とする寄与の計算方法の場合、&#xA;$x = 0 $では残念ながら寄与が等しく0になってしまいます。&#xA;このようにReLUによって勾配が0になってしまうことは、Integrated Gradientsの提案論文のなかでも同様に問題として挙げられています。&lt;/p&gt;&#xA;&lt;h2 id=&#34;discontinuous-gradients&#34;&gt;discontinuous gradients&lt;/h2&gt;&#xA;&lt;p&gt;2つ目に挙げられている問題がdiscontinuous gradientsです。これも下図をご覧ください。&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/b29d1ab9a442911e96451ac4cccdbc63.png&#34; alt=&#34;&#34;&gt;&#xA;左から、ネットワークをあらわしている関数$y={\rm ReLU(x - 10)}$、その勾配、寄与=勾配$\times $入力です。&#xA;このような関数に対しては計算される寄与値が$x=10$で不連続となり、$x=10$までは寄与が全く無いのに、$x=10$を超えると突然寄与の値が$10$を超えるようになります。&#xA;入力値のちょっとした差で寄与が大きく変わるのは良くないですね。&lt;/p&gt;&#xA;&lt;h1 id=&#34;deeplift&#34;&gt;DeepLift&lt;/h1&gt;&#xA;&lt;p&gt;前述した2つの問題を解決するDeepLiftのアイディアと適用結果について述べていきます。DeepLift以外にも、&lt;a href=&#34;https://qrunch.net/@opqrstuvcut/entries/FKxqQpXc0lhh3LMn&#34;&gt;Integrated Gradients&lt;/a&gt;がこれら2つの問題を解決していますが、求まった寄与が直感的ではない場合があります。このことは適用結果で示します。&lt;/p&gt;&#xA;&lt;p&gt;なお、DeepLiftで利用されているアイディアの1つとして、RevealCancel Ruleというものがありますが、書くのが大変になりそうなので省略します。&lt;/p&gt;&#xA;&lt;h2 id=&#34;deepliftのアイディア&#34;&gt;DeepLiftのアイディア&lt;/h2&gt;&#xA;&lt;p&gt;DeepLiftはIntegrated GradientsやSHAPと同様に、基準となる点を決めておき、そこから入力$x$がどれだけ異なるか、また基準点と$x$のネットワークの出力がどれだけ異なるかをもとにして寄与値を計算していきます。&#xA;この基準となる点を$x_1^0, \cdots, x_n^0$としておきます。&lt;/p&gt;&#xA;&lt;p&gt;ディープラーニングで使われる計算は線形変換と非線形変換の2つに分けられ、DeepLiftではこれによって次のように寄与の計算方法が変わってきます。&lt;/p&gt;&#xA;&lt;h3 id=&#34;linear-rule&#34;&gt;Linear Rule&lt;/h3&gt;&#xA;&lt;p&gt;まず線形変換の方からです。線形変換には全結合層、畳み込み層が該当します。&lt;/p&gt;&#xA;&lt;p&gt;入力（あるいはある隠れ層の出力）$x_1,\cdots, x_n$から次の層のあるニューロン$y$が、重み$w_i$とバイバス$b$を用いて次のようにあらわされるとします。&#xA;$$y =  \sum_{i=1}^N w_i x_i + b$$&#xA;基準点$x_1^0, \cdots, x_n^0$でも同様に&#xA;$$y^0 =  \sum_{i=1}^N w_i x_i^0 + b$$&#xA;となります。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
