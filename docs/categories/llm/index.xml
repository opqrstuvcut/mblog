<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on MatLoverによるMatlab以外のブログ</title>
    <link>http://localhost:1313/mblog/categories/llm/</link>
    <description>Recent content in LLM on MatLoverによるMatlab以外のブログ</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <lastBuildDate>Mon, 28 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/mblog/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介</title>
      <link>http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</guid>
      <description>&lt;p&gt;今回紹介するのは&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2506.08008&#34;&gt;Hidden in plain sight: VLMs overlook their visual representations&lt;/a&gt;&#xA;です.&lt;/p&gt;&#xA;&lt;p&gt;テキストの生成というよりも画像が中心となるタスクに対し、オープンソースのVisual Language Modelの性能について調査した論文になっています.&#xA;DINOやCLIPをLLMに組み込んだマルチモーダルモデルは、単体のViT系のモデルよりも性能が&lt;strong&gt;大きく下がる&lt;/strong&gt;ことを示しています.&lt;/p&gt;&#xA;&lt;p&gt;例えば、次の図では左の2枚の画像が与えられ、上の画像の「Ref」と書かれている点と同じ点は下の画像のA~Dの4つの点のどれか？というのを当てる問題を解くことを考えます.&lt;br&gt;&#xA;DINOやCLIP単体によって問題を解いたとき、DINOでは80%、CLIPでは60%程度のAccuracyでしたが、VLMを用いるとチャンスレート（適当に答えたときの性能）よりも低くなってしまいます.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig1.png&#34; alt=&#34;性能比較&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;llavaによるvlmの実現&#34;&gt;LLaVAによるVLMの実現&lt;/h2&gt;&#xA;&lt;p&gt;まず、VLMはどのように実現されているかの話になります.&lt;br&gt;&#xA;本論文で扱われている&lt;a href=&#34;https://github.com/TRI-ML/prismatic-vlms&#34;&gt;LLaVA&lt;/a&gt;ではDINOのようなVisualモデルから得られた画像のトークン列をLLMのembeddingの空間にマッピングするようなProjector層を追加しています.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/llava_arch.png&#34; alt=&#34;LLaVAの構成&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;LLaVAでのファインチューニングは次の2段階の処理から構成されるようです.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Projector層単体ののファインチューニング&lt;/li&gt;&#xA;&lt;li&gt;EndToEndのファインチューニング&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;比較実験の方法&#34;&gt;比較実験の方法&lt;/h2&gt;&#xA;&lt;p&gt;LLaVAはEndToEndでファインチューニングしていますが、本論文ではProjector層のファインチューニングした場合のVLMとVisualモデルの比較を中心におこなっています.これは、VLMのVisualモデルの重みを固定しておくことで、VLMとVisualモデル単体との正確な比較をおこなうようにするためです.&lt;br&gt;&#xA;ただし、VLM用にEndToEndでファインチューニングされた既存のオープンソースのVisualモデルでさえも性能悪化の傾向があることを示すため、 QwenやPhi-3などでも一部の実験をおこなっています.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;タスク&#34;&gt;タスク&lt;/h2&gt;&#xA;&lt;p&gt;以下では扱っているタスクの一覧を示します.&lt;br&gt;&#xA;タスクの具体例をあらわす画像は論文のなかのVisualモデルとVLMが間違った例を用いています.&lt;/p&gt;&#xA;&lt;h3 id=&#34;カメラ距離を推定するタスク&#34;&gt;カメラ距離を推定するタスク&lt;/h3&gt;&#xA;&lt;p&gt;どちらのBounding Boxのほうがカメラに近いかを判定するタスク.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/depth_estimation.png&#34; alt=&#34;カメラ距離&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLMにはPromptとBounding Box付きの画像を入力し、どちらのBounding Boxがカメラに近いかを出力させる.&lt;/li&gt;&#xA;&lt;li&gt;Visualモデルはそのままだと解くことができないため、NYUv2という深度を推定するタスクのデータセットを用いてVisualモデルにDPT Headを追加して訓練する.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DPT Headは奥行きの推定の出力部分&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;視覚的な類似箇所を推定するタスク&#34;&gt;視覚的な類似箇所を推定するタスク&lt;/h3&gt;&#xA;&lt;p&gt;2枚の画像から視覚的に似ている箇所を見つけるタスク.&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/semantic_correspondence.png&#34; alt=&#34;類似箇所&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Reference画像に1つの点があり、もう一枚の画像にはA,B,C,Dの4つの点を用意する.&lt;/li&gt;&#xA;&lt;li&gt;VLMにはどの点が対応するかを出力させる.&lt;/li&gt;&#xA;&lt;li&gt;Visualモデルではそのまま解くことはできないため、Reference画像の点の特徴量と一番類似度が高くなる特徴量に対応する点をA~Dから選ぶ.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;機能的な類似箇所を推定するタスク&#34;&gt;機能的な類似箇所を推定するタスク&lt;/h3&gt;&#xA;&lt;p&gt;2枚の画像から機能的に似ている箇所を見つけるタスク.&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/affordance.png&#34; alt=&#34;機能的類似箇所&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;問題の解き方は1つ前のタスクと同じです.&lt;/p&gt;&#xA;&lt;h3 id=&#34;同じ位置を推定するタスク&#34;&gt;同じ位置を推定するタスク&lt;/h3&gt;&#xA;&lt;p&gt;2枚の照明条件や視点が異なる画像から同じ位置を見つけるタスク&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/low_level_match.png&#34; alt=&#34;同定&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;問題の解き方は1つ前のタスクと同じです.&lt;/p&gt;&#xA;&lt;h3 id=&#34;最も似ていない3dオブジェクトを推定するタスク&#34;&gt;最も似ていない3Dオブジェクトを推定するタスク&lt;/h3&gt;&#xA;&lt;p&gt;4枚の画像から4枚の画像から4枚の画像から4枚の画像から最も似ていない3Dオブジェクトの画像を選択するタスク.&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/3d.png&#34; alt=&#34;3D&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VLMは4つのなかで一番似ていない画像を出力させる.&lt;/li&gt;&#xA;&lt;li&gt;VisualモデルはVisualモデルの[CLS]トークン部分の特徴量同士の類似度をもとに選択する.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;似ている画風の絵画を選択するタスク&#34;&gt;似ている画風の絵画を選択するタスク&lt;/h3&gt;&#xA;&lt;p&gt;与えられた画像に似ている画風の絵画を2枚のなかから選択するタスク.&#xA;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/art.png&#34; alt=&#34;art&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VLMは3つの画像を与えて、似ている画風の画像を出力させる.&lt;/li&gt;&#xA;&lt;li&gt;Visualモデルの場合、画像のパッチの各特徴量からグラム行列を作ると画像のstyleを表現できることを利用し、Reference画像と比較対象画像のグラム行列の二乗誤差が小さいものを似ている画風であると選択する.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;実験結果&#34;&gt;実験結果&lt;/h2&gt;&#xA;&lt;h3 id=&#34;visualモデルとprojector層をftしたvlmの性能比較&#34;&gt;Visualモデルとprojector層をFTしたVLMの性能比較&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig2.png&#34; alt=&#34;VisualモデルとVLMの比較&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Visualモデル単体に比べてVLMは性能が悪化し、チャンスレートよりも低くなりうる.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;オープンソースのvlmとvisualモデル部分の性能比較&#34;&gt;オープンソースのVLMとVisualモデル部分の性能比較&lt;/h3&gt;&#xA;&lt;p&gt;他のVLMでのVisualモデルとの性能比較.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig3.png&#34; alt=&#34;オープンソースモデルとの比較&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;この結果でも、基本的にはVisualモデル単体のほうが性能が高い.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;InternVLの3D Objectの問題のように、そもそもVisualモデルの性能が低い場合には改善することもある.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;以上の結果から、VLMは全く入力画像を正しく参照できていないかもしれない&amp;hellip;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>拡散言語モデルのLLaDA</title>
      <link>http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</guid>
      <description>&lt;h1 id=&#34;bertを拡張した生成モデル拡散型llmlladaの概要と可能性&#34;&gt;BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性&lt;/h1&gt;&#xA;&lt;p&gt;2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります.&#xA;提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig1.png&#34; alt=&#34;性能比較top&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;自己回帰型モデルの限界&#34;&gt;自己回帰型モデルの限界&lt;/h2&gt;&#xA;&lt;p&gt;従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;逐次処理のため推論効率が悪い&lt;/li&gt;&#xA;&lt;li&gt;「Reversal Curse」に弱い（参考：&lt;a href=&#34;https://arxiv.org/pdf/2309.12288%EF%BC%89&#34;&gt;THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A”&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/reversal_curse.png&#34; alt=&#34;Reversal Curse&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;従来のllmのアプローチ&#34;&gt;従来のLLMのアプローチ&lt;/h2&gt;&#xA;&lt;p&gt;LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)).&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}).&#xA;$$&lt;/p&gt;&#xA;&lt;h2 id=&#34;lladaのアプローチ拡散モデル型のllm&#34;&gt;LLaDAのアプローチ：拡散モデル型のLLM&lt;/h2&gt;&#xA;&lt;p&gt;LLaDAは、自己回帰ではなく&lt;strong&gt;拡散モデル&lt;/strong&gt;のアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.&lt;/p&gt;&#xA;&lt;h3 id=&#34;事前学習pretraining&#34;&gt;事前学習（Pretraining）&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_1.png&#34; alt=&#34;事前学習の概要&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.&lt;/p&gt;&#xA;&lt;p&gt;損失関数は次の通りです：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right].&#xA;$$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
