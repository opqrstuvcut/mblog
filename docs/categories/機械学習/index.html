<!DOCTYPE html>
<html lang="ja">
<head><script src="/mblog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=mblog/livereload" data-no-instant defer></script>
  
    <title>機械学習 :: MatLoverによるMatlab以外のブログ</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="http://localhost:1313/mblog/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/" />


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LFC5W8DKV1"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-LFC5W8DKV1');
        }
      </script>



  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/main.min.775ac2af004d44c22a6d000fbd1d9af529642f5cef27399d0280d180af2c2e9b.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/menu.min.310d32205bdedd6f43144e3c3273c9deecd238eba5f9108db5ea96ca0cfbe377.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/post.min.ad50c7f4d00e7975918f37fc74c6029e1959a40d66fb5b2c6564a8715e985573.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/syntax.min.e9ab635cf918bc84b901eb65c0b2caa74c9544245e3647c1af5c129896ef276e.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terminal.min.77ee67c1d456ac0c280223661a10b75c7729c59aeb33001424a72a14b363e310.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="http://localhost:1313/mblog/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/mblog/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="ja" />
<meta property="og:type" content="website" />
<meta property="og:title" content="機械学習">
<meta property="og:description" content="" />
<meta property="og:url" content="http://localhost:1313/mblog/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/" />
<meta property="og:site_name" content="MatLoverによるMatlab以外のブログ" />

  <meta property="og:image" content="http://localhost:1313/mblog/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">





  <link href="/mblog/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/index.xml" rel="alternate" type="application/rss+xml" title="MatLoverによるMatlab以外のブログ" />







<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css"
    integrity="sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5"
    crossorigin="anonymous">

<script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js"
    integrity="sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd"
    crossorigin="anonymous"></script>

<script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js"
    integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
            ],
            throwOnError : false
        });
    });
</script>


<style>
  section.article-content h2 {
    background-color: rgb(245, 245, 245);
    padding: 10px;
  }

  :root {
    --ja-font-family: "メイリオ", "Meiryo";
    --base-font-family: "Lato", var(--sys-font-family), var(--ja-font-family),
      sans-serif;
    --body-background: #ffffe0;
  }


</style>


</head>
<body>


<div class="container full">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="http://localhost:1313/mblog/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/mblog/">Home</a></li>
        
      
        
          <li><a href="/mblog/archives/">Archives</a></li>
        
      
        
          <li><a href="/mblog/search/">Search</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/mblog/" >Home</a></li>
        
      
        
          <li><a href="/mblog/archives/" >Archives</a></li>
        
      
        
          <li><a href="/mblog/search/" >Search</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
  <h1>Posts for: #機械学習</h1>
  
  <div class="posts">
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/rt-detr-v2%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">RT-DETR v2のファインチューニング</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-12-23</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/rt-detr/">RT-DETR</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%89%A9%E4%BD%93%E8%AA%8D%E8%AD%98/">物体認識</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/rt-detr-v2%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/feature.png"
    class="post-cover"
    alt="RT-DETR v2のファインチューニング"
    title="Cover Image" />


        <div class="post-content">
          
            <p>RT-DETR v2のファインチューニングをおこなったのでメモ。<br>
レポジトリは <a href="https://github.com/lyuwenyu/RT-DETR">https://github.com/lyuwenyu/RT-DETR</a> を参照してください。 また、本記事ではPyTorch版を前提としています。</p>
<h2 id="手順">手順</h2>
<p>PyTorch版の<a href="https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetrv2_pytorch">README</a>を見てみるとそれほど記載がないですが、結構簡単にファインチューニングが可能になっています。<br>
おおまかに以下の手順になります。</p>
<ol>
<li>学習済みモデルのダウンロード</li>
<li>COCO形式のデータセットを準備</li>
<li>configを用意</li>
<li>学習実行</li>
</ol>
<h3 id="学習済みモデルのダウンロード">学習済みモデルのダウンロード</h3>
<p><a href="https://github.com/lyuwenyu/RT-DETR/tree/main/rtdetrv2_pytorch">README</a>の中から目的のサイズのモデルを選んでダウンロードさせていただきましょう。</p>
<h3 id="coco形式のデータセットを準備">COCO形式のデータセットを準備</h3>
<p>データについてはCOCO形式のデータセットを準備すればOKです。実行していないですが、Pascal VOC形式でも大丈夫なはずです。<br>
また、もし自作のデータセットならば、COCO形式のデータセットを学習と検証用データへ分割をしてjsonとして保存しておくところまではやっておく必要があります。</p>
<h3 id="configを用意">configを用意</h3>
<p>学習時のデータの設定やパラメータなどはすべてyml形式のファイルで管理されています。
以下を設定すればOKです。</p>
<ul>
<li>データセットの設定
<ul>
<li>configs/dataset/coco_detection.ymlをもとに作成する。</li>
<li>COCOデータセットとラベルを同じにしたくないため、remap_mscoco_categoryはFalseにする必要がある。</li>
<li>train_dataloaderとval_dataloaderのimg_folderとann_fileに自分が用意したデータセットのパスを記載する。 img_folderに画像が格納されているディレクトリを指定できるため、jsonファイルに記載している画像ファイルのパスはimg_folderを起点にする形でOK（よくある形式のやつですね）</li>
</ul>
</li>
<li>dataloaderの設定
<ul>
<li>configs/rtdetrv2/include/dataloader.yml をもとに作成する。</li>
<li>transforms部分にaugmentationを記述するので、不要なものを削除したり、逆に必要なものは追加する。ちなみに、複数の画像を合成するタイプのaugmentationが設定されていないが、著者曰く<a href="https://github.com/lyuwenyu/RT-DETR/issues/174">mosaic augmentationでは良い結果が得られなかったとのこと</a>。ただし、関数自体は用意されていて、データセットによってはやはり効果がでる。もしmosaic augmentationを試す場合はtransforms.opsのRandomHorizontalFlipのあとに以下を追加すれば動く。
<pre tabindex="0"><code>    - {type: Mosaic, size: [640, 640]}
</code></pre></li>
</ul>
</li>
<li>基本となる設定
<ul>
<li>configs/rtdetrv2/以下のymlをもとに作成する。Sサイズのモデルならばrtdetrv2_r18vd_120e_coco.ymlを用いる。</li>
<li>__include__に自分で作成したymlを指定する。</li>
<li>epochもここで変更する。</li>
</ul>
</li>
</ul>
<p>ここまでやれば、とりあえずは学習が動くようになります。  細かい設定はコードを見たり挙動を確認しながら修正しましょう。</p>
<h3 id="学習実行">学習実行</h3>
<p>READMEには複数GPUのケースが記載されていますが、もし1GPUならば以下のようにすれば良いです。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span> torchrun tools/train.py -c ./configs/rtdetrv2/rtdetrv2_r18vd_120e_coco_ft.yml -t ./models/rtdetrv2_r18vd_120e_coco_rerun_48.1.pth --use-amp --seed<span class="o">=</span><span class="m">0</span> --summary-dir<span class="o">=</span>output/rtdetrv2_r18vd_120e_coco/xxx
</span></span></code></pre></div><p>学習後は学習済みモデルを用いて推論の実行と結果の保存ができます。用意されているスクリプトだと画像を1枚ずつ処理する必要がありますが、一気に推論できたほうが何かと便利なので、<a href="https://github.com/opqrstuvcut/RT-DETR/blob/main/rtdetrv2_pytorch/references/deploy/rtdetrv2_torch.py">こちら</a>を用いて以下のようにしています。</p>
<pre tabindex="0"><code>python -m references.deploy.rtdetrv2_torch -c ./configs/rtdetrv2/rtdetrv2_r18vd_120e_coco_ft.yml -r ./output/rtdetrv2_r18vd_120e_coco/best.pth --im-dir=./dataset/validation_images/ -d cuda
</code></pre><p>onnxへの変換はREADMEに書かれているとおりなのですが、以下で動きます。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python tools/export_onnx.py -c ./configs/rtdetrv2/rtdetrv2_r18vd_120e_coco_ft.yml  -r ./output/rtdetrv2_r18vd_120e_coco/last.pth  --check
</span></span></code></pre></div><p>onnxへ変換したモデルを用いて<a href="https://github.com/lyuwenyu/RT-DETR/blob/main/rtdetrv2_pytorch/references/deploy/rtdetrv2_onnxruntime.py">ここ</a>のように推論が可能です。
これをみていると、NMSが不要になったおかげでonnxの出力をほぼそのまま使えばよいという形になっており、シンプルかつTorchを入れたくないような場合の本番システムの構築が楽ですね（このコードだとtorchvisionを少し使っていますが、使わない形に置き換え可能ですね）。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/rt-detr-v2%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/were-rnns-all-we-needed%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%81%BE%E3%81%A8%E3%82%81/">Were RNNs All We Needed?を読んだのでまとめ</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-12-01</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/rnn/">RNN</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/mamba/">Mamba</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/transformer/">Transformer</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/lstm/">LSTM</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/gru/">GRU</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/were-rnns-all-we-needed%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%81%BE%E3%81%A8%E3%82%81/feature.png"
    class="post-cover"
    alt="Were RNNs All We Needed?を読んだのでまとめ"
    title="Cover Image" />


        <div class="post-content">
          
            <p>Were RNNs All We Needed?を読んだので、その内容をまとめておきます。<br>
<a href="https://arxiv.org/abs/2410.01201">https://arxiv.org/abs/2410.01201</a></p>
<h2 id="概要">概要</h2>
<p>Transformerを用いたアーキテクチャの場合、推論に時系列長の二乗に比例した計算量が必要となるため、単純には非常に長い時系列データを扱うことはできません．<br>
ちょうど一年前くらいにMambaという状態空間モデルベースの手法が提案されており、このMambaならば時系列長に比例した計算量となるため計算量的にはMambaが好ましいです．また学習も効率良くおこなえるうえ、精度的にも良い性能が得られることが分かってきており、有望な手法の1つです．</p>
<p>旧来のLSTMやGRUといったRNNベースの手法の場合はMambaとも似ているように思えますが、BPTTをおこなって学習をしていく必要があり、この点が長い時系列の学習においてネックとなります．というのも、BPTTでは時系列を遡って順に計算をおこなっていく必要があり、これは時系列長の分だけ深いネットワークを利用しているようなもので、どうしても計算時間が長くなってしまいます．
一方でTransformerはBPTTが不要で、必要な計算は並列化して効率よく学習ができます．<br>
本論文では状態空間モデルベースの手法からインスパイアされた、LSTMやGRUを修正して効率的に学習をおこなえるようにした手法を提案しています．</p>
<h2 id="mamba">Mamba</h2>
<p>Mambaをさらっと説明すると、次のように入力 $x_t \in \mathbb{R}$ と1つ前の時刻の隠れ状態 $h_{t-1} \in \mathbb{R}^n$ を用いて、次の時刻の隠れ状態 $h_t$と出力$ y_t \in \mathbb{R}$ を計算するモデルです．</p>
<p>$$
\begin{align*}
h_t &amp;= A_t h_{t - 1} + B_t x_t \\
y_t &amp;= C_t h_t
\end{align*}.
$$
ここで、 $A_t \in \mathbb{R}^{n\times n}, B_t \in \mathbb{R}^n, C_t \in \mathbb{R}^{1 \times n} $ です．</p>
<p>上記の隠れ状態の更新と出力値の計算方法より、</p>
<ul>
<li>時系列長に比例した計算量で推論が可能</li>
<li>時系列長に依存しないメモリ使用量</li>
</ul>
<p>とわかります．</p>
<h3 id="選択メカニズム">選択メカニズム</h3>
<p>Mambaの大事なポイントとして、係数$A_t \in \mathbb{R}^{n\times n}, B_t \in \mathbb{R}^n, C_t \in \mathbb{R}^{1 \times n} $ は入力$ x_t $に依存する値です．これにより、Selective Copying TaskとInduction Heads Taskを解けるようになっています．これは隠れ状態へ入力に関する情報を取り込むかどうか、あるいは隠れ状態の情報を捨てるかを入力値に依存して動的に変えることでモデルの性能が高まったということを意味します．<br>
また、これはRNNのゲート機構を内包した形になっていることも論文中で示されています．<br>
詳しくは<a href="https://arxiv.org/abs/2312.00752">Mambaの論文</a>を参照して下さい．</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/were-rnns-all-we-needed%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%81%BE%E3%81%A8%E3%82%81/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/">Tabularデータ向けのサーベイ論文を読んだのでメモ</a>
        </h1>
        <div class="post-meta"><time class="post-date">2022-07-17</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/table/">Table</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/tabular/">Tabular</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/deep-learning/">Deep Learning</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/">深層学習</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/tabnet/">TabNet</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/tabtransformer/">TabTransformer</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig3.png"
    class="post-cover"
    alt="Tabularデータ向けのサーベイ論文を読んだのでメモ"
    title="Cover Image" />


        <div class="post-content">
          
            <p>Deep Learning(DL)を用いたテーブルデータ向けの手法は色々提案されており、度々、精度面で勾配ブースティング法を超えたとか超えないと話題になる気がします。<br>
テーブルデータ周りのDL手法に詳しくない身からすると実際のところどうなのかというのは謎だったので、サーベイ論文を読んでみました。<br>
読んだ論文：<a href="https://arxiv.org/abs/2110.01889">Deep Neural Networks and Tabular Data: A Survey</a></p>
<p>手法の細かい説明をまとめるのはしんどいので省略して、結果の部分だけのメモになります。</p>
<h1 id="評価値での比較">評価値での比較</h1>
<p>下図は各手法のデータセットごとの評価値の比較結果をあらわしています。上部は非DL手法で、下部DL手法になります。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/table5.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/table5_hu_fa34dfc2022dd946.png" alt="評価値での比較">
	</a>


<p>これをみると、だいたいのデータセットに対してDL手法よりもXGBoostやLightGBM、CatBoostといった勾配ブースティング法が勝っていることがわかります。ただし、HIGGSデータセットではDL手法であるSAINTが他手法に勝っています。<br>
HIGGSデータセットはシミュレーションによって作成されたデータセットであり、データ数は1100万という巨大なものになります。巨大なデータセットに限ってはDeep Learning手法が有利になるのかもしれません。</p>
<h1 id="accuracyと計算時間比較">Accuracyと計算時間比較</h1>
<p>次にAccuracyと計算時間(訓練と推論)の比較になります。DL手法と勾配ブースティングはGPU利用のようです。</p>
<h2 id="adultデータセット">Adultデータセット</h2>
<p>下図はAdultデータセットの場合をあわらしています。図中で左上にある手法ほど良く、右下に近いほど良くない手法という見方になります。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig3.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig3_hu_cf9ce6a8d9eff8fe.png" alt="Adultデータセットでの計算時間とAccuracy">
	</a>


<p>これをみると、訓練と推論の両方で左上に書かれている決定木はバランスが良いです。
Accuracyを優先するならXGBoostやCatBoostといった選択肢があるという結果になっています（LightGBMはどこにいったのか？）。<br>
DL手法で比較的良いのはDeepFMといえるでしょうか。</p>
<h2 id="higgsデータセット">HIGGSデータセット</h2>
<p>下図はHIGGSデータセットの場合をあらわしています。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig4.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig4_hu_209adededba950e8.png" alt="HIGGSデータセットでの計算時間とAccuracy">
	</a>


<p>訓練はXGBoostやCatBoostが良いのですが、推論に比較的時間がかかるという結果になっています。このデータセットに対しては深い木になっているのかもしれません。<br>
主観ですが、訓練と推論の両方でバランスが取れているのはMLP、DeepFMでしょうか？<br>
Accuracyを求めるならSAINTですが、他手法よりも計算時間が多めです。</p>
<h1 id="accuracyとモデルサイズ比較">Accuracyとモデルサイズ比較</h1>
<p>Adultデータセットの場合のDeep LearningモデルのモデルサイズとAccuracyの比較になります。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig5.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig5_hu_1984611cb8fcade0.png" alt="DLモデルのサイズとAccuracy">
	</a>


<p>結果をみると、MLP、TabNet、DeepFMあたりが良いバランスでしょうか。<br>
ここでもSAINTはAccuracyが高めですが、同程度のAccuracyのDeepFMと比べるとモデルサイズが2桁近く大きくなっています。実運用上はモデルサイズは非常に大事でクラウドで動かすときには料金に直結しうるため、場合によっては使用するのが難しいかもしれません。</p>
<h1 id="ディープラーニングモデルの特徴量の分析">ディープラーニングモデルの特徴量の分析</h1>
<h2 id="ablation-test">Ablation Test</h2>
<p>次にディープラーニング手法のAttentionから得られる特徴量の寄与についての分析結果になります。<br>
下記の上部の図(a)は寄与が大きい特徴量から順に削除・モデルを学習・評価というプロセスを繰り返したときのAccuracyの推移をあらわしています。<br>
逆に下部の図(b)は寄与が小さい特徴量から順に削除していったケースをあらわします。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig6.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/fig6_hu_b31b1255af9bb541.png" alt="特徴量の寄与">
	</a>


<p>(a)の場合には寄与が大きい特徴量を順に削除していくため、<strong>本当に寄与が高ければ</strong>すぐにAccuracyが落ちるはずです。
実際にはすぐにガクッとAccuracyが落ちていくことはなく、いくつか特徴量を削除してからようやくAccuracyが下がっていきます。<br>
図中の手法のなかでは比較的TabNetのAccuracyがはやく落ちています。</p>
<p>(b)の場合には寄与が小さい特徴量を順に削除していくため、あまりAccuracyが落ちていかないことが予想されます。
ここでもTabNetが他手法よりも想定に近い挙動をしています。</p>
<p><strong>以上から、比較的TabNetの寄与は信頼できるといえそうですが、全体的にはあまり予想通りの挙動ではないという印象です。</strong></p>
<h2 id="shapとの相関">SHAPとの相関</h2>
<p>最後にDL手法から求まった特徴量の寄与とSHAP値（SHAPから求まった特徴量の寄与）との相関になります。
SHAPは理論的にきちんとしている数少ない（唯一？）寄与の求め方になります。</p>
<p>もしDL手法から求まった特徴量の寄与が良いものであれば、SHAP値との相関が高くなることが予想されます。<br>
2つの値はスケールが異なる都合、相関の計算にはスピアマンの順位相関係数を用いています。これは-1から1の範囲の値を取り、1は特徴量を寄与が高い順に並べた結果が全く同じ、-1は逆順、0は全く似ていないという結果をあらわします。</p>



	
	<a href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/table6.png">
	<img src="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/table6_hu_208f7f6f8cd3020f.png" alt="SHAP値とDLモデルから算出された寄与の関係性">
	</a>


<p>上の表をみると、ほとんど値が0ですので、DL手法で求まる寄与とSHAP値には<strong>ほぼほぼ相関がない</strong>ということがわかります。<br>
SHAP値の計算には時間が結構かかりますので、DL手法から求まる寄与がSHAP値に類似すると大変好都合なのですが、そうはならず残念です。</p>
<h1 id="個人的な結論">個人的な結論</h1>
<p>ここまでの話を踏まえた上で、以下の理由からテーブルデータに対しては基本は決定木系の手法を使ってみるでOKという結論です。</p>
<ul>
<li>高いAccuracy</li>
<li>訓練、推論の両方が比較的速い</li>
<li>GPUが必須ではない</li>
<li>SHAP値が厳密に高速に求まる</li>
</ul>
<p>ただし、データが非常に大きかったり、マルチモーダルなデータ、テーブルデータのaugmentation、またコンペでのスタッキングなどのアンサンブル（実運用でやるのは稀かと思いますが）では活用されると思います。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/tabular%E3%83%87%E3%83%BC%E3%82%BF%E5%90%91%E3%81%91%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%99%E3%82%A4%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%81%AE%E3%81%A7%E3%83%A1%E3%83%A2/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/yolov5%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92onnx%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%81%97%E3%81%A6%E4%BD%BF%E3%81%84%E3%81%9F%E3%81%84%E3%81%91%E3%81%A9%E5%BE%8C%E5%87%A6%E7%90%86%E3%81%8C%E9%9D%A2%E5%80%92%E3%81%AA%E3%81%A8%E3%81%8D/">YOLOv5モデルをONNXモデルにして使いたいけど後処理が面倒なとき</a>
        </h1>
        <div class="post-meta"><time class="post-date">2022-07-17</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/yolo/">YOLO</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/yolov5/">YOLOv5</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/onnx/">ONNX</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/onnx-runtime/">ONNX Runtime</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/object-detection/">Object Detection</a>&nbsp;
            
          </span>
        

        


        <div class="post-content">
          
            <h1 id="困ったこと">困ったこと</h1>
<p><a href="https://github.com/ultralytics/yolov5">YOLOv5</a>は便利なライブラリですが、ONNXへモデルを変換したときにちょっと困ったことがあります。<br>
というのも、変換後のONNXモデルにはNMSなどの後処理が含まれていないため、後処理は別途用意する必要があります。<br>
公式ではPyTorchの関数を使ったNMSになっているため、そのまま後処理のコードをコピーしようとすれば実行環境上にONNX RuntimeとPyTorchの両方を用意しないといけません。でもせっかくONNXを使うなら、環境にPyTorchを入れたくないですよね。</p>
<h1 id="解決方法">解決方法</h1>
<p>PyTorchを入れたくないけどどうしよう…と困っていたところ、こちらのプルリクを見つけました。<br>
<a href="https://github.com/ultralytics/yolov5/pull/7736">https://github.com/ultralytics/yolov5/pull/7736</a><br>
どうやらNMSの処理がONNXモデルに含まれるような修正をおこなっているようです。</p>
<p>2022/07/17現在はまだmasterへはマージされていないのですが、<a href="https://github.com/triple-Mu/yolov5/tree/trtNMS">fork先のブランチ</a>を試してみると、うまくいくことが確認できました。<br>
実際にONNXモデルへ変換をおこなうときにはexport.pyに&quot;&ndash;nms&quot;オプションをつければOKです。
モデルの出力値の扱いは<a href="https://github.com/triple-Mu/yolov5/blob/trtNMS/onnx_nms_ort.ipynb">こちら</a>を参考にすると分かるかと思います。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/yolov5%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92onnx%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%81%97%E3%81%A6%E4%BD%BF%E3%81%84%E3%81%9F%E3%81%84%E3%81%91%E3%81%A9%E5%BE%8C%E5%87%A6%E7%90%86%E3%81%8C%E9%9D%A2%E5%80%92%E3%81%AA%E3%81%A8%E3%81%8D/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/individual-conditional-expectation/">Individual Conditional Expectation</a>
        </h1>
        <div class="post-meta"><time class="post-date">2021-10-19</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/individual-conditional-expectation/">Individual Conditional Expectation</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/">機械学習</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E8%AA%AC%E6%98%8E%E5%8F%AF%E8%83%BDai/">説明可能AI</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/individual-conditional-expectation/ice_age.png"
    class="post-cover"
    alt="Individual Conditional Expectation"
    title="Cover Image" />


        <div class="post-content">
          
            <p>Individual Conditional Expectation(ICE)は任意のモデルのある特徴量に対するデータごとの挙動を確認する手法です。<br>
例えば、ある特定のデータのある特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかを見ます。</p>
<p>PDPの記事を先に見ると、理解がはやいかと思います。<br>
<a href="https://opqrstuvcut.github.io/mblog/posts/partial-dependence-plot/">Partial Dependence Plotの解説記事</a></p>
<h1 id="individual-conditional-expectationの概要">Individual Conditional Expectationの概要</h1>
<p>ICEは冒頭に述べたとおりなので、あまり細かい話をする必要がないのですが、Partial Dependence Plot(PDP)との違いを述べておきます。<br>
PDPはデータの集合の全体に対して、ある特徴量の値を順に変化させていき、そのときのモデルの出力の平均値をみる方法でした。<br>
一方で、ICEはモデルの出力の平均値を取らず、データごとに変化をみます。そのため、PDPだと一本の曲線がプロットできますが、ICEではデータの数だけ曲線がプロットできます。</p>
<h1 id="individual-conditional-expectationはの実験">Individual Conditional Expectationはの実験</h1>
<p>kaggleのtitanicの問題でIndividual Conditional Expectationを試してみます。
モデルはLightGBMの勾配ブースティング法を利用しています。</p>
<h2 id="年齢に対するindividual-conditional-expectation">年齢に対するIndividual Conditional Expectation</h2>
<p>年齢を$0,5,10,\cdots,65$と変化させてみた結果が以下のとおりです。縦軸はタイタニックに乗った乗客の生存確率の予測値です。1つ1つの曲線が1つの乗客に対応します。<br>
<img src="/mblog/posts/individual-conditional-expectation/ice_age.png" alt="年齢に対するIndividual Conditional Expectation"><br>
これを見ると、傾向として年齢が大人になるくらいまでは、年齢とともに生存確率が下がっていきます。これは直感に合った結果です。
変わったところでいくと、生存確率が年齢の変化とともに変わらない人がいます。<br>
生存確率が0.7以上であり続けた人のデータを軽く確認したところ、性別は全員女性でした。PDPのときもそうでしたが、女性の生存確率が高いモデルになっているのがここからもわかります。</p>
<p>また、ICEでは左端の値をすべてのデータで揃えることで見やすくすることがあります。<br>
各データごとに、0歳のときの予測値でそれぞれの予測値を引いてみた結果が以下のとおりです。
<img src="/mblog/posts/individual-conditional-expectation/ice_age_centered.png" alt="年齢に対するIndividual Conditional Expectation"><br>
データの変化の比較がしやすくなりましたね。</p>
<h2 id="実装">実装</h2>
<p>実装は次のとおりです。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">individual_conditional_expectation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                       <span class="n">x</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">target</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">candidates</span><span class="p">:</span><span class="n">List</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">replaced_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">ice_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidates</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">replaced_val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidates</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">replaced_x</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="n">replaced_val</span>
</span></span><span class="line"><span class="cl">        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">replaced_x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ice_vals</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ice_vals</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">candidates</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="s2">&#34;Age&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ice</span> <span class="o">=</span> <span class="n">individual_conditional_expectation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                         <span class="n">train_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">)</span>
</span></span></code></pre></div><h1 id="まとめ">まとめ</h1>
<p>PDPのようにICEも実装が簡単で、わかりやすい結果が得られます。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/individual-conditional-expectation/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/partial-dependence-plot/">Partial Dependence Plot</a>
        </h1>
        <div class="post-meta"><time class="post-date">2021-10-14</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/partial-dependence-plot/">Partial Dependence Plot</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/">機械学習</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E8%AA%AC%E6%98%8E%E5%8F%AF%E8%83%BDai/">説明可能AI</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/partial-dependence-plot/ppd_sex.png"
    class="post-cover"
    alt="Partial Dependence Plot"
    title="Cover Image" />


        <div class="post-content">
          
            <p>Partial Dependence Plotは任意のモデルのある特徴量に対するglobalな挙動を確認できる手法です。<br>
例えば、特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかがわかります。</p>
<h1 id="partial-dependence-plotの概要">Partial Dependence Plotの概要</h1>
<p>学習済みのモデル$f$へ入力する特徴量$x$のうち、$i$番目の特徴量の変化に対する$f$の出力の挙動の変化を確認したいとします。<br>
このとき、次のようにモデルの出力の期待値を計算します。
$$ E_{X_C}[f(x_i, X_C)] = \int p(X_C) f(x_i, X_C) dX_C .$$
ここで$X_C$は$x_i$以外の特徴量になっていまして、$x_i$の値だけを固定し、それ以外の特徴量について積分をしています。<br>
こうすることで、$x_i$の値により、おおよそどれくらいの出力の差が出るのかがわかります。</p>
<p>注意点として、$x_i$と$X_C$は独立でなければいけません。<br>
独立でないときには$x_i$は$X_C$の関数としてあらわされるため、期待値の計算において$X_C$の変化にともない、$x_i$が変化することになります。こうなると、$x_i$が固定という前提と一致しなくなります。</p>
<h1 id="partial-dependence-plotの実験">Partial Dependence Plotの実験</h1>
<p>kaggleのtitanicの問題でPartial Dependence Plotを試してみます。
モデルはLightGBMの勾配ブースティング法を利用しています。</p>
<p>期待値の計算は訓練データの特徴量$X_{C_{j}},j=1,2,\cdots,n$を用いて以下のように近似値を利用しています。
$$ E_{X_C}[f(x_i, X_C)] = \int p(X_C) f(x_i, X_C) dX_C \approx \frac{1}{n} \sum_{j=1}^n f(x_i,X_{jC}) .$$</p>
<h2 id="年齢に対するpartial-dependence-plot">年齢に対するPartial Dependence Plot</h2>
<p>年齢を$0,5,10,\cdots,65$と変化させてみた結果が以下のとおりです。<br>
<img src="/mblog/posts/partial-dependence-plot/ppd_age.png" alt="年齢に対するPartial Dependence Plot"><br>
年齢が低いほうが、生存しやすかった傾向が読み取れます。また35歳にピークがありますので、なにか理由がありそうです。例えば、年齢があがるほど客室のクラスが良くなりやすいのかもしれません（そうだとすると年齢と客室のクラスは独立ではないのかという話になりますので、実際にはここの考察が必要になりそうです）。</p>
<h2 id="性別に対するpartial-dependence-plot">性別に対するPartial Dependence Plot</h2>
<p>性別についても結果を示します。<br>
<img src="/mblog/posts/partial-dependence-plot/ppd_sex.png" alt="性別に対するPPD"><br>
女性のほうが生存しやすかったという傾向が見て取れます。</p>
<h2 id="実装">実装</h2>
<p>実装は次のとおりです。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_partial_dependence_func_val</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                    <span class="n">x</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">target</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">candidates</span><span class="p">:</span><span class="n">List</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">expecteds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">replaced_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">replaced_val</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">replaced_x</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="n">replaced_val</span>
</span></span><span class="line"><span class="cl">        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">replaced_x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">expecteds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">expecteds</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">candidates</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="s2">&#34;Age&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">ppd</span> <span class="o">=</span> <span class="n">get_partial_dependence_func_val</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                      <span class="n">train_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                      <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                      <span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">)</span>
</span></span></code></pre></div><h1 id="まとめ">まとめ</h1>
<p>Partial Dependence Plotは実装が簡単で、わかりやすい結果が得られます。<br>
ただし、特徴量間が独立でないときは仮定が崩れますので、注意が必要です。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/partial-dependence-plot/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/">貧乏人なのでPoor Man’s BERTを読んで解説</a>
        </h1>
        <div class="post-meta"><time class="post-date">2020-06-21</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/bert/">BERT</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/deeplearning/">DeepLearning</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E/">自然言語</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E8%92%B8%E7%95%99/">蒸留</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E9%AB%98%E9%80%9F%E5%8C%96/">高速化</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/transformer/">Transformer</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/">深層学習</a>&nbsp;
            
          </span>
        

        


        <div class="post-content">
          
            <p>本記事はQrunchからの転載です。</p>
<hr>
<p>最近自然言語処理をよくやっていて、BERTを使うことも多いです。
BERTの性能は高く素晴らしいのですが、実際使う上では、私のような計算リソース弱者には辛いところがあります。</p>
<p>例えば、BERTは非常にパラメータ数が多いことで有名ですが、パラメータが多いと、fine-tuningでの学習や推論の時間がかかることや大きめのメモリが積んであるGPUがないと学習ができない、といった部分がネックになりえます。</p>
<p>BERTのパラメータ数を減らす試みとしてはTinyBERTやDistilBERTによる蒸留を使った手法がありますが、今回紹介する<a href="https://arxiv.org/abs/2004.03844">Poor Man’s BERT: Smaller and Faster Transformer Models</a>ではBERTのTransformerの数を単純に減らすことでパラメータ数を減らしています。</p>
<p>実際にTinyBERTやDistilBERTと同じことをするのは難しいですが、今回のように層を減らして学習するのは容易にできますので、とても実用性があるのではないかと思います。</p>
<h1 id="比較実験">比較実験</h1>
<p>論文では12層のTransformerをもつBERTモデルから色々な方法でTransformerを減らし、性能比較をおこなっています。24層をもつ、いわゆるBERT-Largeは、貧乏人にはメモリが足らずにfine-tuningも難しいのです。</p>
<p>次の図がTransformer層の減らし方の一覧です。</p>
<p><img src="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/5f4774908272540e27f4ce5fc5750c2a.png" alt=""></p>
<p>各方法の詳細は以下のとおりです。</p>
<h2 id="top-layer-dropping">Top-Layer Dropping</h2>
<p>先行研究によると、BERTの後ろの層は目的関数に特化したような重みになっているようです。つまり、BERTで汎用的に使えるように学習されている部分は前の層ということになります。
このため、後ろの層に関しては減らしても性能がそんなに悪化しないんじゃないかという仮定のもと、BERTの最後から4つあるいは6つのTransformerを削除します。</p>
<h2 id="even-alternate-droppingodd-alternate-dropping">Even Alternate Dropping、Odd Alternate Dropping</h2>
<p>先行研究によると、BERTの各層では冗長性があります。つまり、隣り合った層の出力は似ているということです。
このため、1個おきにTransformerを削除します。</p>
<h2 id="contribution-based-dropping">Contribution based Dropping</h2>
<p>Alternate Droppingと少し似ていますが、入力と出力があまり変わらないような層を削除するような方法です。
各Transformer層のなかで[CLS]の入力と出力のcosine類似度が大きい傾向にある層をあらかじめ見つけておき、それを削除します。</p>
<h2 id="symmetric-dropping">Symmetric Dropping</h2>
<p>もしかすると、12層のTransformerのうち、真ん中のあたりはあまり重要じゃないかもしれません。
ということで、前と後ろは残して真ん中付近のTransformerを削除します。</p>
<h2 id="bottom-layer-dropping">Bottom-Layer Dropping</h2>
<p>BERTの最初のほうの層が文脈の理解に重要といわれており、最初のほうを消す理論的な理由はないですが、年のために最初のほうのTransformerを削除したモデルも試します。</p>
<h1 id="実験">実験</h1>
<h2 id="手法間の性能比較">手法間の性能比較</h2>
<p>先程示した方法とDistilBERTをGLUEタスクのスコアで比較した結果が以下になります。BERTだけではなくXLNetでも実験してくれています。</p>
<p><img src="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/ade970e39b6211acf56131ea9aadba79.png" alt=""></p>
<p>これから以下のことが分かります。</p>
<ul>
<li>各方法のスコアは12層あるBertには劣る。</li>
<li>4層減らす分にはBottom-Layer Dropping以外の方法ではそれほど性能に差がでないが、6層減らす場合にはTop-Layer Dropping（最後の6層を消す）が性能劣化が小さい。</li>
<li>Top-Layer Droppingの6層を消した場合はDistilBERTと似たような性能になっている。学習の手間はDistilBERTのほうが圧倒的に大きいので、性能が同程度、計算時間も同程度ならば本手法を使うメリットが大きいです。</li>
<li>XLNetの場合には最後の4層を消したモデルでも12層あるXLNetとほぼ同じ性能が出せる（＝性能劣化が少ない）。</li>
</ul>
<h2 id="タスクごとの性能変化の検証">タスクごとの性能変化の検証</h2>
<p>次にタスクごとの性能の変化を見ていきます。前の実験から後ろの層を消していくTop-Layer Droppingが良いとわかっているため、Top-Layer Droppingに限って実験がされています。</p>
<p><img src="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/b57a4ec7197ef20d888886b7a515f4d1.png" alt=""></p>
<p>問題によっては6層消してもほとんど変化がなかったりします。</p>
<p>余談ですが、私が自分で試したある問題では6層消して8ポイント分、4層消して4ポイント分の性能劣化、2層消して2ポイント分の性能劣化になりました。</p>
<h2 id="タスクごとの性能劣化がおこる層数の検証">タスクごとの性能劣化がおこる層数の検証</h2>
<p>タスクごとに後ろを何層削ると1%、2%、3%の性能劣化がおこるのかを示した表です。</p>
<p><img src="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/43d338f8c5365b5751f603e4304d4337.png" alt=""></p>
<p>ビックリしますが、XLNetは結構層を消しても性能劣化が起こりづらいですね。</p>
<h2 id="パラメータ数や計算時間比較">パラメータ数や計算時間比較</h2>
<p>学習時間・推論時間は削った層の割合だけおおよそ減ることが予想されますが、実際に計算時間がどれくらい変わったかを示したのが以下の表です。</p>
<p><img src="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/75c986295e10731fc36355969fc01cf6.png" alt=""></p>
<p>6層削ったモデルでは学習時間・推論時間の両方でだいたい半分くらいになってますね。</p>
<h2 id="bertとxlnetの層数での比較">BERTとXLNetの層数での比較</h2>
<p>BERTとXLNetのTransformerの数を変えると、どう性能が変化するかを示したのが以下の図です。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/">KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？</a>
        </h1>
        <div class="post-meta"><time class="post-date">2020-03-02</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/python/">Python</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/kldivergence/">KLdivergence</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/pytorch/">PyTorch</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/">機械学習</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%AD%A3%E8%A6%8F%E5%88%86%E5%B8%83/">正規分布</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/983be7a190c4aaf3488cd6c3d4471158.png"
    class="post-cover"
    alt="KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？"
    title="Cover Image" />


        <div class="post-content">
          
            <p>本記事はQrunchからの転載です。</p>
<hr>
<p>みんながよく使うKL(Kullback–Leibler) divergenceの話題です。
KL divergenceといえば2つの確率分布の違いを計算できるやつですね。
KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。
今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。</p>
<h1 id="kl-divergence">KL divergence</h1>
<p>KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。
具体的には次のように定義されます。
$$ KL(p||q) = \int p(\mathbf{x}) \log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) {\rm d\mathbf{x}}. $$
$p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。
なお、$KL(p||q) \geq 0$が成り立つことに注意してください。</p>
<h1 id="kl-divergenceの最小化問題">KL divergenceの最小化問題</h1>
<h2 id="klpqのケース">KL(p||q)のケース</h2>
<p>仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。</p>
<p>前述したKL divergenceの定義をみてみると、$p(\mathbf{x})$が0でない値をもつ領域では$q(\mathbf{x})$も$p(\mathbf{x})$に近い値かあるいは$p(\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような**$q$は$p$全体をカバーするように広がる分布**になると考えられます。</p>
<h2 id="klqpのケース">KL(q||p)のケース</h2>
<p>次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は
$$ KL(q||p) = \int q(\mathbf{x}) \log \left(\frac{q(\mathbf{x})}{p(\mathbf{x})}\right) {\rm d\mathbf{x}}$$
ですね。
$KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\mathbf{x})$が0に近いような領域で$q(\mathbf{x})$が小さくなるようにすればよいです。$p(\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\mathbf{x})$が割り当てられると、$KL(q||p)$が大きくなってしまいますね。このため、イメージとしては、$KL(q||p)$を最小化するような**$q$の密度は$p$の密度が大きいところに集中するような分布**になると考えられます。</p>
<h1 id="実験">実験</h1>
<p>上記の話が成り立つのかを実験してみます。</p>
<h2 id="実験準備">実験準備</h2>
<p>$p(\mathbf{x})$は次のようにします。</p>
<p>$$p(\mathbf{x}|\mathbf{u},\Sigma)=\frac{1}{{2\pi}|\Sigma|^{1/2}}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{u})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{u})}{2}\biggr].$$
また$\mathbf{u}$と$\Sigma$はそれぞれ
$$\mathbf{u} = \begin{pmatrix} 0.3 \\ -0.2 \end{pmatrix}, \Sigma =\begin{pmatrix} 0.9&amp;-0.7 \\ -0.7 &amp; 0.9 \end{pmatrix}$$
とました。
$p$を確率密度毎に色わけして表示してみると、以下のとおりです。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/">画像と自然言語でのマルチモーダルなImageBERT</a>
        </h1>
        <div class="post-meta"><time class="post-date">2020-02-24</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/imagebert/">ImageBERT</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/bert/">BERT</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%94%BB%E5%83%8F/">画像</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E/">自然言語</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%82%AD%E3%83%A3%E3%83%97%E3%82%B7%E3%83%A7%E3%83%8B%E3%83%B3%E3%82%B0/">キャプショニング</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8.png"
    class="post-cover"
    alt="画像と自然言語でのマルチモーダルなImageBERT"
    title="Cover Image" />


        <div class="post-content">
          
            <p>本記事はQrunchからの転載です。</p>
<hr>
<p>最近Microsoftから発表されたImageBERTについて紹介します。<br>
ImageBERTはBERTの入力に自然言語だけではなく、画像も受け付けるようにしたマルチモーダルなモデルです。
また論文ではモデルのアーキテクチャだけではなく、学習方法にも新たな提案がされています。<br>
実験ではImage-to-Sentenceでの検索とSentence-to-Imageの検索タスクでSOTAが示されています。</p>
<p>論文：<a href="https://arxiv.org/abs/2001.07966">ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data</a></p>
<h1 id="アーキテクチャ">アーキテクチャ</h1>
<p>ImageBERTのアーキテクチャは以下のとおりです。
<img src="/mblog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8.png" alt="">
テキストの入力と画像の入力で分けて説明します。
なお、論文中では画像のcaptioningのデータセットを用いています。</p>
<h2 id="テキストの入力">テキストの入力</h2>
<p>テキストは通常のBERTのようにsubwordに分割して、それらのembeddingを入力します。
BERTでは2つの文を与えるときに、1つ目の文か2つ目の文かを識別する情報をsubwordのembeddingに加えますが、ImageBERTでも同じように画像か文かを識別する情報を加えます。図でいうところのSegment Embeddingになります。<br>
また、文の位置情報もBERTやTransformerでは与える必要があり、ImageBERTでも位置情報を加えます。しかし、ここではtokenの順番を昇順に与えるというシンプルなやり方のようです。これは図中のSequence Position Embeddingになります。</p>
<h2 id="画像の入力">画像の入力</h2>
<p>画像はそのままモデルに入力するのではなく、FasterRCNNで物体検出をして、検出された箇所の特徴量をそれぞれ入力する形になります（画像の特徴量はsubwordのembeddingと同じ次元に射影します）。<br>
テキストの場合と同じようにSegment EmbeddingとSequence Position Embeddingも与えるのですが、Sequence Position Embeddingはテキストの場合とは与え方が異なります。テキストの場合にはsubwordに順序がありましたが、画像中の物体には順序がありませんので、すべて同じSequence Position Embeddingを与えます。</p>
<p>また、これら以外にPosition Embeddingというものも与えます。Position Emebeddingは以下で与えられるベクトルをsubwordのembeddingと同じ次元に射影したものです。
$$ c = \begin{pmatrix} \frac{x_{tl}}{W}, \frac{y_{tl}}{H}, \frac{x_{br}}{W}, \frac{y_{br}}{H}, \frac{(x_{br} - x_{tl}) (y_{br} - y_{tl}) }{WH} \end{pmatrix}.$$
ここで、$x_{tl}, y_{tl},  x_{br}, y_{br}$はそれぞれ物体の左上の$x$と$y$、右下の$x$と$y$座標になります。$W$と$H$は入力画像の横と縦の大きさです。
つまり、$c$は物体の位置と面積の割合の情報になります。</p>
<h1 id="事前学習のタスク">事前学習のタスク</h1>
<p>ImageBERTでは事前学習に次の4つタスクを解きます。</p>
<ol>
<li>Masked Language Modeling (MLM)
これは通常のBERTと同じように、入力されるsubwordをランダムにマスクし、マスクされた単語を予測するようなタスクです。</li>
<li>Masked Object Classification (MOC)
これはMLMの画像版のタスクです。検出された物体をランダムにマスクし、マスクされた物体のラベルを予測するようなタスクです。正解ラベルはFaster-RCNNで求まったラベルとしています。</li>
<li>Masked Region Feature Regression (MRFR)
MOCはラベルを予測するようなタスクですが、MRFRはマスクされた物体の箇所の特徴量を予測するタスクです。</li>
<li>Image-Text Matching (ITM)
入力テキストと画像が対応しているかを予測するタスクです。ランダムに画像を選ぶことで、対応していないテキストと画像のペアを作っています。</li>
</ol>
<h1 id="マルチステージの事前学習">マルチステージの事前学習</h1>
<p>ImageBERTでは事前学習をデータセット単位で別々におこないます。実験結果で書かれていますが、別々にすることで性能が大きく変わります。
以下の図のように最初にLarge-Scale Weak-supervised Image-Text Data（これは次に説明します）
で事前学習をし、その次にConceptual CaptionsとSBU Captionsのデータセットで事前学習をします。最後にfinetuningをおこないます。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/">Pandasのgroupbyの使い方をまとめる</a>
        </h1>
        <div class="post-meta"><time class="post-date">2020-02-14</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/python/">Python</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/pabdas/">Pabdas</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/groupby/">GroupBy</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E9%9B%86%E8%A8%88/">集計</a>&nbsp;
            
          </span>
        

        


        <div class="post-content">
          
            <p>本記事はQrunchからの転載です。</p>
<hr>
<p>Pandasのgroupbyについては雰囲気でやっていたところがありますので、ちょっと真面目に使い方を調べてみました。使っているPandasのバージョンは1.0.1です。</p>
<p>以下では次のようなDataFrameを使用します。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&#34;名字&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;田中&#34;</span><span class="p">,</span> <span class="s2">&#34;山田&#34;</span><span class="p">,</span> <span class="s2">&#34;上田&#34;</span><span class="p">,</span> <span class="s2">&#34;田中&#34;</span><span class="p">,</span> <span class="s2">&#34;田中&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                            <span class="s2">&#34;年齢&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                            <span class="s2">&#34;出身&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;北海道&#34;</span><span class="p">,</span> <span class="s2">&#34;東京&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;沖縄&#34;</span><span class="p">,</span> <span class="s2">&#34;北海道&#34;</span><span class="p">]})</span>
</span></span></code></pre></div><table>
  <thead>
      <tr>
          <th style="text-align: right">　</th>
          <th style="text-align: left">名字</th>
          <th style="text-align: right">年齢</th>
          <th style="text-align: left">出身</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">0</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">10</td>
          <td style="text-align: left">北海道</td>
      </tr>
      <tr>
          <td style="text-align: right">1</td>
          <td style="text-align: left">山田</td>
          <td style="text-align: right">20</td>
          <td style="text-align: left">東京</td>
      </tr>
      <tr>
          <td style="text-align: right">2</td>
          <td style="text-align: left">上田</td>
          <td style="text-align: right">30</td>
          <td style="text-align: left">　</td>
      </tr>
      <tr>
          <td style="text-align: right">3</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">40</td>
          <td style="text-align: left">沖縄</td>
      </tr>
      <tr>
          <td style="text-align: right">4</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">50</td>
          <td style="text-align: left">北海道</td>
      </tr>
  </tbody>
</table>
<h1 id="pandasのgroupby">Pandasのgroupby</h1>
<p>PandasのgroupbyはSQLにおけるgroupbyと似たような働きになります。つまるところ、主に集計に使われます。</p>
<p>例えば名字という列をキーとしてgroupbyするときには次のようにします。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&#34;名字&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>ただしこれだけでは全く意味がありません。
以下ではgroupbyをしたあとにどう利用することができるかを示します。</p>
<h1 id="グループ毎にdataframeを取り出す">グループ毎にDataFrameを取り出す</h1>
<h2 id="forを使う">forを使う</h2>
<p>forを使ってグループ毎にDataFrameとしてデータを取り出せます。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">grouped_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&#34;名字&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;名字：</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">grouped_df</span><span class="p">)</span>
</span></span></code></pre></div><p>名字：上田</p>
<table>
  <thead>
      <tr>
          <th style="text-align: right">　</th>
          <th style="text-align: left">名字</th>
          <th style="text-align: right">年齢</th>
          <th style="text-align: left">出身</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">2</td>
          <td style="text-align: left">上田</td>
          <td style="text-align: right">30</td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
<p>名字：山田</p>
<table>
  <thead>
      <tr>
          <th style="text-align: right">　</th>
          <th style="text-align: left">名字</th>
          <th style="text-align: right">年齢</th>
          <th style="text-align: left">出身</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">1</td>
          <td style="text-align: left">山田</td>
          <td style="text-align: right">20</td>
          <td style="text-align: left">東京</td>
      </tr>
  </tbody>
</table>
<p>名字：田中</p>
<table>
  <thead>
      <tr>
          <th style="text-align: right">　</th>
          <th style="text-align: left">名字</th>
          <th style="text-align: right">年齢</th>
          <th style="text-align: left">出身</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: right">0</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">10</td>
          <td style="text-align: left">北海道</td>
      </tr>
      <tr>
          <td style="text-align: right">3</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">40</td>
          <td style="text-align: left">沖縄</td>
      </tr>
      <tr>
          <td style="text-align: right">4</td>
          <td style="text-align: left">田中</td>
          <td style="text-align: right">50</td>
          <td style="text-align: left">北海道</td>
      </tr>
  </tbody>
</table>
<h2 id="get_groupを使う">get_groupを使う</h2>
<p>get_groupを使えば1つのグループを指定することもできます。</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/">[Read more]</a>
          </div>
        
      </article>
    

    <div class="pagination">
  <div class="pagination__buttons">
    
    
    
      <a href="/mblog/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/page/2/" class="button inline next">
        [<span class="button__text">Older posts</span>] &gt;
      </a>
    
  </div>
</div>

  </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/mblog/bundle.min.js"></script>





  
</div>

</body>
</html>
