<!DOCTYPE html>
<html lang="ja">
<head><script src="/mblog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=mblog/livereload" data-no-instant defer></script>
  
    <title>ディープラーニング :: MatLoverによるMatlab以外のブログ</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="http://localhost:1313/mblog/categories/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/" />


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-LFC5W8DKV1"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-LFC5W8DKV1');
        }
      </script>



  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/main.min.775ac2af004d44c22a6d000fbd1d9af529642f5cef27399d0280d180af2c2e9b.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/menu.min.310d32205bdedd6f43144e3c3273c9deecd238eba5f9108db5ea96ca0cfbe377.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/post.min.ad50c7f4d00e7975918f37fc74c6029e1959a40d66fb5b2c6564a8715e985573.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/syntax.min.e9ab635cf918bc84b901eb65c0b2caa74c9544245e3647c1af5c129896ef276e.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terminal.min.77ee67c1d456ac0c280223661a10b75c7729c59aeb33001424a72a14b363e310.css">

  
  <link rel="stylesheet" href="http://localhost:1313/mblog/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="http://localhost:1313/mblog/favicon.png">
<link rel="apple-touch-icon" href="http://localhost:1313/mblog/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="ja" />
<meta property="og:type" content="website" />
<meta property="og:title" content="ディープラーニング">
<meta property="og:description" content="" />
<meta property="og:url" content="http://localhost:1313/mblog/categories/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/" />
<meta property="og:site_name" content="MatLoverによるMatlab以外のブログ" />

  <meta property="og:image" content="http://localhost:1313/mblog/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">





  <link href="/mblog/categories/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/index.xml" rel="alternate" type="application/rss+xml" title="MatLoverによるMatlab以外のブログ" />







<link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css"
    integrity="sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5"
    crossorigin="anonymous">

<script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js"
    integrity="sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd"
    crossorigin="anonymous"></script>

<script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js"
    integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
            ],
            throwOnError : false
        });
    });
</script>


<style>
  section.article-content h2 {
    background-color: rgb(245, 245, 245);
    padding: 10px;
  }

  :root {
    --ja-font-family: "メイリオ", "Meiryo";
    --base-font-family: "Lato", var(--sys-font-family), var(--ja-font-family),
      sans-serif;
    --body-background: #ffffe0;
  }


</style>


</head>
<body>


<div class="container full">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="http://localhost:1313/mblog/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/mblog/">Home</a></li>
        
      
        
          <li><a href="/mblog/archives/">Archives</a></li>
        
      
        
          <li><a href="/mblog/search/">Search</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/mblog/" >Home</a></li>
        
      
        
          <li><a href="/mblog/archives/" >Archives</a></li>
        
      
        
          <li><a href="/mblog/search/" >Search</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
  <h1>Posts for: #ディープラーニング</h1>
  
  <div class="posts">
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-07-28</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/nlp/">NLP</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/llm/">LLM</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/">大規模言語モデル</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/vlm/">VLM</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/clip/">CLIP</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%94%BB%E5%83%8F/">画像</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/thm.png"
    class="post-cover"
    alt="「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介"
    title="Cover Image" />


        <div class="post-content">
          
            <p>今回紹介するのは
<a href="https://arxiv.org/pdf/2506.08008">Hidden in plain sight: VLMs overlook their visual representations</a>
です.</p>
<p>テキストの生成というよりも画像が中心となるタスクに対し、オープンソースのVisual Language Modelの性能について調査した論文になっています.
DINOやCLIPをLLMに組み込んだマルチモーダルモデルは、単体のViT系のモデルよりも性能が<strong>大きく下がる</strong>ことを示しています.</p>
<p>例えば、次の図では左の2枚の画像が与えられ、上の画像の「Ref」と書かれている点と同じ点は下の画像のA~Dの4つの点のどれか？というのを当てる問題を解くことを考えます.<br>
DINOやCLIP単体によって問題を解いたとき、DINOでは80%、CLIPでは60%程度のAccuracyでしたが、VLMを用いるとチャンスレート（適当に答えたときの性能）よりも低くなってしまいます.</p>
<p><img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig1.png" alt="性能比較"></p>
<h2 id="llavaによるvlmの実現">LLaVAによるVLMの実現</h2>
<p>まず、VLMはどのように実現されているかの話になります.<br>
本論文で扱われている<a href="https://github.com/TRI-ML/prismatic-vlms">LLaVA</a>ではDINOのようなVisualモデルから得られた画像のトークン列をLLMのembeddingの空間にマッピングするようなProjector層を追加しています.</p>
<p><img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/llava_arch.png" alt="LLaVAの構成"></p>
<p>LLaVAでのファインチューニングは次の2段階の処理から構成されるようです.</p>
<ol>
<li>Projector層単体ののファインチューニング</li>
<li>EndToEndのファインチューニング</li>
</ol>
<h2 id="比較実験の方法">比較実験の方法</h2>
<p>LLaVAはEndToEndでファインチューニングしていますが、本論文ではProjector層のファインチューニングした場合のVLMとVisualモデルの比較を中心におこなっています.これは、VLMのVisualモデルの重みを固定しておくことで、VLMとVisualモデル単体との正確な比較をおこなうようにするためです.<br>
ただし、VLM用にEndToEndでファインチューニングされた既存のオープンソースのVisualモデルでさえも性能悪化の傾向があることを示すため、 QwenやPhi-3などでも一部の実験をおこなっています.</p>
<hr>
<h2 id="タスク">タスク</h2>
<p>以下では扱っているタスクの一覧を示します.<br>
タスクの具体例をあらわす画像は論文のなかのVisualモデルとVLMが間違った例を用いています.</p>
<h3 id="カメラ距離を推定するタスク">カメラ距離を推定するタスク</h3>
<p>どちらのBounding Boxのほうがカメラに近いかを判定するタスク.</p>
<p><img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/depth_estimation.png" alt="カメラ距離"></p>
<ul>
<li>LLMにはPromptとBounding Box付きの画像を入力し、どちらのBounding Boxがカメラに近いかを出力させる.</li>
<li>Visualモデルはそのままだと解くことができないため、NYUv2という深度を推定するタスクのデータセットを用いてVisualモデルにDPT Headを追加して訓練する.
<ul>
<li>DPT Headは奥行きの推定の出力部分</li>
</ul>
</li>
</ul>
<h3 id="視覚的な類似箇所を推定するタスク">視覚的な類似箇所を推定するタスク</h3>
<p>2枚の画像から視覚的に似ている箇所を見つけるタスク.
<img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/semantic_correspondence.png" alt="類似箇所"></p>
<ul>
<li>Reference画像に1つの点があり、もう一枚の画像にはA,B,C,Dの4つの点を用意する.</li>
<li>VLMにはどの点が対応するかを出力させる.</li>
<li>Visualモデルではそのまま解くことはできないため、Reference画像の点の特徴量と一番類似度が高くなる特徴量に対応する点をA~Dから選ぶ.</li>
</ul>
<h3 id="機能的な類似箇所を推定するタスク">機能的な類似箇所を推定するタスク</h3>
<p>2枚の画像から機能的に似ている箇所を見つけるタスク.
<img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/affordance.png" alt="機能的類似箇所"></p>
<p>問題の解き方は1つ前のタスクと同じです.</p>
<h3 id="同じ位置を推定するタスク">同じ位置を推定するタスク</h3>
<p>2枚の照明条件や視点が異なる画像から同じ位置を見つけるタスク
<img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/low_level_match.png" alt="同定"></p>
<p>問題の解き方は1つ前のタスクと同じです.</p>
<h3 id="最も似ていない3dオブジェクトを推定するタスク">最も似ていない3Dオブジェクトを推定するタスク</h3>
<p>4枚の画像から4枚の画像から4枚の画像から4枚の画像から最も似ていない3Dオブジェクトの画像を選択するタスク.
<img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/3d.png" alt="3D"></p>
<ul>
<li>VLMは4つのなかで一番似ていない画像を出力させる.</li>
<li>VisualモデルはVisualモデルの[CLS]トークン部分の特徴量同士の類似度をもとに選択する.</li>
</ul>
<h3 id="似ている画風の絵画を選択するタスク">似ている画風の絵画を選択するタスク</h3>
<p>与えられた画像に似ている画風の絵画を2枚のなかから選択するタスク.
<img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/art.png" alt="art"></p>
<ul>
<li>VLMは3つの画像を与えて、似ている画風の画像を出力させる.</li>
<li>Visualモデルの場合、画像のパッチの各特徴量からグラム行列を作ると画像のstyleを表現できることを利用し、Reference画像と比較対象画像のグラム行列の二乗誤差が小さいものを似ている画風であると選択する.</li>
</ul>
<h2 id="実験結果">実験結果</h2>
<h3 id="visualモデルとprojector層をftしたvlmの性能比較">Visualモデルとprojector層をFTしたVLMの性能比較</h3>
<p><img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig2.png" alt="VisualモデルとVLMの比較"></p>
<ul>
<li>Visualモデル単体に比べてVLMは性能が悪化し、チャンスレートよりも低くなりうる.</li>
</ul>
<h3 id="オープンソースのvlmとvisualモデル部分の性能比較">オープンソースのVLMとVisualモデル部分の性能比較</h3>
<p>他のVLMでのVisualモデルとの性能比較.</p>
<p><img src="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig3.png" alt="オープンソースモデルとの比較"></p>
<ul>
<li>この結果でも、基本的にはVisualモデル単体のほうが性能が高い.
<ul>
<li>InternVLの3D Objectの問題のように、そもそもVisualモデルの性能が低い場合には改善することもある.</li>
</ul>
</li>
</ul>
<p><strong>以上の結果から、VLMは全く入力画像を正しく参照できていないかもしれない&hellip;</strong></p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/">拡散言語モデルのLLaDA</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-06-30</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/nlp/">NLP</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E6%8B%A1%E6%95%A3%E3%83%A2%E3%83%87%E3%83%AB/">拡散モデル</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/llm/">LLM</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/llama/">LLaMA</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/llada/">LLaDA</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/bert/">BERT</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/">大規模言語モデル</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/mlm/">MLM</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/thm.png"
    class="post-cover"
    alt="拡散言語モデルのLLaDA"
    title="Cover Image" />


        <div class="post-content">
          
            <h1 id="bertを拡張した生成モデル拡散型llmlladaの概要と可能性">BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性</h1>
<p>2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります.
提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.</p>
<p><img src="/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig1.png" alt="性能比較top"></p>
<p>本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.</p>
<hr>
<h2 id="自己回帰型モデルの限界">自己回帰型モデルの限界</h2>
<p>従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：</p>
<ul>
<li>逐次処理のため推論効率が悪い</li>
<li>「Reversal Curse」に弱い（参考：<a href="https://arxiv.org/pdf/2309.12288%EF%BC%89">THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A”</a>
<ul>
<li>Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです.</li>
</ul>
</li>
</ul>
<p><img src="/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/reversal_curse.png" alt="Reversal Curse"></p>
<h2 id="従来のllmのアプローチ">従来のLLMのアプローチ</h2>
<p>LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.</p>
<p>$$
\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)).
$$</p>
<p>特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.</p>
<p>$$
p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}).
$$</p>
<h2 id="lladaのアプローチ拡散モデル型のllm">LLaDAのアプローチ：拡散モデル型のLLM</h2>
<p>LLaDAは、自己回帰ではなく<strong>拡散モデル</strong>のアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.</p>
<h3 id="事前学習pretraining">事前学習（Pretraining）</h3>
<p><img src="/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_1.png" alt="事前学習の概要"></p>
<p>事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.</p>
<p>損失関数は次の通りです：</p>
<p>$$
\mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right].
$$</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/">外部知識を活用して効率的に性能向上を達成したYOLO-RD</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-05-31</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA/">物体検出</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/yolo/">YOLO</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%94%BB%E5%83%8F/">画像</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/rag/">RAG</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/yolo-rd/">YOLO-RD</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/thm.png"
    class="post-cover"
    alt="外部知識を活用して効率的に性能向上を達成したYOLO-RD"
    title="Cover Image" />


        <div class="post-content">
          
            <p>YOLO-RD (Retriever-Dictionary) は、物体検出の分野で定番となっているYOLO（You Only Look Once）シリーズの最新研究です.
今回は、ICLR2025で発表されたYOLO-RD(<a href="https://arxiv.org/abs/2410.15346">https://arxiv.org/abs/2410.15346</a>)について解説します.</p>
<h2 id="従来手法の限界と問題意識">従来手法の限界と問題意識</h2>
<p>現在の画像系のモデルのアプローチは大きくわけて2種類です.</p>
<ol>
<li>CNN: 畳み込みによって得られる局所情報を組み合わせて推論</li>
<li>Transformer: 画像内の広範な相互作用を活用して推論</li>
</ol>
<p>これらも現在ではかなり高い精度を達成できるようになりましたが、「入力画像とモデル内で計算される情報」しか活用できません（当たり前なのですが）.<br>
もしRAGのように入力画像に応じて外部からの情報を参照することができれば、モデルの推論性能が向上するのでは？というのがYOLO-RDの発想になります.<br>
人間も、例えば犬のような動物を見たときに、犬ってこういう形だなぁと連想し、それから確かに目の前の動物は犬だと判断したり、あるいは犬じゃない他の動物かも？と判断していると思います．このように適宜情報を連想するという話に近いのかなと思います．</p>
<p><img src="/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig1.png" alt="YOLO-RDの概要"></p>
<h2 id="yolo-rdの概要">YOLO-RDの概要</h2>
<p>YOLO-RDは推論時に外部の知識（特徴量）を参照する仕組みを組み込んだYOLOです.<br>
これにより、モデルが画像からは直接取得できない情報を補い、パラメータ数を大きく増やすことなく高精度化を実現しています.</p>
<p><img src="/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig2.png" alt="YOLO-RDの全体像"></p>
<p>上図はYOLO-RDの全体像をあらわした図です.
バックボーンから得られた特徴量をもとにして、辞書内の特徴量(atomというようです)ごとの重みを決めるというのをRetrieverでおこないます.<br>
得られた重みによる重み付き和を辞書内のatomに対して計算し、Neck層に渡すという流れになっています.</p>
<p>式であらわすと次のようになっています.<br>
バックボーンから得られた$(w,h)$の位置のピクセルの特徴量$X_{w,h} \in \mathbb{R}^f$を用いて、</p>
<p>$$
Y_{w,h} = \lambda \cdot X_{w,h} + (1 - \lambda) \cdot \sum_{i=1}^N c&rsquo;_{i,w,h} \cdot \alpha_i.
$$</p>
<p>を新たな特徴量とします.ここで</p>
<ul>
<li>$\alpha_i  \in \mathbb{R}^f$ は2-ノルムが1のatom</li>
<li>$c&rsquo;_{i,w,h} \in \mathbb{R}$はatomの取捨選択を表現している係数</li>
<li>$\lambda \in [0, 1] $はハイパーパラメータ</li>
<li>$N \in \mathbb{N}$は辞書サイズ</li>
</ul>
<p>をあらわします.</p>
<h3 id="retriever">Retriever</h3>
<p>$(w,h)$上のピクセルの$i$番目のatomの係数$c&rsquo;_{i,w,h}$は以下のようにして計算します.</p>
<ol>
<li>行列$W^G \in \mathbb{R}^{N \times f}$との内積によってベースとなるatomの重み$Y   \in \mathbb{R}^{N \times W \times H}$を計算
$$Y = G(X) = W^G \cdot X. $$</li>
<li>depthwise convolutionをおこない、近傍ピクセルの情報の取り込み($i$はチャネル)
$$c_i = E(Y^{(i)}) = W^{E^{(i)}} * Y^{(i)}.$$</li>
<li>学習を適切に進めるためにPositional Normalizationを適用（$\gamma,\beta$は学習するパラメーター）
$$
\begin{align*}
c^{\prime}_{i,w,h} &amp;= \frac{c_{i,w,h} - \mu_{c_{w,h}}}{\sqrt{\sigma_{c_{w,h}} + \varepsilon}} \cdot \gamma + \beta,\\
\mu_{c_{w,h}} &amp;= \frac{1}{N} \sum_{n=1}^N c_{n,w,h},\\
\sigma_{c_{w,h}} &amp;= \frac{1}{N}\sum_{n=1}^N (c_{w,h} - \mu_{c_{w,h}})^2.
\end{align*}
$$</li>
</ol>
<p>1と2は一度のconvolutionにまとめることができますが、計算コストのためにこのようになっています.<br>
また、3についてはatomの重み付き和が$X$と同じになってしまうことを防ぐためにおこなわれています.バックボーンから得られる特徴量が良い特徴量であるという前提にたつと、そこに学習初期にはノイズに近いような値になっている辞書情報を足すことにメリットがありません.そして、辞書情報側のパラメーターをうまく活用してlossを下げるよりも、バックボーンと同じ特徴量がNeck層に渡るほうが早くlossを下げられるため、結果的に辞書が意味をなさなくなります.Normalizationを入れることで、$X$をそのままコピーするような出力が難しくなるため、適切に学習が進むようになります.</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/label-studio%E3%81%AEapi%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%80%A3%E6%90%BA%E3%81%AE%E3%83%A1%E3%83%A2/">Label StudioのAPIを利用したデータ連携のメモ</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-04-24</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/label-studio/">Label Studio</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%82%A2%E3%83%8E%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3/">アノテーション</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA/">物体検出</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/pre-annotations/">Pre-Annotations</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/label-studio%E3%81%AEapi%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%80%A3%E6%90%BA%E3%81%AE%E3%83%A1%E3%83%A2/thm.png"
    class="post-cover"
    alt="Label StudioのAPIを利用したデータ連携のメモ"
    title="Cover Image" />


        <div class="post-content">
          
            <p>Label StudioのAPIを利用するとき用のメモになります．下記で出てくる例は物体検出を例にしています．</p>
<h2 id="taskの一覧の取得">Taskの一覧の取得</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">requests</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">LABEL_STUDIO_HOST</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;LABEL_STUDIO_HOST&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">LABEL_STUDIO_PROJECT_ID</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;LABEL_STUDIO_PROJECT_ID&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">LABEL_STUDIO_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;LABEL_STUDIO_TOKEN&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># page_sizeとpageによって取得できるタスクの範囲が変わるため、必要に応じて変更.</span>
</span></span><span class="line"><span class="cl"><span class="n">tasks</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">LABEL_STUDIO_HOST</span><span class="si">}</span><span class="s2">/api/tasks/?project=</span><span class="si">{</span><span class="n">LABEL_STUDIO_PROJECT_ID</span><span class="si">}</span><span class="s2">&amp;page_size=1000&amp;page=1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;Authorization&#34;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&#34;Token </span><span class="si">{</span><span class="n">LABEL_STUDIO_TOKEN</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="taskの新規登録">Taskの新規登録</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">task_info</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">LABEL_STUDIO_HOST</span><span class="si">}</span><span class="s2">/api/tasks/?project=</span><span class="si">{</span><span class="n">LABEL_STUDIO_PROJECT_ID</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;Authorization&#34;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&#34;Token </span><span class="si">{</span><span class="n">LABEL_STUDIO_TOKEN</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;data&#34;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&#34;image&#34;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">storage_file_path</span><span class="p">)},</span> <span class="c1"># storage_file_pathはlocalならlocalのパス、Cloud上ならばCloud上のパス.</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;project&#34;</span><span class="p">:</span> <span class="n">LABEL_STUDIO_PROJECT_ID</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;file_upload&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># sync済みのdata source上のデータを読み込む場合は1</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="pre-annotationの登録">Pre-Annotationの登録</h2>
<p>案件によっては、既存のモデルやシステムからラフなアノテーションが得られるときがあります．このときにはPre-Annotationを利用すると良いでしょう．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 登録するアノテーションの作成.</span>
</span></span><span class="line"><span class="cl"><span class="n">annotations</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">bottom</span><span class="p">),</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">labels</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="n">annotations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;id&#34;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;rectanglelabels&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;from_name&#34;</span><span class="p">:</span> <span class="s2">&#34;label&#34;</span><span class="p">,</span> <span class="c1"># このあたりの値もきっちり入れておかないと登録がうまくいかないため注意.</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;to_name&#34;</span><span class="p">:</span> <span class="s2">&#34;image&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;original_width&#34;</span><span class="p">:</span> <span class="n">original_width</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;original_height&#34;</span><span class="p">:</span> <span class="n">original_height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;image_rotation&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;value&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;x&#34;</span><span class="p">:</span> <span class="n">left</span> <span class="o">/</span> <span class="n">original_width</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;y&#34;</span><span class="p">:</span> <span class="n">top</span> <span class="o">/</span> <span class="n">original_height</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;width&#34;</span><span class="p">:</span> <span class="p">(</span><span class="n">right</span> <span class="o">-</span> <span class="n">left</span><span class="p">)</span> <span class="o">/</span> <span class="n">original_width</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;height&#34;</span><span class="p">:</span> <span class="p">(</span><span class="n">bottom</span> <span class="o">-</span> <span class="n">top</span><span class="p">)</span> <span class="o">/</span> <span class="n">original_height</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;rotation&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;rectanglelabels&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">label</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">res</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">LABEL_STUDIO_HOST</span><span class="si">}</span><span class="s2">/api/predictions&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;Authorization&#34;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&#34;Token </span><span class="si">{</span><span class="n">LABEL_STUDIO_TOKEN</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="n">json</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;task&#34;</span><span class="p">:</span> <span class="n">task_info</span><span class="p">[</span><span class="s2">&#34;id&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;model_version&#34;</span><span class="p">:</span> <span class="s2">&#34;1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;result&#34;</span><span class="p">:</span> <span class="n">boxes</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><h2 id="モデルの予測結果の登録">モデルの予測結果の登録</h2>
<p>Label Studioにはモデルの予測結果を用いてアノテーションさせる機能がついています．これによって、モデルが解ける部分についてはアノテーションの手間が減るため、非常に作業が楽になります．</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/label-studio%E3%81%AEapi%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%87%E3%83%BC%E3%82%BF%E9%80%A3%E6%90%BA%E3%81%AE%E3%83%A1%E3%83%A2/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E5%9B%9E%E8%BB%A2%E3%81%97%E3%81%A6%E3%81%84%E3%82%8Bbounding-box%E5%90%91%E3%81%91%E3%81%AEiou%E3%81%AEkfiou/">回転しているBounding Box向けのIoUのKFIoU</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-03-22</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/iou/">IoU</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA/">物体検出</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%82%AB%E3%83%AB%E3%83%9E%E3%83%B3%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%83%BC/">カルマンフィルター</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/rotated-bounding-box/">Rotated Bounding Box</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/cv/">CV</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/kfiou/">KFIoU</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/skewiou/">SkewIoU</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/%E5%9B%9E%E8%BB%A2%E3%81%97%E3%81%A6%E3%81%84%E3%82%8Bbounding-box%E5%90%91%E3%81%91%E3%81%AEiou%E3%81%AEkfiou/thm.png"
    class="post-cover"
    alt="回転しているBounding Box向けのIoUのKFIoU"
    title="Cover Image" />


        <div class="post-content">
          
            <h2 id="従来手法の問題点">従来手法の問題点</h2>
<p>回転しているBounding Box向けの微分可能なIoUの計算というのは簡単ではありません．既存手法としてGWDやKLDがありますが、問題ごとにハイパーパラメータの調整が必要になります．これを解決してより扱いやすく性能が高い手法になったのが<a href="https://arxiv.org/abs/2201.12558">KFIoU</a>になります．</p>
<h2 id="kfiou">KFIoU</h2>
<h3 id="bounding-boxの正規分布への変換">Bounding Boxの正規分布への変換</h3>
<p>KFIoUを計算するための前段階として、正解のBounding Boxと予測されたBounding Boxを正規分布に変換します．<br>
具体的には、Bounding Boxが中心を$(x, y)$、横幅と縦幅を$w, h$, 回転角度を$\theta$としたときに次で定義される平均$\mu$、共分散行列$\Sigma$の正規分布に変換します．
$$
\Sigma = R \Lambda R^T, \ \mu = \begin{pmatrix}x, y\end{pmatrix}.
$$</p>
<p>ここで
$$
R = \begin{pmatrix} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta  \end{pmatrix}, \Lambda = \begin{pmatrix} \frac{w^2}{4} &amp; 0 \\ 0 &amp; \frac{h^2}{4}  \end{pmatrix}
$$</p>
<p>です．  回転しているBounding Boxのそれぞれの辺に沿ったベクトルが$R$の列になっています．
一見、なぞの変換ではあるのですが、このように定義すると以下のようにしてBounding Boxの面積$\mathcal{V_B}(\Sigma)$を求めることができます．</p>
<p>$$
\mathcal{V_B}(\Sigma) = 2^2 |\Sigma|^{1/2}.
$$</p>
<p>$R$が回転行列であることを用いれば
$$
|\Sigma| = |R \Lambda R^T| = |R| |\Lambda| |R| = |\Lambda| = \frac{w^2h^2}{16}
$$
ですので、$\mathcal{V_B}(\Sigma)=wh$ となり、無事にBounding Boxの面積に一致します．</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E5%9B%9E%E8%BB%A2%E3%81%97%E3%81%A6%E3%81%84%E3%82%8Bbounding-box%E5%90%91%E3%81%91%E3%81%AEiou%E3%81%AEkfiou/">[Read more]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="http://localhost:1313/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/">特徴量の次元の柔軟性が高いマトリョーシカ表現学習</a>
        </h1>
        <div class="post-meta"><time class="post-date">2025-02-17</time></div>

        
          <span class="post-tags">
            
            #<a href="http://localhost:1313/mblog/tags/%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/">表現学習</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/">ディープラーニング</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E7%89%B9%E5%BE%B4%E9%87%8F/">特徴量</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/%E9%81%A9%E5%BF%9C%E7%9A%84%E6%A4%9C%E7%B4%A2/">適応的検索</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/embedding/">Embedding</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/rag/">RAG</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/azure/">Azure</a>&nbsp;
            
            #<a href="http://localhost:1313/mblog/tags/ai-search/">AI Search</a>&nbsp;
            
          </span>
        

        
  <img src="/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/thm.png"
    class="post-cover"
    alt="特徴量の次元の柔軟性が高いマトリョーシカ表現学習"
    title="Cover Image" />


        <div class="post-content">
          
            <p>一般には、分類問題向けに学習させたディープラーニングモデルから得られる特徴量の次元はあとから変更することはできず、学習のときに固定されてしまいます．
もしも、学習後に精度をあまり落とさずに次元を小さくできるのであれば、計算リソースやサービスの要求に応じた次元を選択できるため非常に便利です．
それを実現するための方法として<a href="https://arxiv.org/abs/2205.13147">Matryoshka Representation Learning（マトリョーシカ表現学習）</a>があります．<br>
なお、マトリョーシカ表現学習はAzureのAI Searchのベクトル検索で利用可能になっています．</p>
<h2 id="マトリョーシカ表現学習の概要">マトリョーシカ表現学習の概要</h2>
<p><img src="/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/fig1.png" alt="マトリョーシカ表現学習の概要"></p>
<h3 id="学習">学習</h3>
<p>一般にはディープラーニングモデルを用いて$L$クラス分類問題を解くとき、入力データ$x$に対してembedding $z \in \mathbb{R}^d $を計算し、それに対して重み$W \in \mathbb{R}^{L \times d}$を用いて
$$
W z
$$
を計算します。その後、$Wz$の値から分類問題が解けるようにモデルの学習を進めていきます．$z$は$d$次元であることを前提にしていますので、残念ながら学習後に先頭から$m$次元目までを切り取った$z_{1:m}$のようなembeddingの一部をみても良い特徴量にはなりません．</p>
<p>マトリョーシカ表現学習(MRL)ではこれを解決するため、事前にembeddingの次元の集合を$\mathcal{M}$を定義しておき（例えば$\mathcal{M}=\{8,16,\cdots,1024, 2048\}$）、各次元ごとにembeddingを切り出してそれを用いて分類問題が解けるように学習をしていきます．<br>
具体的には、データセットを$\mathcal{D} = \{(x_1,y_1),\cdots,(x_N, y_N) \}$、各$\mathcal{M}$の要素$m$ ごとに用意した重みを$W^{(m)} \in \mathbb{R}^{L \times m}$、スケーリングのパラメータを$c_m$、分類用のLossを$\mathcal{L}$としたとき、以下の最小化問題を解くように学習をおこないます．
$$
\begin{align*}
\min_{\{W^{(m)}\}_{m \in \mathcal{M}}, \theta_F} \frac{1}{N} \sum_{(x_i, y_i) \in \mathcal{D}} \sum_{m \in \mathcal{M}} c_m \cdot \mathcal{L} \left(W^{(m)} \cdot F(x_i;\theta_F)         _{1:m};y_i  \right  )
\end{align*}
$$
ここで、embeddingがモデルのパラメータ$\theta_F$に依存していることを明示するため、$x_i$に対応するembedding $z_i \in \mathbb{R}^d$を$F(x_i;\theta_F)$とあらわしています．<br>
この式であらわしているように、小さい次元のembeddingでも分類問題が解けるようにすることで、そのような一部のembeddingだけでも良い特徴量になることを期待しています．このように、embeddingがマトリョーシカのように入れ子の形になっているためマトリョーシカ表現学習という名前になっています．<br>
また、もしかすると$c_m$の調整が少し面倒なのかな？と思いましたが、論文ではすべて$c_m=1$としているようです．</p>
<p>この手法で少し気になってくるのは、重み行列$W^{(m)}$をそれぞれの$m$ごとに用意するとメモリ使用量が増えることです．<br>
これに対しては、共通の重み行列$W$を1つ用意し、$W^{m} = W_{1:m}$のように$W$の1行目から$m$行目までを切り出すという形で定義する方法が提案されています．これをEfficient Matryoshka Representation Learning（MRL-E）と呼んでいます．</p>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/mblog/posts/%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E6%AC%A1%E5%85%83%E3%81%AE%E6%9F%94%E8%BB%9F%E6%80%A7%E3%81%8C%E9%AB%98%E3%81%84%E3%83%9E%E3%83%88%E3%83%AA%E3%83%A7%E3%83%BC%E3%82%B7%E3%82%AB%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92/">[Read more]</a>
          </div>
        
      </article>
    

    <div class="pagination">
  <div class="pagination__buttons">
    
    
    
  </div>
</div>

  </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/mblog/bundle.min.js"></script>





  
</div>

</body>
</html>
